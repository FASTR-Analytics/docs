
## Module 1 - Data Quality Assessment

What is data quality assessment?

The Data Quality Assessment (DQA) methodology applies three core assessments: detection of extreme outliers in reported service volumes, analysis of indicator-specific completeness, and consistency checks between related indicators. For each facility and month, the data are processed to identify anomalies, assess reporting continuity, and validate internal coherence across indicators. Outliers are flagged using Median Absolute Deviation (MAD) methods and proportional thresholds. Completeness is evaluated based on whether facilities report non-zero values across months for core indicators. Consistency is checked across linked indicators (e.g., ANC1 vs ANC4, Penta1 vs Penta3), using predefined ratio thresholds. The results are integrated into a composite DQA score that summarizes whether a facility's data meet all quality benchmarks.

## Module 2 - Data Quality Adjustment

What is data quality adjustment?

Data quality adjustment is the process of correcting known issues in routine health data to improve the reliability of analyses. In the FASTR approach, adjustments are applied to HMIS data using automated methods that identify and correct outlier values and impute missing data based on patterns in historical reporting. This allows countries to make more robust use of the data they already have, even when reporting is imperfect. Why adjust for data quality? Routine health data, such as those captured in DHIS2, are an essential resource for monitoring and improving primary health care systems. However, data quality issues — like outliers, incomplete reporting, and inconsistencies between related indicators — can distort analyses and misinform decision-making. The FASTR approach includes built-in statistical methods to identify and adjust for these issues, producing more robust and actionable results. While not all data quality challenges can be fully resolved, targeted adjustments improve the reliability of the data and help ensure it can still inform decisions. Rather than treating poor data quality as a barrier, FASTR transforms it into an opportunity for feedback and improvement, enabling countries to use the data they have to make timely, evidence-based decisions. How was adjustment done? 

The Data Quality Adjustment module uses HMIS data together with the outlier and completeness flags generated by the Data Quality Assessment module, which identify where service volumes are unusually high or missing. Outlier values are replaced with a facility-level rolling average centered on the affected month, calculated over a 12-month window using valid (non-flagged) data. Missing values due to incomplete reporting are imputed using the same method. At the edges of the time series, where a centered window isn't possible, forward or backward rolling averages are used instead. If fewer than six clean months are available within the window, a fallback average at the facility level is used. Adjustments are applied under four scenarios - no adjustment, outlier-only, completeness-only, and both combined - to enable sensitivity analysis. Indicators which don't meet minimum volume thresholds are excluded from adjustment. 

For this analysis, adjustment is (insert here method used: outliers only, both, etc). 
Additionally, maternal deaths and under 5 deaths are NOT adjusted.

## Module 3 - Service Utilization
Service utilization refers to the volume of health services delivered and reported through routine health information systems (i.e. DHIS2). It reflects how populations access and use essential healthcare services over time and across different regions. However, service utilization can fluctuate due to various factors, including seasonal trends, policy changes, pandemics, or other external shocks. Identifying whether these variations are part of normal patterns or signal significant disruptions is important for health system monitoring and decision-making.
The control chart analysis helps determine whether deviations in service volumes are part of normal fluctuations or indicate significant disruptions.
The disruption analysis helps quantify the impact of these disruptions by measuring how service volumes changed during flagged periods.

**Control Chart Analysis**
The control chart analysis identifies abnormal changes in service utilization by comparing actual volumes to expected trends. It begins by aggregating service data at the province level, removing flagged outliers and filling in missing months. Low-volume periods are excluded to improve reliability.
Expected service volumes are modeled using robust regression, accounting for long-term trends and seasonality. These predicted values are then smoothed using a centered 6-month rolling median. Deviations from this expected trend are standardized using the Median Absolute Deviation (MAD), allowing for consistent detection across indicators.
Disruptions are flagged using a rule-based system that detects sharp, sustained, or missing patterns in the data. These include sudden drops or spikes, consistent under- or over-performance, and prolonged gaps in reporting. Tail-end months are always flagged to account for recent unmodeled changes.
All rules are configurable, enabling users to tailor the sensitivity of the detection. The output is a set of tagged months (tagged = 1) stored in `M3_chartout.csv`, used to guide further disruption analysis.

**Disruption Analysis**
After identifying potential disruptions using control charts, the disruption analysis estimates how much service volumes were affected. This is done using regression models that separate normal trends and seasonality from disruption effects.
For each indicator, the model compares service use during disruption months (as tagged in `M3_chartout.csv`) to what would be expected based on historical trends. By controlling for time and seasonal patterns, the model isolates the specific impact of each disruption.
These effects are calculated separately at the province and district levels, allowing users to assess both broad and localized impacts.

## Module 4 – Coverage Estimates
This module estimates health service coverage by integrating adjusted health service volume data, population projections, and survey data (MICS/DHS). Coverage estimates are calculated for key health indicators using multiple denominator sources, and the optimal denominator is selected by minimizing the error relative to survey data. The final estimates are used to analyze trends in service coverage at national and sub-national levels.

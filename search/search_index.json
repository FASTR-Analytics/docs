{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#fastr-rmncah-n-service-use-monitoring-methodology-documentation","title":"FASTR RMNCAH-N Service Use Monitoring: Methodology Documentation","text":""},{"location":"#introduction-to-fastr","title":"Introduction to FASTR","text":"<p>The Global Financing Facility (GFF) supports country-led efforts to improve the timely use of data for decision-making, ultimately leading to stronger primary healthcare (PHC) systems and better reproductive, maternal, newborn, child, and adolescent health and nutrition (RMNCAH-N) outcomes. This set of initiatives and technical support is referred to as Frequent Assessments and Health System Tools for Resilience (FASTR).</p> <p>FASTR encompasses four technical approaches: (1) RMNCAH-N service use monitoring using routine HMIS data, (2) rapid-cycle health facility phone surveys, (3) high-frequency household phone surveys, and (4) follow-on analyses. This methodology documentation focuses specifically on the first approach: RMNCAH-N service use monitoring.</p>"},{"location":"#rmncah-n-service-use-monitoring","title":"RMNCAH-N Service Use Monitoring","text":"<p>The GFF collaborates with Ministries of Health to conduct rapid-cycle analyses of routine health management information system (HMIS) data. This approach addresses three core objectives:</p> <ol> <li>Assess data quality at national and sub-national levels to identify and address completeness, accuracy, and consistency issues</li> <li>Track service utilization changes by measuring monthly shifts in priority RMNCAH-N health service volumes</li> <li>Monitor coverage progress by comparing service delivery trends against country-specific targets and benchmarks</li> </ol> <p>These analyses focus on priority indicators tied to national health reforms and World Bank investments, with findings informing country planning processes and project implementation cycles. During the COVID-19 pandemic, the GFF supported Ministries of Health in over 20 countries to monitor the impact of the pandemic on essential health services using this approach.</p> <p></p> <p>Figure 1. Steps to implement RMNCAH-N service use monitoring</p>"},{"location":"#why-rapid-cycle-analytics","title":"Why rapid-cycle analytics?","text":"<p>Existing health systems data sources are critical but often come with challenges that limit their use. Health management information system data may not be analyzed promptly or may be perceived as too low-quality to use for decision making. Traditional in-person household and facility-based surveys demand extensive resources and time, with long lags between survey design, data collection, and the availability of findings. This prevents decision-makers from using data to drive meaningful improvements in health outcomes. To fill this gap, the GFF supports countries to develop and use rapid-cycle analytic approaches.</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Rapid-cycle analytic approaches provide timely, rigorous, and high-priority data that respond to each country's specific priorities and data use needs. This continuous cycle of analyze-learn-strengthen-act seeks to improve the systematic use of data for decision-making towards improved RMNCAH-N outcomes.</p> <p></p> <p>Figure 2. FASTR's rapid-cycle analytics approach: Analyze, learn, strengthen, act</p>"},{"location":"#technical-approaches","title":"Technical Approaches","text":"<p>FASTR's four technical approaches, underpinned by capacity strengthening and data use support, enable countries to use rapid-cycle analytics for strengthening PHC systems and improving RMNCAH-N outcomes through the timely and high-frequency analysis and use of data.</p> <ol> <li> <p>Analysis of routine health management information system (HMIS) data assesses data quality, quantifies changes in priority health service volumes, and compares trends in service coverage to country targets for priority RMNCAH-N indicators.</p> </li> <li> <p>Rapid-cycle health facility phone surveys assess the performance of PHC facilities, monitor the implementation of reforms, identify the impact of shocks, and track changes over time. The phone survey is administered to a representative panel sample of PHCs over four quarterly contacts a year.</p> </li> <li> <p>High-frequency household phone surveys provide a snapshot of care seeking behavior, foregone care, financial protection, service coverage, and patient experience of care. Household surveys are currently done in partnership with the World Bank's Living Standards Measurement Study.</p> </li> <li> <p>Follow-on analyses employ root cause analysis and implementation research approaches to provide deeper understanding of issues uncovered by rapid-cycle analytics (e.g., explaining district-level performance variation, contextualizing the impact of health systems reforms, or investigating underlying causes of data quality issues and service delivery disruptions).</p> </li> </ol> <p>Illustrative capacity-building activities include support to automate the extraction, cleaning, and analysis of routine data and support to institutionalize rapid phone survey data collection and analysis approaches. Data use support prioritizes the integration of rapid-cycle analytics into existing data review and feedback mechanisms at national and subnational levels to strengthen the systematic use of data for decision making.</p> <p></p> <p>Figure 3. Rapid-cycle analytics under the Frequent Assessments and System Tools for Resilience (FASTR) initiative</p>"},{"location":"#acronyms-and-abbreviations","title":"Acronyms and Abbreviations","text":"Acronym Definition FASTR Frequent Assessments and Health System Tools for Resilience GFF Global Financing Facility HMIS Health Management Information System LMIC Low- and Middle-Income Country PHC Primary Healthcare RMNCAH+N Reproductive, Maternal, Newborn, Child, and Adolescent Health and Nutrition SSA Sub-Saharan Africa"},{"location":"#use-of-routine-health-management-information-system-data-in-lmics","title":"Use of Routine Health Management Information System Data in LMICs","text":"<p>Health-facility data collected through routine health management information systems (HMIS) constitute a primary data source for assessing the performance of the health sector. HMIS health data are widely used for a variety of purposes including health sector reviews, planning and resource allocation, program monitoring, health care quality improvement, and reporting purposes. Ministries of Health in low- and middle-income countries (LMICs) are striving to ensure equitable access to quality health services and care towards attaining universal health coverage and other national strategies. The efforts can be more successful if decision making at all levels of the sector are well informed by timely, reliable, and comprehensive data gathered through a well-established health information system. Sound decisions are based on sound data; therefore, it is essential to ensure that the data are of good quality.</p> <p>Poor-quality data impact various levels of the health system in different ways. For program managers, inaccurate data can lead to poor decisions that harm the program's operations and, ultimately, the population's health. At the planning level, poor-quality data can distort evidence of progress towards health-sector goals and hinder annual planning processes by providing misleading results. Additionally, when determining investments in the health sector, poor-quality data can lead to poor targeting of resources. Despite HMIS data being crucial for robust health systems, studies in Sub-Saharan Africa (SSA) have reported challenges with data quality, including issues with completeness, timeliness, accuracy, and consistency (AbouZahr &amp; Boerma, 2005; Amoakoh-Coleman et al., 2015; Belay &amp; Lippeveld, 2013; Gimbel et al., 2011; Mavimbe et al., 2005; Mouk\u00e9net et al., 2021; Mutale et al., 2013; Rowe, 2009; Sychareun et al., 2014; Teklegiorgis et al., 2016)<sup>1</sup> <sup>2</sup> <sup>3</sup> <sup>4</sup> <sup>5</sup> <sup>6</sup> <sup>7</sup> <sup>8</sup> <sup>9</sup> <sup>10</sup>. These concerns about the quality of routine information have undermined its use in decision-making within the health sector (Belay &amp; Lippeveld, 2013; Bhattacharya et al., 2019; Chen et al., 2014; Endriyas et al., 2019; Gl\u00e8l\u00e8 Ahanhanzo et al., 2014; Mutale et al., 2013; Mwangu, 2005; Nshimyiryo et al., 2020; O'Hagan et al., 2017; Ouedraogo et al., 2019; Rowe et al., 2009; Xiao et al., 2017)<sup>4</sup> <sup>9</sup> <sup>11</sup> <sup>12</sup> <sup>13</sup> <sup>14</sup> <sup>15</sup> <sup>16</sup> <sup>17</sup> <sup>18</sup> <sup>19</sup> <sup>20</sup>. However, in recent years, countries have made substantial improvements in HMIS data quality which has been reinforced by a system of data quality assessment, data quality improvement, and data use for evidence-based decision making (Mphatswe et al., 2012; Nisingizwe et al., 2014; Wagenaar et al., 2015)<sup>21</sup> <sup>22</sup> <sup>23</sup>.</p>"},{"location":"#defining-data-quality","title":"Defining Data Quality","text":"<p>Defining data quality is complex, and while there is no one single definition of data quality, there are four dimensions most frequently used to describe it: completeness, timeliness, consistency, and accuracy (World Health Organization, 2017)<sup>24</sup>.</p>"},{"location":"#data-quality-dimensions-and-assessment","title":"Data Quality Dimensions and Assessment","text":"Data Quality Domain What does it measure? How is it assessed? Completeness Are all data present? Is there sufficient information available to make decisions about the health of the population and to target resources to improve health-system coverage, efficiency and quality? \u2022 Assessed by measuring whether all units that are supposed to report actually do (reporting completeness)\u2022 Assessed by measuring the completeness of indicator data (no missing values); this is different from overall reporting completeness in that it looks at completeness of specific data elements and not only at the receipt of the monthly reporting form (indicator completeness) Timeliness Are data regularly submitted on time? \u2022 Assessed by measuring whether the units that submitted reports did so before a set deadline (timeliness) Consistency Are data plausible in view of what has been previously reported? \u2022 Trends are evaluated to determine whether reported values are extreme relative to other values reported during the year or across several years (presence of outliers)\u2022 Assess trends in program indicators to determine whether reported values are extreme in relation to other values that are reported during the year or over several years (consistency over time)\u2022 Assess program indicators which have a predictable relationship to determine whether the expected relationship exists between those 2 indicators (consistency between related indicators)\u2022 Assess the level of agreement between two sources of data measuring the same health indicator; the two sources of data that are usually compared are data flowing through the HMIS and data from a periodic population-based survey (external comparison with other data sources)\u2022 Determine the adequacy of the population data used in evaluating the performance of health indicators by comparing two different sources of related population estimates for congruence (consistency of population data) Accuracy Do data faithfully reflect the actual level of service delivery conducted in the health facility? \u2022 Assess the accuracy for selected indicators through the review of source documents in health facilities and comparison to monthly reports and HMIS values (consistency of reported data and original records, data verification factor)"},{"location":"#fastr-approach-to-routine-data-analysis","title":"FASTR Approach to Routine Data Analysis","text":"<p>The FASTR approach to routine data analysis takes a three-pronged approach:</p> <ol> <li>Identify issues in data quality</li> <li>Adjust for issues with data quality to improve analysis accuracy</li> <li>Analyze data to answer pressing country-specific policy questions including identifying changes in priority service volumes and trends in service coverage as compared to country priorities and targets</li> </ol> <p>This approach enables identification of the highest priority data quality issues and subsequent necessary analytical adjustments so that data can be continually improved while appropriate analyses are conducted. Data quality assessment is conducted by indicator and can be disaggregated at sub-national level given facility-level data is used for the analysis. This is important to generate policy relevant regular reporting on data quality, service volume, and coverage estimates which provides a continual snapshot of RMNCAH-N service use.</p>"},{"location":"#focus-on-a-set-of-core-indicators","title":"Focus on a Set of Core Indicators","text":"<p>The FASTR approach to routine data analysis focuses on a core set of RMNCAH-N indicators that characterize the reproductive, maternal and child healthcare continuum, priority health areas across LMICs. These indicators capture key service delivery events, which have higher completeness rates and higher volume. In addition, these indicators serve as proxies for other services and interventions delivered at the same service contact. In addition, outpatient consultations (OPDs) are used as a proxy for the general use of health services. Additional, country and program-specific indicators can be added to the analysis to be responsive to country priorities.</p>"},{"location":"#focus-on-a-set-of-core-data-quality-metrics","title":"Focus on a Set of Core Data Quality Metrics","text":"<p>The FASTR approach to routine data analysis focuses on a core set of data quality metrics which enables identification of the highest priority data quality issues for which data quality adjustments can be made. In addition to the core data quality measures, the FASTR approach generates an overall data quality score which combines the core metrics into a single summary measure.</p>"},{"location":"#results-communication-and-data-use","title":"Results Communication and Data Use","text":"<p>During the COVID-19 pandemic, the GFF supported Ministries of Health in over 20 countries to monitor the impact of the pandemic on essential health services.</p> <ul> <li>Report: COVID-19: Impact on Essential Health Services</li> <li>Article: Healthcare utilization and maternal and child mortality during the COVID-19 pandemic in 18 low- and middle-income countries</li> <li>Article: Vaccination utilization and subnational inequities during the COVID-19 pandemic</li> <li>Article: Disruptions in maternal and child health service utilization during COVID-19: analysis from eight sub-Saharan African countries</li> </ul> <p>More results and reports can be found in the FASTR Resource Repository.</p>"},{"location":"#what-this-documentation-covers","title":"What This Documentation Covers","text":"<p>This methodology documentation describes the FASTR approach to routine HMIS data analysis. FASTR uses four code modules to automate different parts of the analysis:</p> <ul> <li>Module 1 \u00b7 Data Quality Assessment - Automated assessment of HMIS data quality through completeness, timeliness, consistency, and accuracy metrics</li> <li>Module 2 \u00b7 Data Quality Adjustments - Automated techniques for improving data accuracy by identifying and adjusting for data quality issues</li> <li>Module 3 \u00b7 Service Utilization - Automated analysis of health service usage patterns to identify changes in priority service volumes</li> <li>Module 4 \u00b7 Coverage Estimates - Automated methods for estimating service coverage and comparing trends to country targets</li> </ul>"},{"location":"#references","title":"References","text":"<ol> <li> <p>AbouZahr, C., &amp; Boerma, T. (2005). Health information systems: The foundations of public health. Bulletin of the World Health Organization, 83(8), 578--583.\u00a0\u21a9</p> </li> <li> <p>Mavimbe, J. C., Braa, J., &amp; Bjune, G. (2005). Assessing immunization data quality from routine reports in Mozambique. BMC Public Health, 5, 108. https://doi.org/10.1186/1471-2458-5-108 \u21a9</p> </li> <li> <p>Sychareun, V., Hansana, V., Phengsavanh, A., Chaleunvong, K., Eunyoung, K., &amp; Durham, J. (2014). Data verification at health centers and district health offices in Xiengkhouang and Houaphanh Provinces, Lao PDR. BMC Health Services Research, 14, 255. https://doi.org/10.1186/1472-6963-14-255 \u21a9</p> </li> <li> <p>Mutale, W. et al. (2013). Improving health information systems for decision making across five sub-Saharan African countries: Implementation strategies from the African Health Initiative. BMC Health Services Research, 13(2), S9. https://doi.org/10.1186/1472-6963-13-S2-S9 \u21a9\u21a9</p> </li> <li> <p>Amoakoh-Coleman, M. et al. (2015). Completeness and accuracy of data transfer of routine maternal health services data in the greater Accra region. BMC Research Notes, 8, 114. https://doi.org/10.1186/s13104-015-1058-3 \u21a9</p> </li> <li> <p>Gimbel, S. et al. (2011). An assessment of routine primary care health information system data quality in Sofala Province, Mozambique. Population Health Metrics, 9, 12. https://doi.org/10.1186/1478-7954-9-12 \u21a9</p> </li> <li> <p>Teklegiorgis, K., Tadesse, K., Terefe, W., &amp; Mirutse, G. (2016). Level of data quality from Health Management Information Systems in a resources limited setting and its associated factors, eastern Ethiopia. South African Journal of Information Management, 18(1), 1--8. https://doi.org/10.4102/sajim.v18i1.612 \u21a9</p> </li> <li> <p>Rowe, A. K. (2009). Potential of integrated continuous surveys and quality management to support monitoring, evaluation, and the scale-up of health interventions in developing countries. American Journal of Tropical Medicine and Hygiene, 80(6), 971. https://doi.org/10.4269/ajtmh.2009.80.971 \u21a9</p> </li> <li> <p>Belay, H., &amp; Lippeveld, T. (2013). Inventory of PRISM framework and tools: Application of PRISM tools and interventions for strengthening routine health information system performance (Working Paper Series WP-13-138). MEASURE Evaluation, Carolina Population Center. https://www.measureevaluation.org/resources/publications/wp-13-138/at\\_download/document \u21a9\u21a9</p> </li> <li> <p>Mouk\u00e9net, A. et al. (2021). Health management information system (HMIS) data quality and associated factors in Massaguet district, Chad. BMC Medical Informatics and Decision Making, 21(1), 326. https://doi.org/10.1186/s12911-021-01684-7 \u21a9</p> </li> <li> <p>Xiao, Y. et al. (2017). Challenges in data quality: The influence of data quality assessments on data availability and completeness in a voluntary medical male circumcision programme in Zimbabwe. BMJ Open, 7(1), e013562.\u00a0\u21a9</p> </li> <li> <p>O'Hagan, R. et al. (2017). National assessment of data quality and associated systems-level factors in Malawi. Global Health Science and Practice, 5(3), 367--381. https://doi.org/10.9745/GHSP-D-17-00177 \u21a9</p> </li> <li> <p>Chen, H., Hailey, D., Wang, N., &amp; Yu, P. (2014). A review of data quality assessment methods for public health information systems. International Journal of Environmental Research and Public Health, 11(5), 5170--5207. https://doi.org/10.3390/ijerph110505170 \u21a9</p> </li> <li> <p>Gl\u00e8l\u00e8 Ahanhanzo, Y., Ouedraogo, L. T., Kpoz\u00e8houen, A., Coppieters, Y., Makoutod\u00e9, M., &amp; Wilmet-Dramaix, M. (2014). Factors associated with data quality in the routine health information system of Benin. Archives of Public Health, 72(1), 25. https://doi.org/10.1186/2049-3258-72-25 \u21a9</p> </li> <li> <p>Bhattacharya, A. A. et al. (2019). Quality of routine facility data for monitoring priority maternal and newborn indicators in DHIS2: A case study from Gombe State, Nigeria. PLoS ONE, 14(1), e0211265. https://doi.org/10.1371/journal.pone.0211265 \u21a9</p> </li> <li> <p>Nshimyiryo, A. et al. (2020). Health management information system (HMIS) data verification: A case study in four districts in Rwanda. PLoS ONE, 15(7), e0235823. https://doi.org/10.1371/journal.pone.0235823 \u21a9</p> </li> <li> <p>Ouedraogo, M. et al. (2019). A quality assessment of Health Management Information System (HMIS) data for maternal and child health in Jimma Zone, Ethiopia. PLoS ONE, 14(3), e0213600. https://doi.org/10.1371/journal.pone.0213600 \u21a9</p> </li> <li> <p>Endriyas, M. et al. (2019). Understanding performance data: Health management information system data accuracy in Southern Nations Nationalities and [People's Region]{.nocase}, Ethiopia. BMC Health Services Research, 19, 1--6. https://doi.org/10.1186/s12913-019-3991-7 \u21a9</p> </li> <li> <p>Mwangu, M. (2005). Quality of a routine data collection system for health: Case of Kinondoni district in the [Dar es Salaam]{.nocase} region, Tanzania. South African Journal of Information Management, 7(2).\u00a0\u21a9</p> </li> <li> <p>Rowe, A. K., Kachur, S. P., Yoon, S. S., Lynch, M., Slutsker, L., &amp; Steketee, R. W. (2009). Caution is required when using health facility-based data to evaluate the health impact of malaria control efforts in Africa. Malaria Journal, 8(1), 209. https://doi.org/10.1186/1475-2875-8-209 \u21a9</p> </li> <li> <p>Nisingizwe, M. P. et al. (2014). Toward utilization of data for program management and evaluation: Quality assessment of five years of health management information system data in Rwanda. Global Health Action, 7(1), 25829. https://doi.org/10.3402/gha.v7.25829 \u21a9</p> </li> <li> <p>Wagenaar, B. H., Sherr, K., Fernandes, Q., &amp; Wagenaar, A. C. (2015). Using routine health information systems for well-designed health evaluations in low- and middle-income countries. Health Policy and Planning, czv029. https://doi.org/10.1093/heapol/czv029 \u21a9</p> </li> <li> <p>Mphatswe, W. et al. (2012). Improving public health information: A data quality intervention in KwaZulu-Natal, South Africa. Bulletin of the World Health Organization, 90(3), 176--182. https://doi.org/10.2471/blt.11.092759 \u21a9</p> </li> <li> <p>World Health Organization. (2017). Data quality assurance: A toolkit for facility data quality assessment: Module 1: Framework and metrics. World Health Organization. https://iris.who.int/bitstream/handle/10665/366086/9789240047358-eng.pdf?sequence=1 \u21a9</p> </li> </ol>"},{"location":"00_background_documentation/","title":"Background Documentation","text":"<p>This section contains background information.</p>"},{"location":"01_module_data_quality_assessment_documentation/","title":"Module 1: Data Quality Assessment (DQA)","text":""},{"location":"01_module_data_quality_assessment_documentation/#1-overview-what-why","title":"1. Overview (What &amp; Why)","text":""},{"location":"01_module_data_quality_assessment_documentation/#what-does-this-module-do","title":"What does this module do?","text":"<p>The Data Quality Assessment (DQA) module evaluates the reliability of Health Management Information System (HMIS) data from health facilities. It acts as a quality control checkpoint in the FASTR pipeline, examining monthly facility reports to identify data issues before the information is used for decision-making.</p> <p>The module assesses data quality through three complementary lenses: detecting outliers (unusually high values that may indicate reporting errors), assessing completeness (whether facilities consistently submit their reports), and measuring consistency (whether related health indicators align with expected patterns). These assessments are combined into an overall DQA score that provides a single measure of data reliability.</p> <p>Routinely reported health facility data are an important source for health indicators at the facility and population levels. Health facilities report on events such as immunizations given or live births attended by a skilled provider. As with any data, quality is an issue. The FASTR approach conducts an analysis of monthly data by facility and by indicator to assess data quality. Results are presented as annual estimates but may comprise a partial year of data given the availability of data at the time the analysis is conducted (e.g., an analysis conducted in June 2024 may contain data from January-May 2024, and this will be presented as the analysis for 2024).</p>"},{"location":"01_module_data_quality_assessment_documentation/#why-is-it-needed-in-the-fastr-pipeline","title":"Why is it needed in the FASTR pipeline?","text":"<p>Data quality directly impacts the reliability of health indicators and coverage estimates. Before calculating service utilization rates or estimating population coverage, we must ensure the underlying facility data is trustworthy. This module identifies problematic data patterns that could skew results, allowing analysts to make informed decisions about data adjustments or exclusions in subsequent pipeline steps.</p>"},{"location":"01_module_data_quality_assessment_documentation/#quick-summary","title":"Quick Summary","text":"Component Details Inputs Raw HMIS data (<code>hmis_ISO3.csv</code>) containing facility service volumes by month and indicatorGeographic/administrative area identifiersStandardized indicator names Outputs - Outlier flags and lists- Completeness status by facility-indicator-month- Consistency results at geographic level- Overall DQA scores Purpose Evaluate HMIS data reliability through outlier detection, completeness assessment, and consistency checking to ensure trustworthy inputs for coverage estimation"},{"location":"01_module_data_quality_assessment_documentation/#2-how-it-works","title":"2. How It Works","text":""},{"location":"01_module_data_quality_assessment_documentation/#high-level-workflow","title":"High-Level Workflow","text":"<p>The module follows a logical sequence of quality checks, building from individual data points to an overall quality score:</p> <p>Step 1: Load and Prepare Data The module reads monthly facility reports and organizes them for analysis. It converts dates to a standard format and identifies which geographic areas and health indicators are present in the dataset.</p> <p>Step 2: Detect Outliers For each health facility and indicator (like pentavalent vaccine doses or antenatal care visits), the module identifies unusually high values that might indicate data entry errors. It uses two methods: statistical outliers (values far from the facility's typical volume) and proportional outliers (a single month accounting for most of the year's services).</p> <p>Step 3: Assess Completeness The module checks whether facilities are consistently reporting data. It creates a complete timeline for each facility and indicator, identifying months with missing reports. Facilities that stop reporting for 6+ months are flagged as inactive rather than incomplete.</p> <p>Step 4: Measure Consistency Related indicators should follow predictable patterns. For example, more women should receive their first antenatal visit (ANC1) than their fourth visit (ANC4). The module calculates ratios between paired indicators at the district level (to account for patients visiting multiple facilities) and flags relationships that don't meet expectations.</p> <p>Step 5: Validate Indicator Availability Before running consistency checks, the module verifies that the required indicator pairs actually exist in the dataset. Missing indicators are handled gracefully, with the analysis adapting to available data.</p> <p>Step 6: Calculate DQA Scores For a defined set of core indicators (typically pentavalent vaccine, first antenatal visit, and outpatient visits), the module combines the three quality dimensions. A facility-month receives a perfect DQA score only if all core indicators are complete, free of outliers, and meet consistency benchmarks.</p> <p>Step 7: Export Results The module produces several output files containing outlier lists, completeness flags, consistency results, and final DQA scores. These outputs inform subsequent modules and provide actionable insights for data quality improvement.</p>"},{"location":"01_module_data_quality_assessment_documentation/#workflow-diagram","title":"Workflow Diagram","text":""},{"location":"01_module_data_quality_assessment_documentation/#key-decision-points","title":"Key Decision Points","text":"<p>When is a value considered an outlier? An observation is flagged as an outlier if it meets EITHER of two criteria AND the count exceeds 100: - It's more than 10 times the Median Absolute Deviation (MAD) away from the facility's typical volume, OR - It represents more than 80% of that facility's annual total for that indicator</p> <p>Why measure consistency at the district level instead of facility level? Patients often visit different facilities within their local district for different services. A woman might get her first antenatal visit at one health center but deliver at a district hospital. Measuring consistency at the district level accounts for this patient movement and provides a more accurate picture of service utilization patterns.</p> <p>What happens when required indicators are missing? The module adapts to available data. If consistency pairs cannot be evaluated, the DQA score is calculated using only completeness and outlier checks. The analysis continues with the dimensions that can be assessed.</p> <p>How are inactive facilities handled? If a facility doesn't report for 6 or more consecutive months at the start or end of their reporting period, those months are flagged as \"inactive\" rather than \"incomplete.\" This prevents penalizing facilities that haven't yet started reporting or have permanently closed.</p>"},{"location":"01_module_data_quality_assessment_documentation/#what-happens-to-the-data","title":"What Happens to the Data","text":"<p>Transformation Overview:</p> <p>The module transforms raw facility reports into quality-flagged datasets:</p> <ol> <li>Input Format: Monthly rows with facility ID, period, indicator name, and count</li> <li>Enrichment: Adds calculated fields like median volume, MAD residuals, proportional contributions</li> <li>Completion: Generates explicit rows for missing months (turning implicit gaps into explicit records)</li> <li>Aggregation: Rolls up facility data to district level for consistency calculations</li> <li>Flagging: Adds binary quality flags (outlier yes/no, complete yes/no, consistent yes/no)</li> <li>Scoring: Combines flags into continuous scores (0-1) and binary pass/fail indicators</li> <li>Output Format: Multiple files optimized for different use cases (quick outlier review, full analysis, downstream modules)</li> </ol> <p>The data flows through the module in long format (one row per facility-indicator-period combination) and emerges with quality dimensions that subsequent modules use to weight, adjust, or exclude observations.</p>"},{"location":"01_module_data_quality_assessment_documentation/#analysis-outputs-and-visualization","title":"Analysis Outputs and Visualization","text":"<p>The FASTR analysis generates six main visual outputs:</p> <p>1. Outliers Heatmap</p> <p>Heatmap table with zones as rows and health indicators as columns, color-coded by outlier percentage.</p> <p></p> <p>2. Indicator Completeness</p> <p>Heatmap table with zones as rows and health indicators as columns, color-coded by completeness percentage.</p> <p></p> <p>3. Indicator Completeness Over Time</p> <p>Horizontal timeline charts showing completeness trends for each indicator over the analysis period.</p> <p></p> <p>4. Internal Consistency</p> <p>Heatmap table with zones as rows and consistency benchmark categories as columns, color-coded by performance.</p> <p></p> <p>5. Overall DQA Score</p> <p>Heatmap table with zones as rows and time periods as columns, color-coded by DQA score percentage.</p> <p></p> <p>6. Mean DQA Score</p> <p>Heatmap table with zones as rows and time periods as columns, color-coded by average DQA score.</p> <p></p> <p>Color Coding System: - Green: 90% or above (completeness/consistency), Below 1% (outliers) - Yellow: 80% to 89% (completeness), 1% to 2% (outliers) - Red: Below 80% (completeness/consistency), 3% or above (outliers)</p>"},{"location":"01_module_data_quality_assessment_documentation/#3-detailed-reference","title":"3. Detailed Reference","text":"<p>This section provides technical details for implementers, developers, and analysts who need to understand the underlying methodology.</p>"},{"location":"01_module_data_quality_assessment_documentation/#configuration-parameters","title":"Configuration Parameters","text":"<p>The module uses several configurable parameters that control analysis behavior:</p> Geographic Settings <pre><code># Country identifier\nCOUNTRY_ISO3 &lt;- \"GIN\"  # ISO3 country code\n\n# Geographic level for consistency analysis\nGEOLEVEL &lt;- \"admin_area_3\"  # Admin level (1=national, 2=region, 3=district, etc.)\n</code></pre> <p>The <code>GEOLEVEL</code> parameter determines the aggregation level for consistency analysis. Lower administrative levels (3-4) capture local patterns but may have sparse data. Higher levels (2) provide more stable estimates but may mask local inconsistencies.</p> Outlier Detection Parameters <pre><code># Proportion threshold for outlier detection\nOUTLIER_PROPORTION_THRESHOLD &lt;- 0.8  # Flag if single month &gt; 80% of annual total\n\n# Minimum count to consider for outlier flagging\nMINIMUM_COUNT_THRESHOLD &lt;- 100  # Only flag outliers with count &gt;= 100\n\n# Number of Median Absolute Deviations for statistical outlier detection\nMADS &lt;- 10  # Flag if value &gt; 10 MADs from median\n</code></pre> <p>Tuning Guidance: - More sensitive detection: Lower <code>OUTLIER_PROPORTION_THRESHOLD</code> to 0.6-0.7, reduce <code>MADS</code> to 8 - Less sensitive detection: Increase <code>OUTLIER_PROPORTION_THRESHOLD</code> to 0.9, increase <code>MADS</code> to 12-15 - Small facilities: Lower <code>MINIMUM_COUNT_THRESHOLD</code> to 50 - Large facilities only: Increase <code>MINIMUM_COUNT_THRESHOLD</code> to 200+</p> DQA Indicator Selection <pre><code># Core indicators used for DQA scoring\nDQA_INDICATORS &lt;- c(\"penta1\", \"anc1\", \"opd\")\n\n# Consistency pairs to evaluate\nCONSISTENCY_PAIRS_USED &lt;- c(\"penta\", \"anc\")\n</code></pre> <p>Standard Indicator Sets: - Maternal-child focus: <code>c(\"anc1\", \"anc4\", \"delivery\", \"penta1\", \"penta3\")</code> - Immunization focus: <code>c(\"bcg\", \"penta1\", \"penta3\", \"measles1\")</code> - Comprehensive: <code>c(\"penta1\", \"anc1\", \"opd\", \"delivery\", \"pnc1\")</code></p> Consistency Benchmark Ranges <pre><code>all_consistency_ranges &lt;- list(\n  pair_penta    = c(lower = 0.95, upper = Inf),  # Penta1 &gt;= 0.95 * Penta3\n  pair_anc      = c(lower = 0.95, upper = Inf),  # ANC1 &gt;= 0.95 * ANC4\n  pair_delivery = c(lower = 0.7, upper = 1.3),   # 0.7 &lt;= BCG/Delivery &lt;= 1.3\n  pair_malaria  = c(lower = 0.9, upper = 1.1)    # Malaria indicators within 10%\n)\n</code></pre> <p>The ranges reflect programmatic expectations. For example, ANC1 should always be at least 95% of ANC4 (more women start care than complete four visits). The 5% tolerance accounts for data entry variations. BCG, as a birth dose vaccine, should approximately equal facility deliveries, with 30% tolerance for variation.</p>"},{"location":"01_module_data_quality_assessment_documentation/#inputoutput-specifications","title":"Input/Output Specifications","text":""},{"location":"01_module_data_quality_assessment_documentation/#input-file-structure","title":"Input File Structure","text":"<p>Required File: <code>hmis_[COUNTRY_ISO3].csv</code></p> <p>Required Columns: - <code>facility_id</code> (character/integer): Unique identifier for each health facility - <code>period_id</code> (integer): Time period in YYYYMM format (e.g., 202401 for January 2024) - <code>indicator_common_id</code> (character): Standardized indicator names (e.g., \"penta1\", \"anc1\", \"opd\") - <code>count</code> (numeric): Service volume or count for the indicator - <code>admin_area_1</code> through <code>admin_area_8</code> (character): Geographic/administrative area columns</p> <p>Format Example:</p> <pre><code>facility_id,period_id,indicator_common_id,count,admin_area_1,admin_area_2,admin_area_3\nFAC001,202401,penta1,45,Country_A,Province_A,District_A\nFAC001,202401,anc1,67,Country_A,Province_A,District_A\nFAC001,202402,penta1,52,Country_A,Province_A,District_A\n</code></pre> <p>Data Requirements: - At least 12 months of data recommended for robust outlier detection - Missing values represented as NA or absent rows (both handled) - Zero counts should be explicit zeros, not missing - Geographic columns detected automatically (columns 2-8 are optional)</p>"},{"location":"01_module_data_quality_assessment_documentation/#output-files","title":"Output Files","text":"M1_output_outlier_list.csv - Flagged Outliers Only <p>Purpose: Quick reference list of only the observations flagged as outliers</p> <p>Columns:</p> <ul> <li><code>facility_id</code>: Facility identifier</li> <li><code>admin_area_[2-8]</code>: Geographic areas (dynamically included based on data)</li> <li><code>indicator_common_id</code>: Health indicator name</li> <li><code>period_id</code>: Time period (YYYYMM)</li> <li><code>year</code>, <code>month</code>: Extracted time components</li> <li><code>count</code>: Reported service volume</li> <li><code>median_volume</code>: Facility-indicator median</li> <li><code>mad_volume</code>: Median Absolute Deviation</li> <li><code>mad_residual</code>: Standardized residual</li> <li><code>pc</code>: Proportion of annual total</li> <li><code>outlier_mad</code>: MAD-based outlier flag</li> <li><code>outlier_pc</code>: Proportion-based outlier flag</li> <li><code>outlier_flag</code>: Final combined outlier flag</li> </ul> <p>Use Case: Data managers reviewing specific outliers for investigation or correction</p> M1_output_outliers.csv - All Records with Outlier Flags <p>Purpose: Complete dataset with outlier metrics for all facility-indicator-period combinations</p> <p>Columns: Same as outlier_list.csv but includes all observations</p> <p>Use Case:</p> <ul> <li>Input for Module 2 (Data Quality Adjustments)</li> <li>Statistical analysis of outlier patterns</li> <li>Generating visualizations of outlier prevalence</li> </ul> M1_output_completeness.csv - Completeness Status <p>Purpose: Completeness flags for all facility-indicator-period combinations, including explicitly created records for missing months</p> <p>Columns:</p> <ul> <li><code>facility_id</code>: Facility identifier</li> <li><code>admin_area_[2-8]</code>: Geographic areas</li> <li><code>indicator_common_id</code>: Health indicator name</li> <li><code>period_id</code>: Time period (YYYYMM)</li> <li><code>year</code>, <code>month</code>, <code>quarter_id</code>: Time components</li> <li><code>count</code>: Reported volume (NA if missing)</li> <li><code>completeness_flag</code>: 0=Incomplete, 1=Complete, 2=Inactive (removed from output)</li> </ul> <p>Special Features:</p> <ul> <li>Contains explicit rows for non-reporting months</li> <li>Inactive periods (6+ months at start/end) excluded</li> <li>Full time series for each facility-indicator combination</li> </ul> <p>Use Case:</p> <ul> <li>Calculating completeness percentages</li> <li>Identifying reporting gaps</li> <li>Trend analysis of reporting behavior</li> </ul> M1_output_consistency_geo.csv - Geographic-Level Consistency <p>Purpose: Consistency ratios calculated at the specified geographic level (e.g., district)</p> <p>Columns:</p> <ul> <li><code>admin_area_1</code> through <code>admin_area_[X]</code>: Geographic identifiers up to specified GEOLEVEL</li> <li><code>period_id</code>: Time period (YYYYMM)</li> <li><code>ratio_type</code>: Name of consistency pair (e.g., \"pair_penta\", \"pair_anc\")</li> <li><code>consistency_ratio</code>: Calculated ratio value</li> <li><code>sconsistency</code>: Binary flag (1=consistent, 0=inconsistent, NA=cannot calculate)</li> </ul> <p>Format: Long format with one row per geographic area-period-ratio type</p> <p>Use Case:</p> <ul> <li>Understanding district-level service delivery patterns</li> <li>Identifying geographic areas with consistency issues</li> <li>Creating consistency heatmaps by zone</li> </ul> M1_output_consistency_facility.csv - Facility-Level Consistency <p>Purpose: Geographic consistency results expanded to facility level, pivoted to wide format</p> <p>Columns:</p> <ul> <li><code>facility_id</code>: Facility identifier</li> <li><code>period_id</code>: Time period</li> <li><code>pair_[X]</code>: One column per consistency pair with flag values</li> </ul> <p>Format: Wide format with one row per facility-period</p> <p>Use Case:</p> <ul> <li>Input for DQA scoring</li> <li>Merging consistency flags with facility-level analyses</li> <li>Facility-specific quality reports</li> </ul> M1_output_dqa.csv - Final DQA Scores <p>Purpose: Composite data quality scores by facility and time period</p> <p>Columns:</p> <ul> <li><code>facility_id</code>: Facility identifier</li> <li><code>admin_area_[2-8]</code>: Geographic areas</li> <li><code>period_id</code>: Time period (YYYYMM)</li> <li><code>completeness_outlier_score</code>: Proportion of DQA indicators passing completeness &amp; outlier checks (0-1)</li> <li><code>consistency_score</code>: Proportion of consistency pairs passing benchmarks (0-1, or NA if no pairs)</li> <li><code>dqa_mean</code>: Average of component scores (0-1)</li> <li><code>dqa_score</code>: Binary overall pass/fail (1 = all checks passed, 0 = any check failed)</li> </ul> <p>Use Case:</p> <ul> <li>Filtering data for subsequent modules (e.g., only use facility-months with dqa_score=1)</li> <li>Tracking data quality trends over time</li> <li>Identifying facilities needing data quality improvement support</li> </ul>"},{"location":"01_module_data_quality_assessment_documentation/#key-functions-documentation","title":"Key Functions Documentation","text":"load_and_preprocess_data() <p>Signature: <code>load_and_preprocess_data(file_path)</code></p> <p>Purpose: Loads HMIS data and prepares it for analysis by creating necessary date fields and composite indicators</p> <p>Parameters:</p> <ul> <li><code>file_path</code> (character): Path to HMIS CSV file</li> </ul> <p>Returns: List containing:</p> <ul> <li><code>data</code>: Preprocessed dataframe with date field added</li> <li><code>geo_cols</code>: Vector of detected geographic column names</li> </ul> <p>Process: 1. Reads CSV file with HMIS data 2. Converts <code>period_id</code> (YYYYMM format) to Date objects for temporal ordering 3. Detects all administrative area columns (admin_area_1 through admin_area_8) 4. Creates composite malaria indicator if component indicators exist:    - Combines <code>rdt_positive</code> + <code>micro_positive</code> into <code>rdt_positive_plus_micro</code>    - This composite is used for malaria consistency checks</p> <p>Example:</p> <pre><code>inputs &lt;- load_and_preprocess_data(\"hmis_ISO3.csv\")\ndata &lt;- inputs$data\ngeo_cols &lt;- inputs$geo_cols\n</code></pre> validate_consistency_pairs() <p>Signature: <code>validate_consistency_pairs(consistency_params, data)</code></p> <p>Purpose: Validates that required indicator pairs exist in the dataset before running consistency analysis</p> <p>Parameters:</p> <ul> <li><code>consistency_params</code>: List containing consistency_pairs and consistency_ranges</li> <li><code>data</code>: The HMIS dataset</li> </ul> <p>Returns: Updated consistency_params with only valid pairs (empty list if no valid pairs)</p> <p>Process: 1. Checks which indicators are available in the dataset 2. Removes consistency pairs where one or both indicators are missing 3. Issues warnings about removed pairs 4. Returns empty list if no valid pairs remain</p> <p>Example Output:</p> <pre><code>Warning: Skipping pair_delivery - indicator 'delivery' not found in data\nWarning: Skipping pair_malaria - indicator 'rdt_positive_plus_micro' not found in data\nRemaining consistency pairs: pair_penta, pair_anc\n</code></pre> outlier_analysis() <p>Signature: <code>outlier_analysis(data, geo_cols, outlier_params)</code></p> <p>Purpose: Identifies statistical outliers in facility service volumes using dual detection methods</p> <p>Parameters:</p> <ul> <li><code>data</code>: HMIS data with facility_id, indicator_common_id, period_id, count</li> <li><code>geo_cols</code>: Vector of geographic column names</li> <li><code>outlier_params</code>: List containing:</li> <li><code>outlier_pc_threshold</code>: Proportion threshold (default 0.8)</li> <li><code>count_threshold</code>: Minimum count threshold (default 100)</li> </ul> <p>Returns: Dataframe with outlier flags and diagnostic metrics for each facility-indicator-period</p> <p>Calculated Fields:</p> <ul> <li><code>median_volume</code>: Median count by facility-indicator</li> <li><code>mad_volume</code>: MAD calculated on values &gt;= median</li> <li><code>mad_residual</code>: Standardized residual (|count - median| / MAD)</li> <li><code>outlier_mad</code>: Binary flag (1 if mad_residual &gt; MADS)</li> <li><code>pc</code>: Proportional contribution to annual total</li> <li><code>outlier_pc</code>: Binary flag (1 if pc &gt; threshold)</li> <li><code>outlier_flag</code>: Final flag (1 if either method flags AND count &gt; minimum threshold)</li> </ul> <p>Algorithm Steps:</p> <p>Step 1: Calculate median volume for each facility-indicator combination</p> <p>Step 2: Compute MAD using only values equal to or above the median - Avoids bias from facilities with many low-volume months - Standardizes residuals by dividing (count - median) by MAD - Flags outlier_mad = 1 if mad_residual &gt; MADS parameter</p> <p>Step 3: Calculate proportional contribution - For each facility-indicator-year, sum total annual count - Calculate pc = count / annual_total - Flags outlier_pc = 1 if pc &gt; OUTLIER_PROPORTION_THRESHOLD</p> <p>Step 4: Combine flags - Final outlier_flag = 1 if (outlier_mad = 1 OR outlier_pc = 1) AND count &gt; MINIMUM_COUNT_THRESHOLD - This ensures only substantial volumes are flagged</p> process_completeness() <p>Signature: <code>process_completeness(outlier_data_main)</code></p> <p>Purpose: Main orchestration function that generates complete time series and assigns completeness flags for all indicators</p> <p>Parameters:</p> <ul> <li><code>outlier_data_main</code>: Outlier analysis results (contains all facility-indicator-period combinations with counts)</li> </ul> <p>Returns: Long format dataset with completeness flags for all facility-indicator-period combinations</p> <p>Process:</p> <ol> <li>Identifies first and last reporting period for each indicator globally</li> <li>Calls <code>generate_full_series_per_indicator()</code> for each indicator</li> <li>Applies completeness tagging logic (complete/incomplete/inactive)</li> <li>Merges with geographic metadata</li> <li>Combines results across all indicators</li> <li>Removes inactive periods (completeness_flag = 2)</li> </ol> <p>Output Structure:</p> <ul> <li>Explicit rows for both reported and non-reported periods</li> <li>Completeness flag: 0 (incomplete), 1 (complete), 2 (inactive - removed)</li> <li>Full time series from first to last reporting period per indicator</li> </ul> generate_full_series_per_indicator() <p>Signature: <code>generate_full_series_per_indicator(outlier_data, indicator_id, timeframe)</code></p> <p>Purpose: Creates a complete monthly time series for a specific indicator, filling in gaps where facilities did not report</p> <p>Parameters:</p> <ul> <li><code>outlier_data</code>: data.table with outlier results</li> <li><code>indicator_id</code>: Specific indicator to process (e.g., \"penta1\")</li> <li><code>timeframe</code>: Data table with first_pid and last_pid for each indicator</li> </ul> <p>Returns: Complete time series with explicit rows for both reported and non-reported periods</p> <p>Process:</p> <ol> <li>Subsets data to specific indicator</li> <li>Generates monthly sequence from first to last period_id for that indicator</li> <li>Creates complete facility-period grid (all facilities \u00d7 all months) using <code>CJ()</code> cross join</li> <li>Merges with actual reported data</li> <li>Missing counts indicate non-reporting periods</li> <li>Applies inactive detection algorithm</li> </ol> <p>Inactive Detection Algorithm:</p> <pre><code># A facility is flagged inactive (offline_flag = 2) if:\n# 1. Missing 6+ consecutive months BEFORE first report, OR\n# 2. Missing 6+ consecutive months AFTER last report\n\noffline_flag := fifelse(\n  (missing_group == 1 &amp; missing_count &gt;= 6 &amp; !first_report_idx) |\n  (missing_group == max(missing_group) &amp; missing_count &gt;= 6 &amp; !last_report_idx),\n  2L, 0L\n)\n</code></pre> <p>Example Timeline:</p> <pre><code>Facility A reporting pattern for indicator \"penta1\":\nPeriod:  202001 202002 202003 202004 202005 202006 202007 202008 202009 202010\nCount:   NA     NA     NA     NA     50     30     NA     NA     40     35\nFlag:    2      2      2      2      1      1      0      0      1      1\n         [----Inactive----] [---Active period with gaps---]\n\nExplanation:\n- First 4 months: Inactive (6+ months missing before first report at 202005)\n- 202005-202006: Complete (reported)\n- 202007-202008: Incomplete (gaps in active period)\n- 202009-202010: Complete (reported)\n</code></pre> geo_consistency_analysis() <p>Signature: <code>geo_consistency_analysis(data, geo_cols, geo_level, consistency_params)</code></p> <p>Purpose: Calculates consistency ratios at the geographic level to account for patients seeking services across multiple facilities within a district/ward</p> <p>Parameters:</p> <ul> <li><code>data</code>: Outlier data (with outliers already flagged)</li> <li><code>geo_cols</code>: Vector of geographic column names</li> <li><code>geo_level</code>: Geographic level for aggregation (e.g., \"admin_area_3\")</li> <li><code>consistency_params</code>: List with consistency_pairs and consistency_ranges</li> </ul> <p>Returns: Long format dataframe with geographic-level consistency results</p> <p>Process:</p> <ol> <li>Excludes outliers (sets count to NA where outlier_flag = 1)</li> <li>Aggregates data to specified geographic level by period (sums across facilities)</li> <li>Reshapes to wide format (one column per indicator)</li> <li>Calculates ratio for each indicator pair</li> <li>Flags consistency based on predefined ranges</li> </ol> <p>Output Columns:</p> <ul> <li>Geographic identifiers (up to specified level)</li> <li><code>period_id</code>: Time period</li> <li><code>ratio_type</code>: Name of the consistency pair (e.g., \"pair_penta\")</li> <li><code>consistency_ratio</code>: Calculated ratio value</li> <li><code>sconsistency</code>: Binary flag (1 = consistent, 0 = inconsistent, NA = cannot calculate)</li> </ul> <p>Example Output:</p> <pre><code>admin_area_2  admin_area_3  period_id  ratio_type    consistency_ratio  sconsistency\nDistrict_A    Ward_1        202401     pair_penta    1.05               1\nDistrict_A    Ward_1        202401     pair_anc      0.88               0\nDistrict_A    Ward_2        202401     pair_penta    0.97               1\n</code></pre> <p>Rationale: Measuring consistency at the geographic level accounts for patient movement between facilities and provides a more accurate picture of service utilization patterns across a community.</p> expand_geo_consistency_to_facilities() <p>Signature: <code>expand_geo_consistency_to_facilities(facility_metadata, geo_consistency_results, geo_level)</code></p> <p>Purpose: Assigns geographic-level consistency results to individual facilities</p> <p>Parameters:</p> <ul> <li><code>facility_metadata</code>: Facility list with geographic assignments</li> <li><code>geo_consistency_results</code>: Output from geo_consistency_analysis()</li> <li><code>geo_level</code>: Geographic level used in consistency analysis</li> </ul> <p>Returns: Facility-level dataset with consistency flags</p> <p>Process:</p> <ul> <li>Extracts facility list with their geographic assignments</li> <li>Performs left join to replicate geo-level consistency scores to all facilities in that area</li> <li>Uses many-to-many relationship to handle multiple periods and ratio types</li> </ul> <p>Rationale: Since consistency is measured at the geographic level (accounting for patient movement between facilities), all facilities within the same district/ward receive the same consistency scores.</p> dqa_with_consistency() <p>Signature: <code>dqa_with_consistency(completeness_data, consistency_data, outlier_data, geo_cols, dqa_rules)</code></p> <p>Purpose: Calculates comprehensive DQA scores including consistency checks when consistency pairs are available</p> <p>Parameters:</p> <ul> <li><code>completeness_data</code>: Output from process_completeness()</li> <li><code>consistency_data</code>: Wide-format facility consistency results</li> <li><code>outlier_data</code>: Output from outlier_analysis()</li> <li><code>geo_cols</code>: Vector of geographic column names</li> <li><code>dqa_rules</code>: List specifying required values for each dimension</li> </ul> <p>DQA Rules Configuration:</p> <pre><code>dqa_rules &lt;- list(\n  completeness = 1,   # Must be complete (flag = 1)\n  outlier_flag = 0,   # Must NOT be an outlier (flag = 0)\n  sconsistency = 1    # Must be consistent (flag = 1)\n)\n</code></pre> <p>Scoring Algorithm:</p> <p>1. Completeness-Outlier Score (per facility-period): - Each DQA indicator scores 0-2 points (1 for completeness + 1 for no outlier) - Maximum possible = 2 \u00d7 number of DQA indicators - Score = Total Points / Maximum Points</p> <p>2. Consistency Score (per facility-period): - Only counts pairs where both indicators exist (NA pairs excluded from denominator) - Score = Number of passing pairs / Number of available pairs - If no pairs available, score = 0</p> <p>3. Mean DQA Score: - Average of completeness-outlier score and consistency score - Formula: <code>(completeness_outlier_score + consistency_score) / 2</code></p> <p>4. Binary DQA Score: - 1 if ALL available consistency pairs pass AND completeness-outlier score is perfect - 0 otherwise</p> <p>Handling Missing Indicators: The function intelligently handles cases where some consistency indicators are missing: - NA values in consistency pairs are NOT replaced with 0 - Only available pairs contribute to the denominator - This prevents penalizing facilities for indicators they don't provide</p> <p>Example Calculation:</p> <pre><code>Facility X in period 202401:\n- DQA Indicators: penta1, anc1, opd (3 indicators)\n- Completeness: All 3 complete \u2192 3 points\n- Outliers: None \u2192 3 points\n- Total: 6/6 \u2192 completeness_outlier_score = 1.0\n\nConsistency Pairs:\n- pair_penta (penta1/penta3): Pass (1)\n- pair_anc (anc1/anc4): Fail (0)\n- pair_delivery: NA (bcg not a DQA indicator)\n\nConsistency calculation:\n- Available pairs: 2 (penta, anc)\n- Passing pairs: 1 (penta)\n- consistency_score = 1/2 = 0.5\n\nFinal scores:\n- dqa_mean = (1.0 + 0.5) / 2 = 0.75\n- dqa_score = 0 (not all pairs passed)\n</code></pre> dqa_without_consistency() <p>Signature: <code>dqa_without_consistency(completeness_data, outlier_data, geo_cols, dqa_rules)</code></p> <p>Purpose: Calculates DQA scores using only completeness and outlier checks when consistency data is unavailable or no valid consistency pairs exist</p> <p>When Used:</p> <ul> <li>No consistency pairs defined in configuration</li> <li>All consistency pairs have missing indicators</li> <li>Dataset doesn't contain paired indicators</li> </ul> <p>Scoring:</p> <ul> <li>Uses only completeness and outlier components</li> <li><code>dqa_mean</code> = <code>completeness_outlier_score</code></li> <li><code>dqa_score</code> = 1 if all completeness and outlier checks pass, 0 otherwise</li> </ul> <p>Output Structure:</p> <pre><code>dqa_results &lt;- data.frame(\n  facility_id,\n  admin_area_X,              # Dynamic geographic columns\n  period_id,\n  completeness_outlier_score, # Range: 0-1\n  dqa_mean,                   # Range: 0-1 (equals completeness_outlier_score)\n  dqa_score                   # Binary: 0 or 1\n)\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#statistical-methods-algorithms","title":"Statistical Methods &amp; Algorithms","text":"Median Absolute Deviation (MAD) Calculation <p>The MAD is a robust measure of variability that is less sensitive to outliers than standard deviation.</p> <p>Standard MAD Algorithm: 1. Compute the median of the dataset 2. Calculate absolute deviations: |value - median| for each data point 3. Find the median of these absolute deviations</p> <p>FASTR Modification: The module calculates MAD using only values at or above the median, making it more sensitive to high outliers while avoiding bias from facilities with many low-volume months.</p> <p>Outlier Degree Calculation:</p> \\[ \\text{MAD Residual} = \\frac{|\\text{volume} - \\text{median volume}|}{\\text{MAD}} \\] <p>Outlier Classification: - If MAD Residual &gt; 10 (configurable via <code>MADS</code> parameter), the value is flagged as a statistical outlier</p> <p>Example:</p> <pre><code>Facility ABC, Indicator: penta1\nMonthly counts: 20, 25, 22, 28, 24, 26, 150, 23, 27, 25, 21, 24\n\nStep 1: Calculate median = 24.5\nStep 2: Values &gt;= median: 25, 28, 24.5, 26, 150, 27, 25, 24.5\nStep 3: Absolute deviations from median: 0.5, 3.5, 0, 1.5, 125.5, 2.5, 0.5, 0\nStep 4: MAD = median(0, 0, 0.5, 0.5, 1.5, 2.5, 3.5, 125.5) = 1.0\nStep 5: For count=150: MAD residual = |150 - 24.5| / 1.0 = 125.5\nStep 6: 125.5 &gt; 10, therefore 150 is flagged as an outlier\n</code></pre> Proportional Outlier Detection <p>This method identifies months where a single observation represents an unusually large proportion of the annual total for a facility-indicator combination.</p> <p>Algorithm: 1. For each facility-indicator-year, sum the total annual count 2. Calculate the proportion: <code>pc = monthly_count / annual_total</code> 3. Flag as outlier if <code>pc &gt; OUTLIER_PROPORTION_THRESHOLD</code> (default 0.8)</p> <p>Rationale: A facility reporting 80% of its annual volume in a single month likely indicates a data entry error (e.g., cumulative reporting instead of monthly, extra digit entered).</p> <p>Example:</p> <pre><code>Facility XYZ, Indicator: anc1, Year: 2024\nMonthly counts: 15, 18, 12, 16, 890, 14, 17, 13, 16, 15, 14, 12\nAnnual total: 1052\n\nFor May (count=890):\npc = 890 / 1052 = 0.846\n0.846 &gt; 0.8, therefore May is flagged as a proportional outlier\n</code></pre> Consistency Ratio Benchmarks <p>The module applies programmatically defined benchmarks for indicator pairs:</p> <p>ANC Consistency:</p> \\[ \\text{ANC Consistency} = \\begin{cases} 1, &amp; \\frac{\\text{ANC1 Volume}}{\\text{ANC4 Volume}} \\geq 0.95 \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] <p>Interpretation: More women should start antenatal care (ANC1) than complete four visits (ANC4). The ratio is expected to be \u2265 0.95, allowing up to 5% tolerance for data variations.</p> <p>Penta Consistency:</p> \\[ \\text{Penta Consistency} = \\begin{cases} 1, &amp; \\frac{\\text{Penta1 Volume}}{\\text{Penta3 Volume}} \\geq 0.95 \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] <p>Interpretation: More children should receive the first pentavalent dose (Penta1) than complete the three-dose series (Penta3).</p> <p>BCG/Delivery Consistency:</p> \\[ \\text{BCG/Delivery Consistency} = \\begin{cases} 1, &amp; 0.7 \\leq \\frac{\\text{BCG Volume}}{\\text{Delivery Volume}} \\leq 1.3 \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] <p>Interpretation: BCG is a birth dose vaccine, so BCG vaccinations should approximately equal facility deliveries. The wider range (\u00b130%) accounts for infants born elsewhere receiving BCG at the facility or facility-born infants receiving BCG elsewhere.</p> <p>Implementation Detail: Consistency is assessed at the district/ward level (specified by <code>GEOLEVEL</code>) to account for patients visiting multiple facilities within their local area for different services.</p> Completeness Calculation <p>For a given indicator in a given month:</p> \\[ \\text{Completeness} = \\frac{\\text{Number of reporting facilities}}{\\text{Number of expected facilities}} \\times 100 \\] <p>Expected Facilities Definition: A facility is expected to report for an indicator if it has ever reported for that indicator within the analysis timeframe AND is not flagged as inactive.</p> <p>Inactive Facility Definition: A facility is flagged as inactive for periods where it did not report for six or more consecutive months before its first report or after its last report.</p> <p>Example:</p> <pre><code>District has 20 facilities that have ever reported penta1 data in 2024\nIn March 2024:\n- 18 facilities submitted penta1 data\n- 2 facilities did not submit (but are not inactive)\n\nCompleteness = 18 / 20 \u00d7 100 = 90%\n</code></pre> <p>Important Note: A high level of completeness does not necessarily indicate that the HMIS is representative of all service delivery in the country, as some services may not be delivered in facilities or some facilities may not report. For countries where DHIS2 does not store zeros, indicator completeness may be underestimated if there are many low-volume facilities.</p> DQA Composite Score Calculation <p>The DQA score combines three quality dimensions for a defined set of core indicators.</p> <p>Component Scores:</p> <p>1. Completeness-Outlier Score:</p> \\[ \\text{Completeness-Outlier Score} = \\frac{\\sum (\\text{completeness pass} + \\text{outlier pass})}{2 \\times \\text{number of DQA indicators}} \\] <p>2. Consistency Score:</p> \\[ \\text{Consistency Score} = \\frac{\\text{Number of pairs passing benchmarks}}{\\text{Number of available pairs}} \\] <p>3. Mean DQA Score:</p> \\[ \\text{DQA Mean} = \\frac{\\text{Completeness-Outlier Score} + \\text{Consistency Score}}{2} \\] <p>4. Binary DQA Score:</p> \\[ \\text{DQA Score} = \\begin{cases} 1, &amp; \\text{if all checks passed} \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] <p>Passing Criteria for Binary Score: - ALL DQA indicators must be complete (completeness_flag = 1) - ALL DQA indicators must be free of outliers (outlier_flag = 0) - ALL available consistency pairs must pass benchmarks (sconsistency = 1)</p> <p>Example Calculation:</p> <pre><code>Facility 123, Period 202403\nDQA Indicators: penta1, anc1, opd\n\nCompleteness: penta1=1, anc1=1, opd=1 \u2192 3 points\nOutliers: penta1=0, anc1=0, opd=0 \u2192 3 points\nCompleteness-Outlier Score = 6 / (2\u00d73) = 1.0\n\nConsistency Pairs:\n- pair_penta: 1 (pass)\n- pair_anc: 1 (pass)\nConsistency Score = 2 / 2 = 1.0\n\nDQA Mean = (1.0 + 1.0) / 2 = 1.0\nDQA Score = 1 (all checks passed)\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#code-examples","title":"Code Examples","text":"Example 1: Running the Module with Default Settings <pre><code># Set working directory\nsetwd(\"/path/to/module/directory\")\n\n# Load required libraries\nlibrary(zoo)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(data.table)\n\n# The module will automatically:\n# 1. Load hmis_ISO3.csv\n# 2. Run all analyses with default parameters\n# 3. Generate output CSV files in the working directory\n\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre> Example 2: Adjusting Outlier Detection Sensitivity <pre><code># Make outlier detection more sensitive (lower thresholds)\nOUTLIER_PROPORTION_THRESHOLD &lt;- 0.6   # Flag if &gt;60% of annual volume (was 80%)\nMINIMUM_COUNT_THRESHOLD &lt;- 50         # Consider counts &gt;=50 (was 100)\nMADS &lt;- 8                             # Flag at 8 MADs (was 10)\n\n# Run the module\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre> <p>Use Case: Countries with generally low service volumes where the default thresholds are too conservative.</p> Example 3: Different Geographic Level for Consistency <pre><code># Use district level (admin_area_2) instead of sub-district (admin_area_3)\nGEOLEVEL &lt;- \"admin_area_2\"\n\n# This affects consistency analysis aggregation level\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre> <p>Use Case: Sub-district level has sparse data or too few facilities per area, making district-level aggregation more stable.</p> Example 4: Custom DQA Indicators <pre><code># Focus DQA on maternal health indicators only\nDQA_INDICATORS &lt;- c(\"anc1\", \"anc4\", \"delivery\", \"pnc1\")\n\n# Only evaluate anc consistency pair\nCONSISTENCY_PAIRS_USED &lt;- c(\"anc\")\n\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre> <p>Use Case: Specialized analysis focusing on a specific service area.</p> Example 5: Running for Different Country <pre><code># Configure for your country\nCOUNTRY_ISO3 &lt;- \"ISO3\"  # Replace with your country code\nPROJECT_DATA_HMIS &lt;- \"hmis_ISO3.csv\"\nGEOLEVEL &lt;- \"admin_area_3\"\n\n# Adjust for country-specific indicators if needed\nDQA_INDICATORS &lt;- c(\"penta1\", \"anc1\", \"opd\", \"fp_new\")\n\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre> Example 6: Programmatic Use of Outputs <pre><code># After running the module, work with outputs\n\n# Load DQA results\ndqa_results &lt;- read.csv(\"M1_output_dqa.csv\")\n\n# Filter to high-quality facility-months only\nhigh_quality &lt;- dqa_results %&gt;%\n  filter(dqa_score == 1)\n\n# Calculate percentage of facility-months passing DQA by district\nquality_by_district &lt;- dqa_results %&gt;%\n  group_by(admin_area_2, period_id) %&gt;%\n  summarize(\n    total_facility_months = n(),\n    passing_quality = sum(dqa_score == 1),\n    pct_passing = 100 * passing_quality / total_facility_months\n  )\n\n# Identify facilities with consistently poor quality (never passing)\npoor_quality_facilities &lt;- dqa_results %&gt;%\n  group_by(facility_id) %&gt;%\n  summarize(\n    months_analyzed = n(),\n    months_passed = sum(dqa_score == 1),\n    pct_passed = 100 * months_passed / months_analyzed\n  ) %&gt;%\n  filter(pct_passed == 0)\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#troubleshooting","title":"Troubleshooting","text":"Problem: Module skips consistency analysis <p>Symptoms: - Console message: \"No valid consistency pairs found\" - M1_output_consistency_geo.csv has only headers - DQA scores calculated without consistency component</p> <p>Diagnosis: Check that both indicators in each pair exist in your dataset:</p> <pre><code># Load your data\ndata &lt;- read.csv(\"hmis_[COUNTRY].csv\")\n\n# Check available indicators\nprint(unique(data$indicator_common_id))\n\n# Compare with required pairs\n# For pair_penta: need \"penta1\" and \"penta3\"\n# For pair_anc: need \"anc1\" and \"anc4\"\n# For pair_delivery: need \"bcg\" and \"delivery\" (or \"sba\")\n</code></pre> <p>Solutions: 1. Adjust <code>CONSISTENCY_PAIRS_USED</code> to only include pairs with available indicators 2. Modify indicator names in your data to match expected names 3. Accept that DQA will be calculated without consistency component</p> Problem: All facilities flagged as outliers <p>Symptoms: - Very high percentage of outlier_flag = 1 in M1_output_outliers.csv - Most observations in outlier_list.csv</p> <p>Diagnosis: Your thresholds may be too sensitive for your data context.</p> <p>Solutions:</p> <ol> <li>Increase MAD threshold:</li> </ol> <pre><code>MADS &lt;- 15  # Increase from default 10\n</code></pre> <ol> <li>Increase proportion threshold:</li> </ol> <pre><code>OUTLIER_PROPORTION_THRESHOLD &lt;- 0.9  # Increase from 0.8\n</code></pre> <ol> <li>Increase minimum count threshold (focus on larger facilities):</li> </ol> <pre><code>MINIMUM_COUNT_THRESHOLD &lt;- 200  # Increase from 100\n</code></pre> <ol> <li>Review the data: Check if there are genuine quality issues requiring data cleaning rather than parameter adjustment</li> </ol> Problem: No DQA results generated <p>Symptoms: - M1_output_dqa.csv is empty or has only headers - Console message: \"Skipping DQA analysis - none of the required indicators found\"</p> <p>Diagnosis: None of the indicators specified in <code>DQA_INDICATORS</code> exist in your dataset.</p> <p>Solution: Check which DQA indicators are missing:</p> <pre><code># Load data\ndata &lt;- read.csv(\"hmis_[COUNTRY].csv\")\n\n# Check which DQA indicators are missing\navailable_indicators &lt;- unique(data$indicator_common_id)\nmissing_indicators &lt;- setdiff(DQA_INDICATORS, available_indicators)\nprint(paste(\"Missing DQA indicators:\", paste(missing_indicators, collapse=\", \")))\n\n# Available DQA indicators\navailable_dqa &lt;- intersect(DQA_INDICATORS, available_indicators)\nprint(paste(\"Available DQA indicators:\", paste(available_dqa, collapse=\", \")))\n</code></pre> <p>Then update <code>DQA_INDICATORS</code> to include only available indicators:</p> <pre><code>DQA_INDICATORS &lt;- c(\"penta1\", \"anc1\")  # Only use what's available\n</code></pre> Problem: Consistency ratios seem incorrect <p>Symptoms: - All consistency flags are 0 (inconsistent) - Consistency ratios are unexpectedly high or low</p> <p>Diagnosis: The geographic aggregation level may be inappropriate for your data.</p> <p>Investigation:</p> <pre><code># Load geographic consistency results\ngeo_cons &lt;- read.csv(\"M1_output_consistency_geo.csv\")\n\n# Check distribution of consistency ratios\nsummary(geo_cons$consistency_ratio)\n\n# Check sample sizes at geographic level\noutliers &lt;- read.csv(\"M1_output_outliers.csv\")\ngeo_summary &lt;- outliers %&gt;%\n  group_by(admin_area_3, period_id) %&gt;%\n  summarize(\n    n_facilities = n_distinct(facility_id),\n    total_volume = sum(count, na.rm = TRUE)\n  )\nsummary(geo_summary$n_facilities)\n</code></pre> <p>Solutions:</p> <ol> <li>If geographic areas have very few facilities (1-2), use higher level:</li> </ol> <pre><code>GEOLEVEL &lt;- \"admin_area_2\"  # Use district instead of sub-district\n</code></pre> <ol> <li> <p>If ratios are generally below 0.95 for ANC/Penta pairs, this may indicate genuine programmatic issues (high dropout) rather than data quality problems</p> </li> <li> <p>Review the consistency benchmark ranges - they may need adjustment for your context:</p> </li> </ol> <pre><code># Example: Allow higher dropout (lower ratio) for Penta\nall_consistency_ranges$pair_penta &lt;- c(lower = 0.85, upper = Inf)\n</code></pre> Problem: Completeness percentages seem low <p>Symptoms: - High proportion of completeness_flag = 0 in M1_output_completeness.csv</p> <p>Diagnosis: This could be legitimate (poor reporting) or an artifact of how your DHIS2 stores zero values.</p> <p>Investigation:</p> <pre><code># Load completeness data\ncompleteness &lt;- read.csv(\"M1_output_completeness.csv\")\n\n# Check pattern: Are there explicit zeros or just missing values?\noutliers &lt;- read.csv(\"M1_output_outliers.csv\")\ntable(is.na(outliers$count), outliers$count == 0)\n\n# Check completeness by indicator\ncomp_by_indicator &lt;- completeness %&gt;%\n  group_by(indicator_common_id) %&gt;%\n  summarize(\n    pct_complete = 100 * mean(completeness_flag == 1),\n    pct_incomplete = 100 * mean(completeness_flag == 0)\n  )\nprint(comp_by_indicator)\n</code></pre> <p>Considerations: 1. If your DHIS2 doesn't store zeros, low-volume facilities may appear incomplete when they legitimately had no services to report 2. Completeness percentages should be interpreted in context - 70% completeness may be acceptable depending on the health system 3. Use the completeness_flag in subsequent modules to weight estimates appropriately</p> Problem: Error reading input file <p>Symptoms: - Error: \"Cannot open file 'hmis_[COUNTRY].csv'\" - Module crashes during data loading</p> <p>Solutions:</p> <ol> <li>Check file path and working directory:</li> </ol> <pre><code>getwd()  # Verify working directory\nlist.files()  # Check if HMIS file is present\n</code></pre> <ol> <li> <p>Verify file name matches <code>PROJECT_DATA_HMIS</code> parameter</p> </li> <li> <p>Check file format (CSV, proper encoding, comma-separated)</p> </li> <li> <p>Ensure required columns exist:</p> </li> </ol> <pre><code># After loading\nnames(data)  # Should include: facility_id, period_id, indicator_common_id, count\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#usage-notes","title":"Usage Notes","text":"Data Type Handling <p>period_id Flexibility: The module accepts <code>period_id</code> in multiple formats: - Integer: <code>202401</code> - String: <code>\"202401\"</code> - Numeric: <code>202401.0</code></p> <p>All formats are internally converted to Date objects for correct chronological ordering:</p> <pre><code># Internal conversion\nas.Date(sprintf(\"%04d-%02d-01\", year, month))\n</code></pre> <p>This ensures proper temporal ordering even with gaps in reporting periods.</p> <p>Count Values: - Numeric values required (integers or decimals) - Zero counts should be explicit <code>0</code>, not <code>NA</code> - Missing counts represented as <code>NA</code> or absent rows</p> <p>Geographic Columns: - Character type recommended - Can contain spaces and special characters - Case-sensitive in some operations</p> Missing Value Strategy <p>The module uses context-specific approaches to missing values:</p> <p>Outlier Analysis: - NA values excluded from median/MAD calculations - Only non-NA values contribute to statistics - Prevents bias from sparse reporting</p> <p>Completeness: - Explicit NA in count column indicates non-reporting - Assigned completeness_flag = 0 (incomplete) - Distinguished from inactive periods (flag = 2, removed)</p> <p>Consistency: - NA ratios (from division by zero) kept as NA, not converted to 0 - NA pairs excluded from consistency scoring denominator - Prevents penalizing facilities for unavailable indicators</p> <p>DQA Scoring: - NA consistency pairs excluded from denominator - Only available pairs affect consistency score - Allows partial scoring when some indicators missing</p> Memory Considerations <p>For large datasets (&gt;1 million rows), the module implements several optimizations:</p> <p>data.table Usage: - Completeness processing uses <code>data.table</code> for in-place operations - Significantly faster and more memory-efficient than <code>dplyr</code> for large data</p> <p>Filtering Strategy: - Filters to relevant indicators before expensive operations - Reduces memory footprint during calculations</p> <p>Object Management: - Removes intermediate objects after use - Prevents memory accumulation during sequential processing</p> <p>Recommendations for Large Datasets: - Allocate at least 8GB RAM for countries with &gt;1000 facilities - Consider processing by year if multi-year datasets cause memory issues - Monitor memory usage: <code>pryr::mem_used()</code> at various stages</p> Performance Optimization Opportunities <p>Current Implementation: The completeness analysis processes indicators sequentially using <code>lapply()</code>.</p> <p>Potential Enhancement: For datasets with many indicators, parallelization could improve performance:</p> <pre><code># Future enhancement (not in current code)\nlibrary(parallel)\n\n# Detect available cores\nn_cores &lt;- detectCores() - 1\n\n# Parallel processing of indicators\ncompleteness_list &lt;- mclapply(\n  unique(outlier_data_main$indicator_common_id),\n  function(ind) generate_full_series_per_indicator(\n    outlier_data = outlier_data_main,\n    indicator_id = ind,\n    timeframe = indicator_timeframe\n  ),\n  mc.cores = n_cores\n)\n\n# Combine results\ncompleteness_data &lt;- rbindlist(completeness_list)\n</code></pre> <p>Expected Speedup: - 3-4x faster with 4 cores on datasets with 10+ indicators - Most beneficial for countries with many indicators and long time series</p> Dynamic Indicator Selection <p>The module intelligently adapts to available data:</p> <p>Delivery Indicator Selection:</p> <pre><code># Automatically chooses between \"delivery\" and \"sba\" for BCG consistency pair\nif (\"delivery\" %in% available_indicators) {\n  PAIR_DELIVERY_B &lt;- \"delivery\"\n} else if (\"sba\" %in% available_indicators) {\n  PAIR_DELIVERY_B &lt;- \"sba\"  # Skilled birth attendant\n} else {\n  PAIR_DELIVERY_B &lt;- \"delivery\"  # Default fallback\n}\n</code></pre> <p>DQA Indicator Validation:</p> <pre><code># Only use DQA indicators that exist in the dataset\ndqa_indicators_to_use &lt;- intersect(DQA_INDICATORS, unique(data$indicator_common_id))\n\n# If none found, skip DQA analysis with informative message\nif (length(dqa_indicators_to_use) == 0) {\n  print(\"Skipping DQA analysis - none of the required indicators found\")\n}\n</code></pre> <p>Consistency Pair Validation: The module checks each consistency pair and removes those with missing indicators, providing clear warnings about which pairs were skipped.</p> Error Handling and Fallbacks <p>The module includes robust error handling:</p> <p>Missing Consistency Pairs: - If no valid pairs exist, skips consistency analysis - Uses <code>dqa_without_consistency()</code> for scoring - Outputs dummy files with proper headers</p> <p>Missing Geographic Levels: - Falls back to lowest available admin level if specified <code>GEOLEVEL</code> not found - Issues warning about the fallback</p> <p>Empty Results: - Creates CSV files with proper headers even when no data - Ensures downstream processes don't break</p> <p>Missing Indicators: - Validates all indicator requirements before analysis - Warns about removed pairs - Continues with available indicators</p> Interpretation Guidelines <p>Outlier Flags: - outlier_flag = 1 suggests potential data quality issues, but require investigation - Not all flagged outliers are errors (genuine service campaigns can trigger flags) - Use mad_residual and pc values to prioritize review</p> <p>Completeness: - Completeness % varies by health system context - 80-90%+ is generally good, but depends on country - Trend over time more informative than absolute percentage - Low completeness for specific indicators may reflect genuine service gaps</p> <p>Consistency: - sconsistency = 0 may indicate data quality issues OR programmatic performance issues (e.g., high dropout) - Requires programmatic knowledge to interpret - Geographic patterns can help distinguish systematic issues from random errors</p> <p>DQA Scores: - dqa_score = 1 indicates data passed all checks, suitable for unadjusted use - dqa_score = 0 requires further investigation - dqa_mean provides nuanced view (0.75 = mostly good, 0.25 = mostly poor)</p>"},{"location":"01_module_data_quality_assessment_documentation/#data-quality-metrics-summary","title":"Data Quality Metrics Summary","text":"Metric Type Range Interpretation outlier_flag Binary 0 or 1 1 = Outlier detected by either method and count &gt; threshold outlier_mad Binary 0 or 1 1 = Statistical outlier (MAD-based) outlier_pc Binary 0 or 1 1 = Proportional outlier (&gt;80% of annual volume) mad_residual Continuous 0 to \u221e Standardized deviation from median (higher = more extreme) pc Continuous 0 to 1 Proportion of annual volume (closer to 1 = more concentrated) completeness_flag Categorical 0, 1, 2 0=Incomplete (missing), 1=Complete (reported), 2=Inactive (removed) sconsistency Binary 0, 1, NA 1=Consistent (passes benchmark), 0=Inconsistent, NA=Cannot calculate consistency_ratio Continuous 0 to \u221e Ratio of paired indicators (interpretation depends on pair) completeness_outlier_score Continuous 0 to 1 Proportion of DQA indicators passing completeness &amp; outlier checks consistency_score Continuous 0 to 1 Proportion of consistency pairs passing benchmarks dqa_mean Continuous 0 to 1 Average of component scores (overall quality measure) dqa_score Binary 0 or 1 1 = All checks passed (high quality), 0 = Any check failed"},{"location":"01_module_data_quality_assessment_documentation/#execution-workflow","title":"Execution Workflow","text":"<p>The module follows this sequence:</p> <pre><code>1. DATA LOADING &amp; PREPROCESSING\n   \u251c\u2500 Load HMIS CSV file\n   \u251c\u2500 Convert period_id to dates\n   \u251c\u2500 Detect geographic columns\n   \u2514\u2500 Create composite malaria indicator (if applicable)\n\n2. CONFIGURATION &amp; VALIDATION\n   \u251c\u2500 Detect available indicators\n   \u251c\u2500 Dynamically select delivery indicator (delivery vs sba)\n   \u251c\u2500 Build consistency pairs based on available indicators\n   \u251c\u2500 Validate consistency pairs\n   \u2514\u2500 Filter DQA indicators to available ones\n\n3. OUTLIER ANALYSIS\n   \u251c\u2500 Calculate median and MAD by facility-indicator\n   \u251c\u2500 Flag MAD-based outliers (&gt;10 MADs)\n   \u251c\u2500 Flag proportion-based outliers (&gt;80% of annual volume)\n   \u2514\u2500 Combine flags (either method + count &gt; 100)\n\n4. COMPLETENESS ANALYSIS\n   \u251c\u2500 Identify reporting timeframe per indicator\n   \u251c\u2500 Generate full time series (all facilities \u00d7 all months)\n   \u251c\u2500 Tag reporting status (complete/incomplete/inactive)\n   \u2514\u2500 Remove inactive periods (6+ months before first/after last report)\n\n5. CONSISTENCY ANALYSIS (if applicable)\n   \u251c\u2500 Exclude outliers from data\n   \u251c\u2500 Aggregate to geographic level (e.g., district)\n   \u251c\u2500 Calculate ratios for indicator pairs\n   \u251c\u2500 Flag consistency based on predefined ranges\n   \u251c\u2500 Expand geo-level results to facilities\n   \u2514\u2500 Pivot to wide format (one column per pair)\n\n6. DQA SCORING\n   \u251c\u2500 Filter to DQA indicators only\n   \u251c\u2500 Merge completeness, outlier, and consistency results\n   \u251c\u2500 Calculate component scores:\n   \u2502  \u251c\u2500 Completeness-outlier score (0-1)\n   \u2502  \u2514\u2500 Consistency score (0-1, if applicable)\n   \u251c\u2500 Calculate mean DQA score\n   \u2514\u2500 Assign binary DQA pass/fail flag\n\n7. EXPORT RESULTS\n   \u251c\u2500 M1_output_outlier_list.csv (outliers only)\n   \u251c\u2500 M1_output_outliers.csv (all records with flags)\n   \u251c\u2500 M1_output_completeness.csv (completeness flags)\n   \u251c\u2500 M1_output_consistency_geo.csv (geo-level consistency)\n   \u251c\u2500 M1_output_consistency_facility.csv (facility-level consistency)\n   \u2514\u2500 M1_output_dqa.csv (final DQA scores)\n</code></pre> <p>Last updated: 10-11-2025 Contact: FASTR Project Team</p>"},{"location":"02_module_data_quality_adjustments_documentation/","title":"Module 2: Data Quality Adjustment","text":""},{"location":"02_module_data_quality_adjustments_documentation/#1-overview-what-why","title":"1. Overview (What &amp; Why)","text":""},{"location":"02_module_data_quality_adjustments_documentation/#what-does-this-module-do","title":"What does this module do?","text":"<p>The Data Quality Adjustment module systematically corrects two common problems in routine health facility data: outliers (extreme values caused by reporting errors or data entry mistakes) and missing data (from incomplete reporting). Rather than simply deleting problematic data, this module replaces questionable values with statistically sound estimates based on each facility's own historical patterns.</p> <p>The module uses sophisticated temporal smoothing techniques that analyze trends over time. By calculating rolling averages and examining facility-specific historical patterns, it preserves the underlying trends in the data while correcting anomalous values. This approach ensures that adjusted data remains grounded in real service delivery patterns rather than arbitrary replacements.</p> <p>To accommodate different analytical needs, the module produces four parallel versions of the data: one with no adjustments (the original data), one with only outlier corrections, one with only missing data filled in, and one with both types of corrections applied. This multi-scenario approach allows analysts to understand how sensitive their results are to different data quality assumptions and choose the most appropriate version for their analysis.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#why-is-it-needed-in-the-fastr-pipeline","title":"Why is it needed in the FASTR pipeline?","text":"<p>Routine health management information system (HMIS) data often contains errors and gaps that can seriously distort trends, mask true patterns, and lead to incorrect conclusions. A single extreme outlier can make service volumes appear to spike dramatically, while missing data can make it seem like services stopped entirely. These issues are particularly problematic when:</p> <ul> <li> <p>Tracking progress toward health goals and targets</p> </li> <li> <p>Comparing facilities or regions to identify high and low performers</p> </li> <li> <p>Allocating resources based on service delivery patterns</p> </li> <li> <p>Detecting genuine changes in health service utilization versus data quality issues</p> </li> </ul> <p>By systematically addressing these data quality issues before analysis, this module ensures that downstream calculations and decisions are based on reliable, consistent data rather than artifacts of poor data quality.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#quick-summary","title":"Quick Summary","text":"Component Details Inputs Raw HMIS data (<code>hmis_ISO3.csv</code>)Outlier flags from Module 1 (<code>M1_output_outliers.csv</code>)Completeness flags from Module 1 (<code>M1_output_completeness.csv</code>) Outputs Facility-level adjusted data (<code>M2_adjusted_data.csv</code>)Subnational aggregated data (<code>M2_adjusted_data_admin_area.csv</code>)National aggregated data (<code>M2_adjusted_data_national.csv</code>)Exclusion metadata (<code>M2_low_volume_exclusions.csv</code>) Purpose Replace outlier values and fill missing data using facility-specific historical patterns; produces four adjustment scenarios (none, outliers only, completeness only, both)"},{"location":"02_module_data_quality_adjustments_documentation/#2-how-it-works","title":"2. How It Works","text":""},{"location":"02_module_data_quality_adjustments_documentation/#high-level-workflow","title":"High-Level Workflow","text":"<p>The module follows a systematic seven-step process to clean and adjust health facility data:</p> <p>Step 1: Load and Prepare Data The module brings together three datasets: the raw facility service volumes, the outlier flags identifying suspicious values (from Module 1), and the completeness flags showing which months had incomplete reporting (from Module 1). It also identifies certain sensitive indicators like deaths that should never be adjusted.</p> <p>Step 2: Identify Low-Volume Indicators Before making any adjustments, the module checks each indicator to see if it has meaningful variation. Indicators that never have values above 100 across the entire dataset are flagged and excluded from outlier adjustment, since outlier detection isn't meaningful for consistently low-count indicators.</p> <p>Step 3: Adjust Outlier Values For each value flagged as an outlier, the module calculates what the value \"should have been\" based on that facility's historical pattern. It uses a hierarchy of methods, preferring to use surrounding months when possible (like averaging the 3 months before and 3 months after), but falling back to other approaches if needed (like using the same month from the previous year for seasonal indicators).</p> <p>Step 4: Fill Missing and Incomplete Data For months where data is missing or marked as incomplete, the module imputes (fills in) values using the same rolling average approach. This ensures that temporary reporting gaps don't create artificial drops to zero in the data.</p> <p>Step 5: Create Multiple Scenarios The module runs the adjustment logic four different ways: with no adjustments (baseline), only outlier corrections, only completeness corrections, and both types of corrections together. This allows analysts to see how different choices affect their results.</p> <p>Step 6: Aggregate to Geographic Levels After adjustments are complete, the facility-level data is aggregated (summed up) to create subnational and national-level datasets. Each geographic level maintains all four scenarios, so analysts can work at whichever level they need.</p> <p>Step 7: Export Results The module saves four CSV files: one for facility-level data, one for subnational areas, one for national totals, and one documenting which indicators were excluded from adjustment and why.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#workflow-diagram","title":"Workflow Diagram","text":""},{"location":"02_module_data_quality_adjustments_documentation/#key-decision-points","title":"Key Decision Points","text":"<p>Which values should be adjusted?</p> <p>The module adjusts two types of problematic values:</p> <ul> <li> <p>Values flagged as outliers by Module 1's statistical detection algorithms</p> </li> <li> <p>Values from months marked as incomplete or entirely missing</p> </li> </ul> <p>However, certain indicators are NEVER adjusted:</p> <ul> <li> <p>Death counts (under-5 deaths, maternal deaths, neonatal deaths) because these represent discrete events that should not be smoothed</p> </li> <li> <p>Low-volume indicators (those that never exceed 100) where outlier detection isn't meaningful</p> </li> </ul> <p>Which scenario should analysts use? By producing four scenarios, the module allows different use cases:</p> <ul> <li>None: Use for validation or when data quality is already excellent</li> <li>Outliers only: Use when completeness is good but occasional extreme values are problematic</li> <li>Completeness only: Use when you trust the reported values but reporting is sporadic</li> <li>Both: Use when both data quality issues are prevalent</li> </ul>"},{"location":"02_module_data_quality_adjustments_documentation/#what-happens-to-the-data","title":"What Happens to the Data","text":"<p>Input Processing: The module receives facility-level monthly service volumes along with quality flags from Module 1 (outlier indicators, completeness status). Each facility-indicator-period combination represents a single observation that may require adjustment.</p> <p>Adjustment Application: Based on the selected scenario, the module creates adjusted versions of the service counts. For outliers, abnormally high values are replaced with mean values calculated from non-outlier months. For incomplete reporting periods, missing values are imputed using facility-specific averages from available data.</p> <p>Multiple Scenario Generation: The module generates four parallel versions of the dataset: <code>count_final_none</code> (no adjustments), <code>count_final_outliers</code> (outliers only), <code>count_final_completeness</code> (missing data only), and <code>count_final_both</code> (both adjustments applied). This allows downstream analysis to compare results across different data quality assumptions.</p> <p>Output Aggregation: The adjusted data is aggregated to geographic levels (country, provinces, districts) while preserving all four adjustment scenarios. Each output row contains the geographic area identifier, indicator code, time period, and all four count versions, enabling flexible analysis depending on data quality tolerance and research questions.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#analysis-outputs-and-visualization","title":"Analysis Outputs and Visualization","text":"<p>The FASTR analysis generates three main visual outputs comparing service volumes before and after adjustments:</p> <p>1. Outlier Adjustment Impact</p> <p>Heatmap showing percent change in service volumes due to outlier replacement, with geographic areas as rows and indicators as columns.</p> <p></p> <p>2. Completeness Adjustment Impact</p> <p>Heatmap showing percent change in service volumes due to missing data imputation, with geographic areas as rows and indicators as columns.</p> <p></p> <p>3. Combined Adjustment Impact</p> <p>Heatmap showing percent change in service volumes when both outlier and completeness adjustments are applied, with geographic areas as rows and indicators as columns.</p> <p></p> <p>Interpretation Guide: tbd</p>"},{"location":"02_module_data_quality_adjustments_documentation/#3-detailed-reference","title":"3. Detailed Reference","text":""},{"location":"02_module_data_quality_adjustments_documentation/#configuration-parameters","title":"Configuration Parameters","text":"Excluded Indicators <p>Some indicators are excluded from all adjustments due to their sensitive nature:</p> <pre><code>EXCLUDED_FROM_ADJUSTMENT &lt;- c(\"u5_deaths\", \"maternal_deaths\", \"neonatal_deaths\")\n</code></pre> <p>Rationale: Death counts should not be smoothed or imputed as they represent discrete events that may have genuine temporal variation. Adjusting these could mask important epidemiological patterns or outbreak signals.</p> Low Volume Exclusions <p>Indicators are also automatically excluded from outlier adjustment if they have zero observations above 100 across the entire dataset. This prevents meaningless outlier detection on indicators with consistently low counts.</p> <p>Exclusion Logic:</p> <pre><code>volume_check &lt;- raw_data[, .(\n  above_100 = sum(count &gt; 100, na.rm = TRUE),\n  total = .N\n), by = indicator_common_id]\n\nno_outlier_adj &lt;- volume_check[above_100 == 0, indicator_common_id]\n</code></pre> <p>This information is saved to <code>M2_low_volume_exclusions.csv</code> for transparency.</p> Rolling Window Configuration <p>The module uses a 6-month window for all rolling averages. This choice balances:</p> <p>Advantages:</p> <ul> <li>Captures medium-term trends</li> <li>Reduces impact of short-term fluctuations</li> <li>Sufficient data points for stable averages</li> <li>Works well for both stable and seasonal indicators</li> </ul> <p>Trade-offs:</p> <ul> <li>May not capture rapid changes in service delivery</li> <li>Could over-smooth in cases of genuine programmatic shifts</li> <li>Requires at least 6 valid observations for optimal centered average</li> </ul>"},{"location":"02_module_data_quality_adjustments_documentation/#inputoutput-specifications","title":"Input/Output Specifications","text":"Input Files <p>The module requires three input files from previous processing steps:</p> File Source Description Key Variables <code>hmis_ISO3.csv</code> Raw HMIS data Facility-level service volumes <code>facility_id</code>, <code>indicator_common_id</code>, <code>period_id</code>, <code>count</code>, admin area columns <code>M1_output_outliers.csv</code> Module 1 Outlier flags for each facility-month-indicator <code>facility_id</code>, <code>indicator_common_id</code>, <code>period_id</code>, <code>outlier_flag</code> <code>M1_output_completeness.csv</code> Module 1 Completeness flags for each facility-month-indicator <code>facility_id</code>, <code>indicator_common_id</code>, <code>period_id</code>, <code>completeness_flag</code> Input Data Structure <p>Raw HMIS Data (<code>hmis_ISO3.csv</code>):</p> <pre><code>facility_id | admin_area_1 | admin_area_2 | admin_area_3 | period_id | indicator_common_id | count\n------------|--------------|--------------|--------------|-----------|---------------------|-------\nFAC001      | ISO3         | Province_A   | District_A   | 202301    | anc1                | 145\nFAC001      | ISO3         | Province_A   | District_A   | 202302    | anc1                | 152\nFAC001      | ISO3         | Province_A   | District_A   | 202303    | anc1                | 890  # Outlier\n</code></pre> <p>Outlier Flags (<code>M1_output_outliers.csv</code>):</p> <pre><code>facility_id | indicator_common_id | period_id | outlier_flag\n------------|---------------------|-----------|-------------\nFAC001      | anc1                | 202301    | 0\nFAC001      | anc1                | 202302    | 0\nFAC001      | anc1                | 202303    | 1           # Flagged as outlier\n</code></pre> <p>Completeness Flags (<code>M1_output_completeness.csv</code>):</p> <pre><code>facility_id | indicator_common_id | period_id | completeness_flag\n------------|---------------------|-----------|------------------\nFAC001      | anc1                | 202301    | 1             # Complete\nFAC001      | anc1                | 202302    | 0             # Incomplete\nFAC001      | anc1                | 202303    | 1             # Complete\n</code></pre> Output Files <p>The module generates four output files:</p> File Level Description Key Columns <code>M2_adjusted_data.csv</code> Facility Adjusted volumes for all scenarios at facility level <code>facility_id</code>, admin areas (excl. admin_area_1), <code>period_id</code>, <code>indicator_common_id</code>, <code>count_final_*</code> <code>M2_adjusted_data_admin_area.csv</code> Subnational Aggregated adjusted volumes at subnational admin areas Admin areas (excl. admin_area_1), <code>period_id</code>, <code>indicator_common_id</code>, <code>count_final_*</code> <code>M2_adjusted_data_national.csv</code> National Aggregated adjusted volumes at national level <code>admin_area_1</code>, <code>period_id</code>, <code>indicator_common_id</code>, <code>count_final_*</code> <code>M2_low_volume_exclusions.csv</code> Metadata Indicators excluded from outlier adjustment due to low volumes <code>indicator_common_id</code>, <code>low_volume_exclude</code> Output Data Structure <p>Facility-Level Output (<code>M2_adjusted_data.csv</code>):</p> <pre><code>facility_id | admin_area_2 | admin_area_3 | period_id | indicator_common_id | count_final_none | count_final_outliers | count_final_completeness | count_final_both\n------------|--------------|--------------|-----------|---------------------|------------------|----------------------|--------------------------|------------------\nFAC001      | Province_A   | District_A   | 202301    | anc1                | 145              | 145                  | 145                      | 145\nFAC001      | Province_A   | District_A   | 202302    | anc1                | 152              | 152                  | 148                      | 148\nFAC001      | Province_A   | District_A   | 202303    | anc1                | 890              | 148                  | 890                      | 148\n</code></pre> <p>Each <code>count_final_*</code> column represents a different adjustment scenario:</p> <ul> <li><code>count_final_none</code>: No adjustments applied (original values)</li> <li><code>count_final_outliers</code>: Only outlier adjustment applied</li> <li><code>count_final_completeness</code>: Only completeness adjustment applied</li> <li><code>count_final_both</code>: Both outlier and completeness adjustments applied</li> </ul>"},{"location":"02_module_data_quality_adjustments_documentation/#key-functions-documentation","title":"Key Functions Documentation","text":"Required Libraries <p>The module depends on the following R packages:</p> <ul> <li><code>data.table</code> - High-performance data manipulation and aggregation</li> <li><code>zoo</code> - Rolling window calculations (<code>frollmean</code> for rolling averages)</li> <li><code>lubridate</code> - Date handling and manipulation</li> </ul> 1. <code>apply_adjustments()</code> <p>Core function that implements the adjustment logic for a single scenario.</p> <p>Purpose:</p> <p>Replaces outlier and/or incomplete values using rolling averages and historical patterns.</p> <p>Parameters:</p> <ul> <li><code>raw_data</code> (data.table): Original HMIS data with service counts</li> <li><code>completeness_data</code> (data.table): Completeness flags from Module 1</li> <li><code>outlier_data</code> (data.table): Outlier flags from Module 1</li> <li><code>adjust_outliers</code> (logical): Whether to apply outlier adjustment</li> <li><code>adjust_completeness</code> (logical): Whether to apply completeness adjustment</li> </ul> <p>Returns:</p> <p>data.table with adjusted values in <code>count_working</code> column and adjustment metadata</p> <p>Key Operations:</p> <ol> <li>Merges input datasets by <code>facility_id</code>, <code>indicator_common_id</code>, and <code>period_id</code></li> <li>Converts <code>period_id</code> to dates for temporal ordering</li> <li>Calculates rolling averages (centered, forward, backward) for valid values</li> <li>Applies adjustment hierarchy based on data availability</li> <li>Tracks adjustment method used for each replaced value</li> </ol> 2. <code>apply_adjustments_scenarios()</code> <p>Wrapper function that runs adjustments across all four scenarios.</p> <p>Purpose:</p> <p>Applies the adjustment logic under different combinations of outlier and completeness adjustments.</p> <p>Parameters:</p> <ul> <li><code>raw_data</code> (data.table): Original HMIS data</li> <li><code>completeness_data</code> (data.table): Completeness flags</li> <li><code>outlier_data</code> (data.table): Outlier flags</li> </ul> <p>Returns:</p> <p>data.table with four <code>count_final_*</code> columns, one per scenario</p> <p>Scenarios Processed:</p> <ol> <li><code>none</code>: No adjustments (baseline)</li> <li><code>outliers</code>: Outlier adjustment only</li> <li><code>completeness</code>: Completeness adjustment only</li> <li><code>both</code>: Sequential outlier then completeness adjustment</li> </ol> <p>Processing Logic:</p> <ul> <li>Calls <code>apply_adjustments()</code> once per scenario</li> <li>Preserves original values for excluded indicators (deaths)</li> <li>Merges all scenario results into a single wide-format table</li> </ul>"},{"location":"02_module_data_quality_adjustments_documentation/#statistical-methods-algorithms","title":"Statistical Methods &amp; Algorithms","text":"Outlier Adjustment Methodology <p>Outlier adjustment is applied to any facility-month value flagged in Module 1 (<code>outlier_flag == 1</code>). The goal is to replace these outlier values using valid historical data from the same facility and indicator.</p> <p>Statistical Approach:</p> <p>Rolling averages are used to estimate expected values. A rolling average (also called moving average) is the mean of a set of time periods surrounding the target period. This technique smooths short-term fluctuations and highlights longer-term trends.</p> <p>Valid Values Definition:</p> <p>Only values meeting ALL of the following criteria are used in calculations:</p> <ul> <li><code>count &gt; 0</code> (positive non-zero values)</li> <li><code>!is.na(count)</code> (non-missing)</li> <li><code>outlier_flag == 0</code> (not flagged as outlier)</li> </ul> <p>Implementation:</p> <p>The module uses <code>frollmean()</code> from the <code>zoo</code> package for efficient rolling calculations:</p> <pre><code>data_adj[, valid_count := fifelse(outlier_flag == 0L &amp; !is.na(count), count, NA_real_)]\ndata_adj[, `:=`(\n  roll6   = frollmean(valid_count, 6, na.rm = TRUE, align = \"center\"),\n  fwd6    = frollmean(valid_count, 6, na.rm = TRUE, align = \"left\"),\n  bwd6    = frollmean(valid_count, 6, na.rm = TRUE, align = \"right\"),\n  fallback= mean(valid_count, na.rm = TRUE)\n), by = .(facility_id, indicator_common_id)]\n</code></pre> Adjustment Hierarchy for Outliers <p>The adjustment process follows this hierarchical order (stopping at the first available method):</p> <ol> <li> <p>Centered 6-Month Average (<code>roll6</code>)</p> <ul> <li>Uses the three months before and three months after the outlier month</li> <li>Provides a balanced average based on nearby trends</li> <li>Applied when enough valid values exist on both sides of the month</li> <li>Method tag: <code>roll6</code></li> </ul> </li> <li> <p>Forward-Looking 6-Month Average (<code>fwd6</code>)</p> <ul> <li>Used if the centered average can't be calculated (e.g. early in the time series)</li> <li>Takes the average of the next six valid months</li> <li>Method tag: <code>forward</code></li> </ul> </li> <li> <p>Backward-Looking 6-Month Average (<code>bwd6</code>)</p> <ul> <li>Used if neither <code>roll6</code> nor <code>fwd6</code> are available</li> <li>Takes the average of the six most recent valid months before the outlier</li> <li>Method tag: <code>backward</code></li> </ul> </li> <li> <p>Same Month from Previous Year</p> <ul> <li>If no valid 6-month average exists, the value from the same calendar month in the previous year is used (e.g., Jan 2023 for Jan 2024)</li> <li>Only applied if that previous value is valid (not an outlier, and &gt; 0)</li> <li>Particularly useful for seasonal indicators (e.g., malaria, respiratory infections)</li> <li>Method tag: <code>same_month_last_year</code></li> <li>Implementation:</li> </ul> <pre><code>data_adj[, `:=`(mm = month(date), yy = year(date))]\ndata_adj &lt;- data_adj[, {\n  for (i in which(outlier_flag == 1L &amp; is.na(adj_method))) {\n    j &lt;- which(mm == mm[i] &amp; yy == yy[i] - 1 &amp; outlier_flag == 0L &amp; !is.na(count))\n    if (length(j) == 1L) {\n      count_working[i] &lt;- count[j]\n      adj_method[i]    &lt;- \"same_month_last_year\"\n      adjust_note[i]   &lt;- format(date[j], \"%b-%Y\")\n    }\n  }\n  .SD\n}, by = .(facility_id, indicator_common_id)]\n</code></pre> </li> <li> <p>Mean of All Historical Values (Fallback)</p> <ul> <li>If all previous methods fail, the mean of all valid historical values for that facility-indicator is used</li> <li>Provides a facility-specific baseline when no temporal pattern is available</li> <li>Method tag: <code>fallback</code></li> </ul> </li> </ol> <p>Edge Case:</p> <p>If no valid replacement can be found from any of these methods, the original outlier value is retained.</p> Completeness Adjustment Methodology <p>Completeness adjustment is applied to any facility-month where:</p> <ul> <li>The month is flagged as incomplete (<code>completeness_flag != 1</code>) in Module 1, OR</li> <li>The value is missing (<code>is.na(count_working)</code>)</li> </ul> <p>Statistical Approach:</p> <p>The same rolling average methodology is applied, but the definition of \"valid values\" differs slightly:</p> <p>Valid Values for Completeness Adjustment:</p> <ul> <li><code>!is.na(count_working)</code> (non-missing, possibly already adjusted for outliers)</li> <li><code>outlier_flag == 0</code> (not flagged as outlier in original data)</li> </ul> <p>Key Difference from Outlier Adjustment:</p> <ul> <li>Completeness adjustment can use values that were already adjusted for outliers (when scenarios include both adjustments)</li> <li>No same-month-last-year method is used (only rolling averages and fallback)</li> </ul> <p>Implementation:</p> <pre><code>data_adj[, valid_count := fifelse(!is.na(count_working) &amp; outlier_flag == 0L, count_working, NA_real_)]\ndata_adj[, `:=`(\n  roll6   = frollmean(valid_count, 6, na.rm = TRUE, align = \"center\"),\n  fwd6    = frollmean(valid_count, 6, na.rm = TRUE, align = \"left\"),\n  bwd6    = frollmean(valid_count, 6, na.rm = TRUE, align = \"right\"),\n  fallback= mean(valid_count, na.rm = TRUE)\n), by = .(facility_id, indicator_common_id)]\n</code></pre> Adjustment Hierarchy for Completeness <p>The replacement follows this hierarchical order:</p> <ol> <li> <p>Centered 6-Month Average (<code>roll6</code>)</p> <ul> <li>Uses three valid months before and after the missing or incomplete month</li> <li>Preferred method when sufficient surrounding data exists</li> <li>Method tag: <code>roll6</code></li> </ul> </li> <li> <p>Forward-Looking 6-Month Average (<code>fwd6</code>)</p> <ul> <li>Used if the centered average cannot be calculated (e.g., at start of time series)</li> <li>Method tag: <code>forward</code></li> </ul> </li> <li> <p>Backward-Looking 6-Month Average (<code>bwd6</code>)</p> <ul> <li>Used if no centered or forward-looking values are available (e.g., at end of time series)</li> <li>Method tag: <code>backward</code></li> </ul> </li> <li> <p>Mean of All Historical Values (Fallback)</p> <ul> <li>If no rolling averages can be calculated, uses the mean of all valid values for that facility-indicator</li> <li>Provides a facility-specific baseline</li> <li>Method tag: <code>fallback</code></li> </ul> </li> </ol> <p>Edge Case:</p> <p>If no valid replacement is found, the value remains missing (<code>NA</code>).</p> Scenario Processing Logic <p>The module processes all four adjustment scenarios simultaneously using the <code>apply_adjustments_scenarios()</code> function:</p> <p>Scenario 1: None (<code>count_final_none</code>)</p> <ul> <li><code>adjust_outliers = FALSE</code>, <code>adjust_completeness = FALSE</code></li> <li>Original raw data with no modifications</li> <li>Serves as baseline for comparison</li> </ul> <p>Scenario 2: Outliers (<code>count_final_outliers</code>)</p> <ul> <li><code>adjust_outliers = TRUE</code>, <code>adjust_completeness = FALSE</code></li> <li>Only outlier values are replaced</li> <li>Missing/incomplete values remain as-is</li> <li>Use case: When completeness is high but outliers are a concern</li> </ul> <p>Scenario 3: Completeness (<code>count_final_completeness</code>)</p> <ul> <li><code>adjust_outliers = FALSE</code>, <code>adjust_completeness = TRUE</code></li> <li>Only missing/incomplete values are imputed</li> <li>Outliers are retained in the data</li> <li>Use case: When data quality is good but reporting is sporadic</li> </ul> <p>Scenario 4: Both (<code>count_final_both</code>)</p> <ul> <li><code>adjust_outliers = TRUE</code>, <code>adjust_completeness = TRUE</code></li> <li>Sequential processing: Outliers adjusted first, then completeness</li> <li>Most comprehensive adjustment</li> <li>Use case: When both data quality issues are prevalent</li> </ul> <p>Processing Order for \"Both\" Scenario:</p> <ol> <li>Outlier adjustment creates <code>count_working</code> with outliers replaced</li> <li>Completeness adjustment then operates on <code>count_working</code>, using the already-adjusted values</li> <li>This ensures completeness imputation uses cleaned (non-outlier) values when available</li> </ol> <p>Important:</p> <p>After scenario-specific adjustments, excluded indicators (deaths) are reset to their original values:</p> <pre><code>dat[indicator_common_id %in% EXCLUDED_FROM_ADJUSTMENT, count_working := count]\n</code></pre> Aggregation Methods <p>All geographic aggregations use simple sums:</p> <pre><code>sum(count_final_both, na.rm = TRUE)\n</code></pre> <p>Rationale:</p> <ul> <li>Service volumes are additive (e.g., total deliveries = sum of facility deliveries)</li> <li>Missing values (<code>NA</code>) are treated as zero in aggregation</li> <li>Consistent with standard HMIS reporting practices</li> </ul> <p>Caution:</p> <p>If many facilities have <code>NA</code> values after adjustment, subnational/national totals may be underestimated. The <code>count_final_none</code> scenario provides a reference point for assessing impact.</p> Handling Missing Data in Calculations <p>The module applies <code>na.rm = TRUE</code> in all rolling calculations:</p> <pre><code>frollmean(valid_count, 6, na.rm = TRUE, align = \"center\")\n</code></pre> <p>Implication:</p> <p>Rolling averages are calculated from available valid values only. If fewer than 6 values exist, the average is computed from whatever is available. If no valid values exist, the result is <code>NA</code>.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#code-examples","title":"Code Examples","text":"Example 1: Outlier Adjustment <p>Scenario:</p> <p>A facility reports an unusually high ANC1 visit count in March 2023.</p> <p>Data:</p> <pre><code>period_id | count | outlier_flag | Surrounding valid values\n----------|-------|--------------|-------------------------\n202301    | 145   | 0            | valid\n202302    | 152   | 0            | valid\n202303    | 890   | 1            | OUTLIER\n202304    | 148   | 0            | valid\n202305    | 155   | 0            | valid\n202306    | 147   | 0            | valid\n</code></pre> <p>Adjustment Calculation (centered 6-month average):</p> <ul> <li>Valid values: [145, 152, 148, 155, 147] (excludes outlier 890)</li> <li>Average: (145 + 152 + 148 + 155 + 147) / 5 = 149.4</li> <li>Adjusted value: 149.4</li> </ul> <p>Method used:</p> <p><code>roll6</code></p> Example 2: Completeness Adjustment <p>Scenario:</p> <p>A facility fails to report malaria tests in February 2023.</p> <p>Data:</p> <pre><code>period_id | count | completeness_flag | Surrounding valid values\n----------|-------|-------------------|-------------------------\n202301    | 45    | 1                 | valid\n202302    | NA    | 0                 | INCOMPLETE\n202303    | 48    | 1                 | valid\n202304    | 52    | 1                 | valid\n202305    | 50    | 1                 | valid\n</code></pre> <p>Adjustment Calculation (centered 6-month average):</p> <ul> <li>Valid values: [45, 48, 52, 50, ...]</li> <li>Average: 48.75 (using available surrounding months)</li> <li>Imputed value: 48.75</li> </ul> <p>Method used:</p> <p><code>roll6</code></p> Example 3: Seasonal Indicator with Same-Month-Last-Year <p>Scenario:</p> <p>Malaria cases show strong seasonality, and a June 2023 outlier needs adjustment.</p> <p>Data:</p> <pre><code>period_id | count | outlier_flag | Notes\n----------|-------|--------------|-------\n202206    | 234   | 0            | June 2022 (valid)\n202306    | 1850  | 1            | June 2023 (OUTLIER)\n</code></pre> <p>Adjustment Logic:</p> <ol> <li>Centered, forward, and backward rolling averages unavailable (insufficient data)</li> <li>Same-month-last-year method activated</li> <li>June 2022 value = 234 (valid)</li> <li>Adjusted value: 234</li> </ol> <p>Method used:</p> <p><code>same_month_last_year</code></p> Example 4: Scenario Comparison <p>Facility:</p> <p>FAC001</p> <p>Indicator:</p> <p>Institutional deliveries</p> <p>Period:</p> <p>Q1 2023</p> <p>Original Data:</p> <pre><code>Month    | Count | Outlier? | Complete?\n---------|-------|----------|----------\nJan 2023 | 78    | No       | Yes\nFeb 2023 | 450   | Yes      | Yes       # Outlier\nMar 2023 | NA    | -        | No        # Incomplete\n</code></pre> <p>Scenario Results:</p> Month None Outliers Completeness Both Jan 2023 78 78 78 78 Feb 2023 450 82* 450 82* Mar 2023 NA NA 80** 80** <p>*Adjusted using rolling average</p> <p>**Imputed using rolling average</p> <p>Interpretation:</p> <ul> <li>None: Raw data with obvious issues</li> <li>Outliers: February corrected, but March remains missing</li> <li>Completeness: March filled in, but February outlier retained</li> <li>Both: Most complete and clean dataset</li> </ul> Example 5: Geographic Aggregation <p>Subnational Aggregation Code:</p> <pre><code>adjusted_data_admin_area_final &lt;- adjusted_data_export[\n  ,\n  .(\n    count_final_none         = sum(count_final_none,         na.rm = TRUE),\n    count_final_outliers     = sum(count_final_outliers,     na.rm = TRUE),\n    count_final_completeness = sum(count_final_completeness, na.rm = TRUE),\n    count_final_both         = sum(count_final_both,         na.rm = TRUE)\n  ),\n  by = c(geo_admin_area_sub, \"indicator_common_id\", \"period_id\")\n]\n</code></pre> <p>National Aggregation Code:</p> <pre><code>adjusted_data_national_final &lt;- adjusted_data_export[\n  ,\n  .(\n    count_final_none         = sum(count_final_none,         na.rm = TRUE),\n    count_final_outliers     = sum(count_final_outliers,     na.rm = TRUE),\n    count_final_completeness = sum(count_final_completeness, na.rm = TRUE),\n    count_final_both         = sum(count_final_both,         na.rm = TRUE)\n  ),\n  by = .(admin_area_1, indicator_common_id, period_id)\n]\n</code></pre>"},{"location":"02_module_data_quality_adjustments_documentation/#troubleshooting","title":"Troubleshooting","text":"Common Issues <p>Issue 1: All values remain unadjusted</p> <p>Possible causes:</p> <ul> <li>Indicator is in the excluded list (deaths)</li> <li>Indicator flagged as low-volume</li> <li>No outlier or completeness flags in input data</li> </ul> <p>Solution:</p> <p>Check <code>M2_low_volume_exclusions.csv</code> and verify Module 1 outputs contain flags</p> <p>Issue 2: Adjusted values seem unreasonable</p> <p>Possible causes:</p> <ul> <li>Insufficient valid historical data for rolling averages</li> <li>Genuine program changes being smoothed out</li> <li>Seasonal patterns not captured by 6-month window</li> </ul> <p>Solution:</p> <ul> <li>Review facility-specific time series plots</li> <li>Consider using \"outliers only\" scenario if completeness is good</li> <li>Validate against program implementation records</li> </ul> <p>Issue 3: Many NA values after adjustment</p> <p>Possible causes:</p> <ul> <li>Facility has very sparse data</li> <li>No valid values available for any adjustment method</li> <li>Early months in time series lack historical data</li> </ul> <p>Solution:</p> <ul> <li>Expected for facilities with limited reporting history</li> <li>Consider facility-level data quality filtering</li> <li>National/subnational aggregates will sum available values</li> </ul> <p>Issue 4: Subnational/national totals don't match expectations</p> <p>Possible causes:</p> <ul> <li>NA values treated as zero in aggregation</li> <li>Different scenarios produce different totals</li> <li>Low reporting completeness overall</li> </ul> <p>Solution:</p> <ul> <li>Compare <code>count_final_none</code> vs <code>count_final_both</code> to assess adjustment impact</li> <li>Review Module 1 completeness statistics</li> <li>Consider data quality threshold for inclusion</li> </ul> Quality Assurance Checks <p>The module includes several quality checks:</p> <ol> <li>Low Volume Exclusions: Automatically identifies and excludes indicators with zero high-value observations</li> <li>Adjustment Tracking: Counts and reports number of values adjusted by each method</li> <li>Excluded Indicators: Ensures deaths are never adjusted</li> <li>Console Logging: Provides detailed progress and summary statistics</li> </ol> <p>Example Console Output:</p> <pre><code>Running adjustments...\n -&gt; Adjusting outliers...\n     Roll6 adjusted: 1,245\n     Forward-filled: 89\n     Backward-filled: 67\n     Same-month LY: 34\n     Fallback mean: 12\n -&gt; Adjusting for completeness...\n     Roll6 filled: 2,103\n     Forward-filled: 234\n     Backward-filled: 178\n     Fallback mean: 45\n</code></pre>"},{"location":"02_module_data_quality_adjustments_documentation/#usage-notes-recommendations","title":"Usage Notes &amp; Recommendations","text":"Choosing the Right Scenario Situation Recommended Scenario Rationale High data quality, minimal issues <code>none</code> No adjustment needed Sporadic outliers, good completeness <code>outliers</code> Address quality without imputation Good quality, poor reporting frequency <code>completeness</code> Fill gaps while preserving actual values Poor quality and completeness <code>both</code> Comprehensive cleaning Uncertainty about data quality Compare all scenarios Sensitivity analysis Validation Steps <p>After running this module, consider:</p> <ol> <li>Compare scenarios: Examine differences between <code>count_final_none</code> and <code>count_final_both</code></li> <li>Review exclusions: Check <code>M2_low_volume_exclusions.csv</code> for unexpected indicators</li> <li>Aggregate analysis: Ensure subnational and national totals are reasonable</li> <li>Temporal plots: Visualize trends before/after adjustment to identify over-smoothing</li> <li>Facility-level spot checks: Review adjustments for a sample of facilities</li> </ol> Limitations <ol> <li> <p>Rolling windows assume stability: Adjustments work best when service delivery is relatively stable. Genuine program changes (e.g., new campaigns) may be incorrectly smoothed.</p> </li> <li> <p>No adjustment uncertainty: The module provides point estimates without confidence intervals. Adjusted values should be treated as estimates.</p> </li> <li> <p>Facility-specific adjustments: No cross-facility borrowing of information. Facilities with very sparse data may have unstable adjustments.</p> </li> <li> <p>Seasonal patterns: While same-month-last-year helps, strong within-year seasonality may not be fully captured by 6-month windows.</p> </li> <li> <p>NA treatment in aggregation: Missing values are treated as zero when summing to higher geographic levels, which may underestimate totals if missingness is high.</p> </li> </ol> <p>Last updated: 10-11-2025 Contact: FASTR Project Team</p>"},{"location":"03_module_service_utilization_documentation/","title":"Module 3: Service Utilization","text":""},{"location":"03_module_service_utilization_documentation/#1-overview-what-why","title":"1. Overview (What &amp; Why)","text":""},{"location":"03_module_service_utilization_documentation/#what-does-this-module-do","title":"What does this module do?","text":"<p>The Service Utilization module analyzes health service delivery patterns to detect and quantify disruptions in service volumes over time. It identifies when health services deviate significantly from expected patterns and measures the magnitude of these disruptions at national, provincial, and district levels.</p> <p>Using statistical process control methods and regression analysis, the module compares actual service volumes against historical trends and seasonal patterns. This helps distinguish between normal fluctuations (like expected increases in malaria cases during rainy season) and genuine disruptions that require investigation (like sudden drops in antenatal care during a pandemic or conflict).</p> <p>The analysis produces quantified estimates of service shortfalls and surpluses, enabling evidence-based resource allocation, policy decisions, and health system monitoring.</p>"},{"location":"03_module_service_utilization_documentation/#why-is-it-needed-in-the-fastr-pipeline","title":"Why is it needed in the FASTR pipeline?","text":"<p>Service utilization data reflects how populations access essential healthcare, but this data can fluctuate due to various factors: seasonal patterns, policy changes, external shocks (pandemics, natural disasters, conflicts), data quality issues, or service availability problems. Without systematic analysis, it's difficult to know whether observed changes represent normal variation or significant disruptions requiring action.</p> <p>This module provides objective, data-driven identification of service delivery problems and quantifies their impact. It enables health system managers to detect emerging issues early, target resources to affected areas, and monitor recovery after disruptions. The module's outputs support dashboard monitoring, impact assessments, and policy evaluation.</p>"},{"location":"03_module_service_utilization_documentation/#quick-summary","title":"Quick Summary","text":"Component Details Inputs Raw HMIS data (<code>hmis_ISO3.csv</code>)Outlier flags from Module 1 (<code>M1_output_outliers.csv</code>)Adjusted service volumes from Module 2 (<code>M2_adjusted_data.csv</code>) Outputs Disruption flags (<code>M3_chartout.csv</code>)Quantified impacts by geographic level (<code>M3_disruptions_analysis_*.csv</code>)Shortfall/surplus summaries (<code>M3_all_indicators_shortfalls_*.csv</code>) Purpose Detect and quantify service delivery disruptions through two-stage analysis: control charts identify when disruptions occur, panel regression quantifies their magnitude"},{"location":"03_module_service_utilization_documentation/#2-how-it-works","title":"2. How It Works","text":""},{"location":"03_module_service_utilization_documentation/#high-level-workflow","title":"High-Level Workflow","text":"<p>The module operates in two sequential stages, each with a distinct purpose:</p> <p>Stage 1: Control Chart Analysis - Identifies unusual patterns in service volumes</p> <ol> <li> <p>Prepare the data: Load health service data, remove previously identified outliers, aggregate to the appropriate geographic level, and fill in missing months using interpolation.</p> </li> <li> <p>Model expected patterns: For each combination of health indicator and geographic area, use robust statistical methods to estimate what service volumes should look like based on historical trends and seasonal patterns (e.g., accounting for predictable increases in malaria cases during rainy season).</p> </li> <li> <p>Detect deviations: Compare actual service volumes to expected patterns and identify significant deviations using multiple detection rules:</p> </li> <li>Sharp disruptions: Single months with extreme deviations</li> <li>Sustained drops: Gradual declines over several months</li> <li>Sustained dips: Periods consistently below expected levels</li> <li>Sustained rises: Periods consistently above expected levels</li> <li> <p>Missing data patterns: Gaps in reporting that may signal problems</p> </li> <li> <p>Flag disrupted periods: Mark months where any disruption pattern is detected, ensuring recent months are always flagged for review.</p> </li> </ol> <p>Stage 2: Disruption Analysis - Quantifies the impact of identified disruptions</p> <ol> <li> <p>Apply regression models: Use panel regression at multiple geographic levels (national, provincial, district) to estimate how much service volumes changed during flagged disruption periods, controlling for trends and seasonality.</p> </li> <li> <p>Calculate shortfalls and surpluses: Compare predicted volumes to actual volumes to quantify the magnitude of disruptions in absolute numbers and percentages.</p> </li> <li> <p>Generate outputs: Create summary files showing disruption impacts at each geographic level, ready for visualization and reporting.</p> </li> </ol>"},{"location":"03_module_service_utilization_documentation/#workflow-diagram","title":"Workflow Diagram","text":""},{"location":"03_module_service_utilization_documentation/#key-decision-points","title":"Key Decision Points","text":"<p>Geographic Level Selection: The module can analyze disruptions at different geographic scales. You can choose to run analysis at national and provincial levels only (faster, suitable for routine monitoring) or include district and ward levels (slower, provides detailed local information for targeted interventions).</p> <p>Sensitivity Settings: The module uses configurable thresholds to determine what constitutes a \"disruption.\" More sensitive settings (lower thresholds) will flag smaller deviations, useful for early warning systems. More conservative settings (higher thresholds) will only flag major disruptions, useful for focusing on critical issues.</p> <p>Data Completeness Approach: The module accepts different versions of service counts from Module 2, allowing you to choose whether to adjust for reporting completeness or use raw counts.</p>"},{"location":"03_module_service_utilization_documentation/#what-happens-to-the-data","title":"What Happens to the Data","text":"<p>Input Transformation: The module starts with facility-level service counts (e.g., number of deliveries at each clinic each month) and aggregates them to geographic areas (provinces, districts). Outliers identified in Module 1 are removed to prevent anomalous data points from skewing the analysis.</p> <p>Pattern Detection: Using robust statistical methods, the module learns what \"normal\" looks like for each service and area based on historical patterns. It then identifies months where actual volumes deviate significantly from these patterns, accounting for predictable variations like seasonal changes.</p> <p>Impact Quantification: For months flagged as disrupted, the module uses regression models to estimate what service volumes would have been without the disruption. By comparing predicted to actual volumes, it calculates how many services were missed (shortfalls) or how much service delivery increased (surpluses).</p> <p>Output Generation: The final outputs provide disruption impacts at multiple geographic scales, enabling users to see both national-level summaries and local-level details. All calculations preserve the original data while adding predicted values and disruption metrics.</p>"},{"location":"03_module_service_utilization_documentation/#analysis-outputs-and-visualization","title":"Analysis Outputs and Visualization","text":"<p>The FASTR analysis generates four main visual outputs for disruption analysis:</p> <p>1. Change in Service Volume</p> <p>Time series showing percentage change in service volumes over time, identifying periods of significant increases or decreases across indicators.</p> <p></p> <p>2. Actual vs Expected Services (National)</p> <p>Line charts comparing actual service delivery to expected volumes at the national level, with disrupted periods highlighted.</p> <p></p> <p>3. Actual vs Expected Services (Subnational)</p> <p>Line charts comparing actual service delivery to expected volumes at admin area 2 level, enabling identification of geographic variation in disruptions.</p> <p></p> <p>4. Volume Change Due to Data Quality Adjustments</p> <p>Comparison showing the impact of data quality adjustments from Module 2 on disruption detection and volume estimates.</p> <p></p> <p>Interpretation Guide: - Disrupted periods: Indicated by shaded regions or highlighting where actual volumes deviate significantly from expected - Positive deviations: Service volumes exceed expected (surpluses) - Negative deviations: Service volumes fall below expected (shortfalls) - Control limits: Statistical thresholds indicating normal variation vs. significant disruption</p>"},{"location":"03_module_service_utilization_documentation/#3-detailed-reference","title":"3. Detailed Reference","text":""},{"location":"03_module_service_utilization_documentation/#configuration-parameters","title":"Configuration Parameters","text":"Core Analysis Parameters Parameter Default Type Description Tuning Guidance <code>COUNTRY_ISO3</code> \"ISO3\" String Three-letter country code Set to your country code (e.g., \"RWA\", \"UGA\", \"ZMB\") <code>SELECTEDCOUNT</code> \"count_final_both\" String Data column used for analysis Options: <code>count_final_none</code>, <code>count_final_completeness</code>, <code>count_final_both</code> <code>VISUALIZATIONCOUNT</code> \"count_final_both\" String Data column used for visualization Should match or complement <code>SELECTEDCOUNT</code> Control Chart Parameters Parameter Default Type Description Tuning Guidance <code>SMOOTH_K</code> 7 Integer (odd) Rolling median window size in months Larger values = smoother trends, less sensitivity. Must be odd number (e.g., 5, 7, 9, 11) <code>MADS_THRESHOLD</code> 1.5 Numeric MAD units threshold for sharp disruptions Lower = more sensitive (e.g., 1.0), higher = more conservative (e.g., 2.0) <code>DIP_THRESHOLD</code> 0.90 Numeric Proportion threshold for sustained dips 0.90 = flag if below 90% of expected (10% drop). Use 0.80 for 20% drop threshold <code>DIFFPERCENT</code> 10 Numeric Percentage threshold for plotting disruptions If actual differs from predicted by &gt;10%, use predicted value in visualizations <p>Note: <code>RISE_THRESHOLD</code> is automatically calculated as <code>1 / DIP_THRESHOLD</code> (default: ~1.11) to mirror dip detection symmetrically.</p> Geographic Analysis Parameters Parameter Default Type Description Tuning Guidance <code>CONTROL_CHART_LEVEL</code> Auto-set String Geographic level for control charts Automatically set based on <code>RUN_DISTRICT_MODEL</code> and <code>RUN_ADMIN_AREA_4_ANALYSIS</code> <code>RUN_DISTRICT_MODEL</code> FALSE Logical Whether to run admin_area_3 regressions Set TRUE for district-level analysis (increases runtime) <code>RUN_ADMIN_AREA_4_ANALYSIS</code> FALSE Logical Whether to run admin_area_4 analysis Set TRUE for finest-level analysis (very slow for large datasets) Data Source Parameters Parameter Default Type Description <code>PROJECT_DATA_HMIS</code> \"hmis_ISO3.csv\" String Filename for raw HMIS data Parameter Selection Guide <p>For High-Sensitivity Analysis (detecting smaller disruptions): - <code>MADS_THRESHOLD = 1.0</code> - <code>DIP_THRESHOLD = 0.95</code> (5% drop) - <code>SMOOTH_K = 5</code> (less smoothing)</p> <p>For Conservative Analysis (only major disruptions): - <code>MADS_THRESHOLD = 2.0</code> - <code>DIP_THRESHOLD = 0.80</code> (20% drop) - <code>SMOOTH_K = 9</code> or <code>11</code> (more smoothing)</p> <p>For Faster Runtime: - <code>RUN_DISTRICT_MODEL = FALSE</code> - <code>RUN_ADMIN_AREA_4_ANALYSIS = FALSE</code> - <code>CONTROL_CHART_LEVEL = \"admin_area_2\"</code></p>"},{"location":"03_module_service_utilization_documentation/#inputoutput-specifications","title":"Input/Output Specifications","text":"Input Requirements"},{"location":"03_module_service_utilization_documentation/#primary-inputs","title":"Primary Inputs","text":"<ol> <li><code>hmis_ISO3.csv</code> (country-specific HMIS file where ISO3 is the 3-letter country code)</li> <li>Raw HMIS service utilization data</li> <li> <p>Required columns: <code>facility_id</code>, <code>admin_area_1</code>, <code>admin_area_2</code>, <code>admin_area_3</code>, <code>admin_area_4</code>, <code>indicator_common_id</code>, <code>period_id</code>, service count columns</p> </li> <li> <p><code>M1_output_outliers.csv</code></p> </li> <li>Output from Module 1 (Data Quality Assessment)</li> <li>Contains <code>outlier_flag</code> to exclude anomalous data points</li> <li> <p>Required columns: <code>facility_id</code>, <code>indicator_common_id</code>, <code>period_id</code>, <code>outlier_flag</code></p> </li> <li> <p><code>M2_adjusted_data.csv</code></p> </li> <li>Output from Module 2 (Data Quality Adjustments)</li> <li>Contains adjusted service counts with different completeness assumptions</li> <li>Required columns: <code>facility_id</code>, <code>indicator_common_id</code>, <code>period_id</code>, <code>count_final_none</code>, <code>count_final_completeness</code>, <code>count_final_both</code></li> </ol>"},{"location":"03_module_service_utilization_documentation/#data-requirements","title":"Data Requirements","text":"<ul> <li>Temporal coverage: Minimum 12 months of data for seasonal modeling</li> <li>Data completeness: Missing months are filled using interpolation</li> <li>Geographic completeness: Data at specified administrative levels</li> <li>Count data: Non-negative integer counts (predictions are bounded at zero)</li> </ul>"},{"location":"03_module_service_utilization_documentation/#outputs","title":"Outputs","text":""},{"location":"03_module_service_utilization_documentation/#1-control-chart-results","title":"1. Control Chart Results","text":"<p><code>M3_chartout.csv</code></p> <p>Purpose: Contains flagged disruptions from the control chart analysis</p> <p>Columns:</p> <ul> <li><code>admin_area_*</code>: Geographic identifier (level depends on <code>CONTROL_CHART_LEVEL</code>)</li> <li><code>indicator_common_id</code>: Health service indicator code</li> <li><code>period_id</code>: Time period in YYYYMM format</li> <li><code>tagged</code>: Binary flag (1 = disruption detected, 0 = normal)</li> <li><code>count_original</code>: Actual service volume</li> <li><code>count_predict</code>: Predicted volume from regression</li> <li><code>count_smooth</code>: Smoothed prediction</li> <li><code>residual</code>: Deviation from expected</li> <li><code>robust_control</code>: Standardized residual</li> <li><code>tag_sharp</code>, <code>tag_sustained</code>, <code>tag_sustained_dip</code>, <code>tag_sustained_rise</code>, <code>tag_missing</code>: Individual disruption flags</li> </ul> <p>Use: Identifies which months require further investigation for each indicator-geography combination</p> <p><code>M3_service_utilization.csv</code></p> <p>Purpose: Pass-through copy of adjusted data for visualization</p> <p>Source: Direct copy of <code>M2_adjusted_data.csv</code></p> <p>Use: Provides baseline data for plotting actual service volumes</p> <p><code>M3_memory_log.txt</code></p> <p>Purpose: Tracks memory usage throughout execution</p> <p>Use: Diagnostics for performance optimization and troubleshooting</p>"},{"location":"03_module_service_utilization_documentation/#2-disruption-analysis-results","title":"2. Disruption Analysis Results","text":"<p><code>M3_disruptions_analysis_admin_area_1.csv</code> (National level - always generated)</p> <p>Columns:</p> <ul> <li><code>admin_area_1</code>: Country name</li> <li><code>indicator_common_id</code>: Health service indicator</li> <li><code>period_id</code>: Time period (YYYYMM)</li> <li><code>count_sum</code>: Actual service volume (sum across all facilities)</li> <li><code>count_expect_sum</code>: Expected service volume (sum of predictions)</li> <li><code>count_expected_if_above_diff_threshold</code>: Value for plotting (expected if |difference| &gt; DIFFPERCENT, otherwise actual)</li> </ul> <p><code>M3_disruptions_analysis_admin_area_2.csv</code> (Province level - always generated)</p> <p>Additional column: <code>admin_area_2</code> (province/region name)</p> <p>Same structure: As admin_area_1 file but disaggregated by province</p> <p><code>M3_disruptions_analysis_admin_area_3.csv</code> (District level - conditional)</p> <p>Generated when: <code>RUN_DISTRICT_MODEL = TRUE</code></p> <p>Additional columns: <code>admin_area_2</code>, <code>admin_area_3</code></p> <p>Same structure: As above but disaggregated by district</p> <p><code>M3_disruptions_analysis_admin_area_4.csv</code> (Ward level - conditional)</p> <p>Generated when: <code>RUN_ADMIN_AREA_4_ANALYSIS = TRUE</code></p> <p>Additional columns: <code>admin_area_2</code>, <code>admin_area_3</code>, <code>admin_area_4</code></p> <p>Warning: Very large file size for countries with many wards</p>"},{"location":"03_module_service_utilization_documentation/#3-shortfallsurplus-summary-files","title":"3. Shortfall/Surplus Summary Files","text":"<p><code>M3_all_indicators_shortfalls_admin_area_*.csv</code> (one for each geographic level)</p> <p>Purpose: Pre-calculated shortfall and surplus metrics for reporting</p> <p>Common columns:</p> <ul> <li>Geographic identifier(s): <code>admin_area_*</code></li> <li><code>indicator_common_id</code>: Health service indicator</li> <li><code>period_id</code>: Time period (YYYYMM)</li> <li><code>count_sum</code>: Actual service volume</li> <li><code>count_expect_sum</code>: Expected service volume</li> <li><code>shortfall_absolute</code>: Absolute number of missing services (if negative disruption)</li> <li><code>shortfall_percent</code>: Percentage shortfall relative to expected</li> <li><code>surplus_absolute</code>: Absolute number of excess services (if positive disruption)</li> <li><code>surplus_percent</code>: Percentage surplus relative to expected</li> </ul> <p>Note: If optional geographic levels are disabled, empty placeholder files are created for compatibility with downstream processes.</p>"},{"location":"03_module_service_utilization_documentation/#temporary-files-automatically-cleaned","title":"Temporary Files (Automatically Cleaned)","text":"<p>During execution, the module creates temporary batch files for memory management: - <code>M3_temp_controlchart_batch_*.csv</code> - <code>M3_temp_indicator_batch_*.csv</code> - <code>M3_temp_province_batch_*.csv</code> - <code>M3_temp_district_batch_*.csv</code> - <code>M3_temp_admin4_batch_*.csv</code></p> <p>These are automatically deleted upon successful completion. If the script crashes, these files may remain and will be cleaned up on the next run.</p>"},{"location":"03_module_service_utilization_documentation/#key-functions-documentation","title":"Key Functions Documentation","text":"<code>robust_control_chart(panel_data, selected_count)</code> <p>Purpose: Identifies anomalies in service utilization using robust regression and MAD-based control limits.</p> <p>Inputs:</p> <ul> <li><code>panel_data</code>: Time series data for a specific indicator-geography combination</li> <li><code>selected_count</code>: Column name containing service volume counts to analyze</li> </ul> <p>Process:</p> <ol> <li>Fits a robust linear model (using <code>MASS::rlm()</code>) with seasonal controls and time trends</li> <li>Applies rolling median smoothing to predicted values to reduce noise</li> <li>Calculates residuals and standardizes them using Median Absolute Deviation (MAD)</li> <li>Applies rule-based tagging logic to identify different disruption types</li> <li>Flags recent months automatically to ensure timely detection</li> </ol> <p>Outputs:</p> <ul> <li><code>count_predict</code>: Predicted service volume from robust regression</li> <li><code>count_smooth</code>: Smoothed predictions using rolling median</li> <li><code>residual</code>: Difference between actual and smoothed values</li> <li><code>robust_control</code>: Standardized residual (residual/MAD)</li> <li><code>tagged</code>: Binary flag (1 = disruption detected, 0 = normal variation)</li> <li>Additional flags: <code>tag_sharp</code>, <code>tag_sustained</code>, <code>tag_sustained_dip</code>, <code>tag_sustained_rise</code>, <code>tag_missing</code></li> </ul> <p>Key Features:</p> <ul> <li>Handles missing data gracefully with interpolation</li> <li>Uses robust regression to minimize influence of outliers</li> <li>Employs multiple disruption detection rules for different patterns</li> <li>Ensures non-negative predictions (counts cannot be negative)</li> </ul>"},{"location":"03_module_service_utilization_documentation/#panel-regression-models","title":"Panel Regression Models","text":"<p>The disruption analysis uses fixed-effects panel regression models (<code>fixest::feols()</code>) at multiple geographic levels.</p> <p>Country-wide Model (Admin Area 1):</p> <pre><code>count ~ date + factor(month) + tagged\n</code></pre> <p>Clustered standard errors at district level (<code>admin_area_3</code>)</p> <p>Province-level Models (Admin Area 2):</p> <pre><code>count ~ date + factor(month) + tagged | admin_area_2\n</code></pre> <p>Separate regression for each province, clustered at district level</p> <p>District-level Models (Admin Area 3 - optional):</p> <pre><code>count ~ date + factor(month) + tagged | admin_area_3\n</code></pre> <p>Separate regression for each district, clustered at ward level (<code>admin_area_4</code>)</p> <p>Ward-level Models (Admin Area 4 - optional):</p> <pre><code>count ~ date + factor(month) + tagged | admin_area_4\n</code></pre> <p>Separate regression for each ward/finest unit</p>"},{"location":"03_module_service_utilization_documentation/#supporting-functions","title":"Supporting Functions","text":"<p><code>mem_usage(msg)</code>: Tracks and logs memory consumption throughout execution</p> <p>Data Processing:</p> <ul> <li>Batch processing with disk-based temporary files for memory efficiency</li> <li>Efficient data.table operations for large datasets</li> <li>Progressive aggregation and merging strategies</li> </ul>"},{"location":"03_module_service_utilization_documentation/#statistical-methods-algorithms","title":"Statistical Methods &amp; Algorithms","text":"Control Chart Analysis <p>Service volumes are aggregated at the specified geographic level (configurable via <code>CONTROL_CHART_LEVEL</code>). The pipeline removes outliers (<code>outlier_flag == 1</code>), fills in missing months, and filters low-volume months (&lt;50% of global mean volume).</p> <p>A robust regression model estimates expected service volumes per indicator \u00d7 geographic area (<code>panelvar</code>). A centered rolling median is applied to smooth the predicted values. Residuals (actual - smoothed) are standardized using MAD. Disruptions are identified using a rule-based tagging system.</p>"},{"location":"03_module_service_utilization_documentation/#disruption-detection-rules","title":"Disruption Detection Rules","text":"<p>Each rule is controlled by user-defined parameters, allowing customization of the sensitivity and behavior of the detection logic:</p> <p>Sharp Disruptions: Flags a single month when the standardized residual (residual divided by MAD) exceeds a threshold:</p> \\[ \\left| \\frac{\\text{residual}}{\\text{MAD}} \\right| \\geq \\text{MADS_THRESHOLD} \\] <ul> <li>Parameter: <code>MADS_THRESHOLD</code> (default: <code>1.5</code>)</li> <li>Lower values make the detection more sensitive to sudden spikes or dips.</li> </ul> <p>Sustained Drops: Flags a sustained drop if:</p> <ul> <li>Three consecutive months show mild deviations (standardized residual \u2265 1), and</li> <li>The final month also exceeds the <code>MADS_THRESHOLD</code>.</li> </ul> <p>This captures slower, compounding declines.</p> <p>Sustained Dips: Flags periods where the actual volume falls consistently below a defined proportion of expected volume (smoothed prediction):</p> \\[ \\text{count\\_original} &lt; \\text{DIP\\_THRESHOLD} \\times \\text{count\\_smooth} \\] <ul> <li>Parameter: <code>DIP_THRESHOLD</code> (default: <code>0.90</code>)</li> <li>Users can adjust this to detect deeper or shallower dips (e.g., <code>0.80</code> for a 20% drop).</li> </ul> <p>Sustained Rises: Symmetric to dips, flags periods of consistent overperformance:</p> \\[ \\text{count\\_original} &gt; \\text{RISE\\_THRESHOLD} \\times \\text{count\\_smooth} \\] <ul> <li>Parameter: <code>RISE_THRESHOLD</code> (default: <code>1 / DIP_THRESHOLD</code>, e.g., <code>1.11</code>)</li> <li>Users can adjust this to detect upward surges in volume.</li> </ul> <p>Missing Data: Flags when 2 or more of the past 3 months have missing (<code>NA</code>) or zero service volume.</p> <ul> <li>Fixed rule.</li> </ul> <p>Recent Tail Override: Automatically flags all months in the last 6 months of data to ensure recent trends are reviewed, even if model-based tagging is not conclusive.</p> <ul> <li>Fixed rule.</li> </ul> <p>Final Flag: A month is assigned <code>tagged = 1</code> if any of the following conditions are met:</p> <ul> <li><code>tag_sharp == 1</code></li> <li><code>tag_sustained == 1</code></li> <li><code>tag_sustained_dip == 1</code></li> <li><code>tag_sustained_rise == 1</code></li> <li><code>tag_missing == 1</code></li> <li>It falls within the most recent 6 months (<code>last_6_months == 1</code>)</li> </ul>"},{"location":"03_module_service_utilization_documentation/#robust-regression-model","title":"Robust Regression Model","text":"<p>Model fitting:</p> <p>If \u226512 observations and &gt;12 unique dates:</p> \\[Y_{it} = \\beta_0 + \\sum \\gamma_m \\cdot \\text{month}_m + \\beta_1 \\cdot \\text{date} + \\epsilon_{it}\\] <p>If only \u226512 observations:</p> \\[Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\epsilon_{it}\\] <p>If insufficient data: use the median of observed values.</p> <p>Apply rolling median smoothing to predictions:</p> \\[ \\text{count\\_smooth}_{it} = \\text{Median}(\\text{count\\_predict}_{t-k}, \\dots, \\text{count\\_predict}_t, \\dots, \\text{count\\_predict}_{t+k}) \\] <ul> <li>Parameter: <code>SMOOTH_K</code> (default: 7, must be odd)</li> <li>Larger <code>SMOOTH_K</code> smooths more; smaller retains more variation.</li> </ul> <p>Calculate residuals:</p> \\[ \\text{residual}_{it} = \\text{count\\_original}_{it} - \\text{count\\_smooth}_{it} \\] <p>Standardize residuals using MAD:</p> \\[ \\text{robust\\_control}_{it} = \\text{residual}_{it} / \\text{MAD}_i \\]"},{"location":"03_module_service_utilization_documentation/#disruption-analysis-regression-models","title":"Disruption Analysis Regression Models","text":"<p>Once anomalies are identified and saved in <code>M3_chartout.csv</code>, the disruption analysis quantifies their impact using regression models. These models estimate how much service utilization changed during the flagged disruption periods by adjusting for long-term trends and seasonal variations.</p> <p>For each indicator, we estimate:</p> \\[ Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\sum_{m=1}^{12} \\gamma_m \\cdot \\text{month}_m + \\beta_2 \\cdot \\text{tagged} + \\epsilon_{it} \\] <p>where: - \\(Y_{it}\\) is the observed service volume, - \\(\\text{date}\\) captures time trends, - \\(\\text{month}_m\\) controls for seasonality, - \\(\\text{tagged}\\) is the disruption dummy (from the control chart analysis), - \\(\\epsilon_{it}\\) is the error term.</p> <p>The coefficient on <code>tagged</code> (\\(\\beta_2\\)) measures the relative change in service utilization during flagged disruptions. Separate regressions are run at the national, province and district levels to assess the impact across different geographic scales.</p>"},{"location":"03_module_service_utilization_documentation/#country-wide-regression","title":"Country-wide Regression","text":"<p>The country-wide regression estimates how service utilization changes at the national level when a disruption occurs. Instead of analyzing individual provinces or districts separately, this model considers the entire country's data in a single regression. Errors are clustered at the lowest available geographic level (<code>lowest_geo_level</code>), typically districts.</p> <p>Model Specification:</p> \\[Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\sum_{m=1}^{12} \\gamma_m \\cdot \\text{month} + \\beta_2 \\cdot \\text{tagged} + \\epsilon_{it}\\] <p>Where: - \\(Y_{it}\\) = volume (e.g., number of deliveries) - \\(\\text{date}\\) = time trend - \\(\\text{month}_m\\) = controls for seasonality (factor variable) - \\(\\text{tagged}\\) = dummy for disruption period - \\(\\epsilon_{it}\\) = error term, clustered at the district level (<code>admin_area_3</code>)</p>"},{"location":"03_module_service_utilization_documentation/#province-level-regression","title":"Province-level Regression","text":"<p>The province-level disruption regression estimates how service utilization changes at the province level when a disruption occurs. Unlike the country-wide model, this approach runs separate regressions for each province to capture regional variations.</p> <p>Model specification:</p> \\[Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\sum_{m=1}^{12} \\gamma_m \\cdot \\text{month} + \\beta_2 \\cdot \\text{tagged} + \\alpha_{\\text{province}} + \\epsilon_{it}\\] <p>Where: - \\(Y_{it}\\) = volume (e.g., number of deliveries) - \\(\\text{date}\\) = time trend - \\(\\text{month}_m\\) = controls for seasonality (factor variable) - \\(\\text{tagged}\\) = dummy for disruption period - \\(\\alpha_{\\text{province}}\\) = province fixed effects - \\(\\epsilon_{it}\\) = error term, clustered at the district level</p>"},{"location":"03_module_service_utilization_documentation/#district-level-regression","title":"District-level Regression","text":"<p>The district-level disruption regression estimates how service utilization changes at the district level when a disruption occurs. This approach runs separate regressions for each district to capture localized variations.</p> <p>Model specification:</p> \\[Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\sum_{m=1}^{12} \\gamma_m \\cdot \\text{month} + \\beta_2 \\cdot \\text{tagged} + \\alpha_{\\text{district}} + \\epsilon_{it}\\] <p>Where: - \\(Y_{it}\\) = volume (e.g., number of deliveries) - \\(\\text{date}\\) = time trend - \\(\\text{month}_m\\) = controls for seasonality (factor variable) - \\(\\text{tagged}\\) = dummy for disruption period - \\(\\alpha_{\\text{district}}\\) = district fixed effects - \\(\\epsilon_{it}\\) = error term</p>"},{"location":"03_module_service_utilization_documentation/#regression-outputs","title":"Regression Outputs","text":"<p>Each regression level produces the following outputs:</p> <p>Expected values (<code>expect_admin_area_*</code>): Predicted service volume adjusted for seasonality and trends.</p> <p>Disruption effect (<code>b_admin_area_*</code>): Estimated relative change during disruptions:</p> \\[ b_{\\text{admin\\_area\\_*}} = -\\frac{\\text{diff mean}}{\\text{predict mean}} \\] <p>Trend coefficient (<code>b_trend_admin_area_*</code>): Reflects long-term trend.</p> <ul> <li>Positive = increasing service use</li> <li>Negative = declining service use</li> <li>Near zero = stable trend</li> </ul> <p>P-value (<code>p_admin_area_*</code>): Measures statistical significance of the disruption effect.</p> <ul> <li>Lower values = stronger evidence of true disruption</li> </ul>"},{"location":"03_module_service_utilization_documentation/#statistical-methods-used","title":"Statistical Methods Used","text":"<p>Robust Regression (<code>MASS::rlm</code>):</p> <ul> <li>Uses iteratively reweighted least squares (IRLS)</li> <li>Minimizes influence of outliers and extreme values</li> <li>More resistant to model misspecification than ordinary least squares</li> <li>Default: Huber weighting with maximum 100 iterations</li> </ul> <p>MAD (Median Absolute Deviation):</p> <ul> <li>Robust measure of scale/variability</li> <li>Formula: <code>MAD = median(|x - median(x)|)</code></li> <li>More resistant to outliers than standard deviation</li> <li>Used to standardize residuals for anomaly detection</li> </ul> <p>Panel Regression (<code>fixest::feols</code>):</p> <ul> <li>Fixed-effects estimation with clustered standard errors</li> <li>Accounts for within-group correlation in errors</li> <li>More efficient than traditional panel regression packages</li> <li>Handles unbalanced panels gracefully</li> </ul> <p>Geographic Clustering:</p> <ul> <li>Regressions use clustered standard errors at the lowest available geographic level</li> <li>This accounts for within-area correlation in service delivery patterns</li> <li>Example: Country-wide model clusters by district, province model clusters by district</li> <li>Prevents underestimation of standard errors and false positives</li> </ul>"},{"location":"03_module_service_utilization_documentation/#detailed-analysis-steps","title":"Detailed Analysis Steps","text":"PART 1 - Control Chart Analysis"},{"location":"03_module_service_utilization_documentation/#step-1-prepare-the-data","title":"Step 1: Prepare the Data","text":"<ul> <li>Load raw HMIS service utilization data.</li> <li>Merge in outlier flags (<code>outlier_flag</code>) by facility \u00d7 indicator \u00d7 month.</li> <li>Remove rows flagged as outliers (<code>outlier_flag == 1</code>).</li> <li>Create a <code>date</code> variable from <code>period_id</code> and extract <code>year</code> and <code>month</code>.</li> <li>Create a unique <code>panelvar</code> for each geographic area-indicator combination.</li> <li>Aggregate data to the specified geographic level by summing <code>count_model</code> (based on <code>SELECTEDCOUNT</code>) by date.</li> <li>Fill in missing months within each panel to ensure continuity.</li> <li>Fill missing metadata using forward and backward fill.</li> </ul>"},{"location":"03_module_service_utilization_documentation/#step-2-filter-out-low-volume-months","title":"Step 2: Filter Out Low-Volume Months","text":"<ul> <li>Compute the global mean service volume for each <code>panelvar</code>.</li> <li>If <code>count_original</code> is &lt;50% of the global mean, drop the value by setting it to <code>NA</code>.</li> </ul>"},{"location":"03_module_service_utilization_documentation/#step-3-apply-regression-and-smoothing","title":"Step 3: Apply Regression and Smoothing","text":"<p>Estimate expected service volume using robust regression, then smooth the predicted trend.</p> <ul> <li>Fit robust regression (<code>rlm</code>) for each panel using one of three model specifications based on data availability.</li> <li>Apply rolling median smoothing to predictions using window size <code>SMOOTH_K</code>.</li> <li>If smoothing is not possible (e.g., at series edges), fallback to model predictions.</li> <li>Calculate residuals: actual - smoothed</li> <li>Standardize residuals using MAD</li> </ul> <p>This standardized control variable is used to detect anomalies in Step 4.</p>"},{"location":"03_module_service_utilization_documentation/#step-4-tag-disruptions","title":"Step 4: Tag Disruptions","text":"<p>Apply rule-based tagging to identify potential disruptions. Each rule is governed by user-defined parameters that can be tuned for sensitivity:</p> <ul> <li>Sharp Disruptions: Tag if <code>|robust_control| \u2265 MADS_THRESHOLD</code></li> <li>Sustained Drops: Tag final month if 3 consecutive months have mild deviations and final month exceeds threshold</li> <li>Sustained Dips: Tag entire sequence if <code>count_original &lt; DIP_THRESHOLD \u00d7 count_smooth</code> for 3+ months</li> <li>Sustained Rises: Tag entire sequence if <code>count_original &gt; RISE_THRESHOLD \u00d7 count_smooth</code> for 3+ months</li> <li>Missing Data: Tag if 2+ of past 3 months are missing or zero</li> <li>Recent Tail Override: Automatically tag all months in last 6 months of data</li> </ul> <p>A month is assigned <code>tagged = 1</code> if any of the above conditions are met. Tagged records are saved in <code>M3_chartout.csv</code> and passed to the disruption analysis.</p>"},{"location":"03_module_service_utilization_documentation/#part-2-disruption-analysis","title":"PART 2 - Disruption Analysis","text":""},{"location":"03_module_service_utilization_documentation/#step-1-data-preparation","title":"Step 1: Data preparation","text":"<ul> <li>The <code>M3_chartout</code> dataset is merged with the main dataset to integrate the <code>tagged</code> variable, which identifies flagged disruptions.</li> <li>The lowest available geographic level (<code>lowest_geo_level</code>) is identified for clustering, based on the highest-resolution <code>admin_area_*</code> column available.</li> </ul>"},{"location":"03_module_service_utilization_documentation/#step-2-country-wide-regression","title":"Step 2: Country-wide regression","text":"<p>For each <code>indicator_common_id</code>, estimate the national-level model with errors clustered at district level.</p> <ul> <li>A panel regression model is applied at the country-wide level, estimating the expected service volume (<code>expect_admin_area_1</code>) for each indicator.</li> <li>The model adjusts for historical trends and seasonal variations.</li> <li>If a disruption (<code>tagged</code> = 1) is detected, the predicted service volume is adjusted by subtracting the estimated effect of the disruption to isolate its impact.</li> </ul>"},{"location":"03_module_service_utilization_documentation/#step-3-province-level-regression","title":"Step 3: Province-level regression","text":"<p>For each <code>indicator_common_id</code> \u00d7 <code>admin_area_2</code> combination, estimate province-specific models with errors clustered at district level.</p> <ul> <li>A fixed effects panel regression model is applied at the province level, estimating expected service volume (<code>expect_admin_area_2</code>) while controlling for province-specific factors.</li> <li>The model adjusts for historical trends and seasonal variations.</li> <li>If a disruption is detected, predicted volumes are adjusted to isolate the impact.</li> </ul>"},{"location":"03_module_service_utilization_documentation/#step-4-district-level-regression-if-enabled","title":"Step 4: District-level regression (if enabled)","text":"<p>For each <code>indicator_common_id</code> \u00d7 <code>admin_area_3</code> combination, estimate district-specific models with errors clustered at ward level.</p> <ul> <li>A fixed effects panel regression model is applied at the district level, estimating expected service volume (<code>expect_admin_area_3</code>).</li> <li>The model adjusts for historical trends and seasonal variations.</li> <li>If a disruption is detected, predicted volumes are adjusted to isolate the impact.</li> </ul>"},{"location":"03_module_service_utilization_documentation/#step-5-prepare-outputs-for-visualization","title":"Step 5: Prepare Outputs for Visualization","text":"<p>Once expected values have been calculated for each level (country, province, district), the pipeline compares predicted and actual values to assess the magnitude of disruption.</p> <p>For each month and indicator, the pipeline calculates:</p> <ul> <li>Absolute and percentage difference between predicted and actual values:</li> </ul> \\[ \\text{diff\\_percent} = 100 \\times \\frac{\\text{predicted} - \\text{actual}}{\\text{predicted}} \\] <ul> <li> <p>A configurable threshold parameter <code>DIFFPERCENT</code> (default: <code>10</code>) is used to determine when a disruption is significant.</p> <p>If the percentage difference exceeds \u00b110%, the expected (predicted) value is retained and used for plotting and summary statistics. Otherwise, the actual observed value is used.</p> <p>This ensures that minor fluctuations do not lead to artificial disruptions in the visualization, while meaningful deviations are preserved.</p> </li> <li> <p>The final adjusted value for plotting is stored in a field such as <code>count_expected_if_above_diff_threshold</code>.</p> <p>This value reflects either: - The predicted count (if deviation &gt; threshold), or - The actual count (if within acceptable range).</p> </li> </ul> <p>This logic is applied consistently across all admin levels. These adjusted values are then exported as part of the final output files for each level.</p>"},{"location":"03_module_service_utilization_documentation/#troubleshooting","title":"Troubleshooting","text":"Common Issues and Solutions"},{"location":"03_module_service_utilization_documentation/#issue-script-crashes-with-out-of-memory-error","title":"Issue: Script crashes with \"out of memory\" error","text":"<p>Solutions:</p> <ul> <li>Reduce batch sizes (e.g., <code>BATCH_SIZE_IND &lt;- 3</code>)</li> <li>Set <code>RUN_DISTRICT_MODEL &lt;- FALSE</code></li> <li>Set <code>RUN_ADMIN_AREA_4_ANALYSIS &lt;- FALSE</code></li> <li>Close other applications</li> <li>Run on machine with more RAM</li> </ul>"},{"location":"03_module_service_utilization_documentation/#issue-warning-model-failed-to-converge","title":"Issue: Warning \"model failed to converge\"","text":"<p>Explanation: Robust regression didn't fully converge within 100 iterations</p> <p>Impact: Usually minimal - partial convergence often sufficient</p> <p>Solutions:</p> <ul> <li>Check data quality for that panel</li> <li>Increase <code>maxit</code> parameter in <code>rlm()</code> call (line 229, 247)</li> <li>Generally safe to ignore if only a few panels affected</li> </ul>"},{"location":"03_module_service_utilization_documentation/#issue-many-empty-rows-in-output-files","title":"Issue: Many empty rows in output files","text":"<p>Explanation: Insufficient data for certain indicator-geography combinations</p> <p>Solutions:</p> <ul> <li>Expected behavior for sparse indicators</li> <li>Filter outputs to non-missing values</li> <li>Consider aggregating to higher geographic level</li> </ul>"},{"location":"03_module_service_utilization_documentation/#issue-all-recent-months-flagged-as-disruptions","title":"Issue: All recent months flagged as disruptions","text":"<p>Explanation: Automatic flagging of last 6 months</p> <p>Purpose: Ensures recent trends reviewed even without strong statistical evidence</p> <p>Solutions:</p> <ul> <li>Expected behavior, not a bug</li> <li>Review recent months manually</li> <li>Adjust <code>last_6_months</code> logic if needed (line 333)</li> </ul>"},{"location":"03_module_service_utilization_documentation/#issue-tagged-variable-dropped-from-regression","title":"Issue: <code>tagged</code> variable dropped from regression","text":"<p>Message: Variable automatically set to 0</p> <p>Explanation: No variation in <code>tagged</code> within that panel (all 0 or all 1)</p> <p>Solutions:</p> <ul> <li>Expected in panels with no disruptions or constant disruption</li> <li>Not an error - disruption effect correctly set to 0</li> </ul>"},{"location":"03_module_service_utilization_documentation/#issue-temporary-files-remain-after-run","title":"Issue: Temporary files remain after run","text":"<p>Cause: Script crashed before cleanup</p> <p>Solutions:</p> <ul> <li>Delete manually: <code>M3_temp_*.csv</code></li> <li>Or re-run script (automatic cleanup at start)</li> </ul>"},{"location":"03_module_service_utilization_documentation/#issue-very-different-results-at-different-geographic-levels","title":"Issue: Very different results at different geographic levels","text":"<p>Explanation: Different geographic aggregation captures different patterns</p> <p>Example: National trend may be stable while some districts have large disruptions</p> <p>Solutions:</p> <ul> <li>Expected behavior - not a bug</li> <li>Use appropriate level for your research question</li> <li>Cross-check patterns across levels for robustness</li> </ul>"},{"location":"03_module_service_utilization_documentation/#usage-notes","title":"Usage Notes","text":"Interpretation Guidelines <p>Disruption Effects (b_admin_area_*):</p> <ul> <li>Negative values indicate service volume shortfalls during disrupted periods</li> <li>Positive values indicate service volume surpluses during disrupted periods</li> <li>Values closer to zero indicate smaller disruption impacts</li> </ul> <p>P-values (p_admin_area_*):</p> <ul> <li>Values &lt; 0.05 suggest statistically significant disruptions</li> <li>Values &gt; 0.05 may indicate normal variation rather than true disruptions</li> </ul> <p>Trend Coefficients (b_trend_admin_area_*):</p> <ul> <li>Positive values indicate increasing service utilization over time</li> <li>Negative values indicate declining service utilization over time</li> <li>Values near zero indicate stable utilization patterns</li> </ul> <p>Last updated: 10-11-2025 Contact: FASTR Project Team</p>"},{"location":"03_module_service_utilization_documentation/#performance-considerations","title":"Performance Considerations","text":"<p>Runtime Factors:</p> <ul> <li>Number of indicators: Linear scaling</li> <li>Number of geographic units: Linear scaling within each level</li> <li>Time series length: Minimal impact (efficient regression)</li> <li>Geographic detail: Exponential scaling (many more units at finer levels)</li> </ul> <p>Estimated Runtimes (example dataset: 50 indicators, 100 districts):</p> <ul> <li>Country-wide + Province models: ~5-10 minutes</li> <li>Add District models: ~30-60 minutes</li> <li>Add Ward models: Several hours (depends on number of wards)</li> </ul> <p>Optimization Strategies:</p> <ul> <li>Set <code>RUN_DISTRICT_MODEL = FALSE</code> for faster execution (skips district level)</li> <li>Set <code>RUN_ADMIN_AREA_4_ANALYSIS = FALSE</code> (default) to avoid ward-level analysis</li> <li>Reduce <code>SMOOTH_K</code> for faster rolling median calculation</li> <li>Use <code>SELECTEDCOUNT = \"count_final_none\"</code> to avoid completeness adjustments</li> </ul>"},{"location":"03_module_service_utilization_documentation/#data-processing-details","title":"Data Processing Details","text":"<p>Memory Management:</p> <ul> <li>Uses <code>data.table</code> for efficient operations on large datasets</li> <li>Batch processing: Results saved to disk periodically</li> <li>Progressive cleanup: Objects deleted when no longer needed</li> <li>Temporary files enable processing datasets larger than RAM</li> </ul> <p>Batch Sizes (tunable for memory constraints):</p> <ul> <li>Control chart: 100 panels per batch</li> <li>Indicators: 5 indicators per batch</li> <li>Provinces: 20 results per batch</li> <li>Districts: 15 results per batch</li> <li>Admin area 4: 10 results per batch</li> </ul> <p>Missing Data Handling:</p> <ol> <li>Missing months filled via <code>tidyr::complete()</code></li> <li>Forward/backward fill for metadata</li> <li>Linear interpolation (<code>zoo::na.approx</code>) for count values</li> <li>Maximum gap: Unlimited (rule = 2 extends endpoints)</li> </ol>"},{"location":"03_module_service_utilization_documentation/#model-fallback-logic","title":"Model Fallback Logic","text":"<p>The control chart analysis uses adaptive model selection based on data availability:</p> <p>Full Model (requires \u226512 obs AND &gt;12 unique dates):</p> <pre><code>count ~ month_factor + as.numeric(date)\n</code></pre> <p>Accounts for both seasonality and linear trend</p> <p>Trend-Only Model (requires \u226512 obs):</p> <pre><code>count ~ as.numeric(date)\n</code></pre> <p>Accounts for linear trend only (insufficient data for seasonality)</p> <p>Median Fallback (&lt;12 observations):</p> <pre><code>count_predict = median(count)\n</code></pre> <p>Uses global median when insufficient data for regression</p> <p>Convergence Checks:</p> <ul> <li>Models checked for convergence status</li> <li>Warnings issued for non-convergent models</li> <li>Non-convergent models still used (partial convergence often sufficient)</li> </ul>"},{"location":"03_module_service_utilization_documentation/#quality-assurance","title":"Quality Assurance","text":"<p>Data Cleaning:</p> <ul> <li>Outliers removed prior to control chart analysis (based on Module 1 flags)</li> <li>Low-volume months (&lt;50% of mean) excluded to improve model stability</li> <li>Predictions bounded at zero (counts cannot be negative)</li> </ul> <p>Automatic Flagging:</p> <ul> <li>Recent months (last 6 months) automatically flagged to ensure current disruptions captured</li> <li>Prevents missing ongoing disruptions due to insufficient deviation from trend</li> </ul> <p>Robustness Checks:</p> <ul> <li>Model coefficients checked for <code>NA</code> values before use</li> <li>If <code>tagged</code> variable dropped from model (no variation), disruption effect set to 0</li> <li>P-values calculated only when valid standard errors available</li> </ul> <p>Edge Case Handling:</p> <ul> <li>Single-cluster panels: No clustering applied (would fail)</li> <li>Insufficient data: Skip analysis for that panel/level</li> <li>Missing predictions: Filled with original values where possible</li> </ul>"},{"location":"03_module_service_utilization_documentation/#workflow-integration","title":"Workflow Integration","text":"<p>This module is Module 3 in the FASTR analytical pipeline:</p> <p>Prerequisites:</p> <ol> <li>Module 1: Data Quality Assessment (generates <code>M1_output_outliers.csv</code>)</li> <li>Module 2: Data Quality Adjustments (generates <code>M2_adjusted_data.csv</code>)</li> </ol>"},{"location":"03_module_service_utilization_documentation/#dependencies","title":"Dependencies","text":"<p>R Packages Required:</p> <ul> <li><code>data.table</code>: Efficient data manipulation</li> <li><code>lubridate</code>: Date handling</li> <li><code>zoo</code>: Rolling statistics and interpolation</li> <li><code>MASS</code>: Robust regression (rlm)</li> <li><code>fixest</code>: Fixed-effects panel regression</li> <li><code>dplyr</code>: Data manipulation</li> <li><code>tidyr</code>: Data tidying</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/","title":"Module 4: Coverage Estimates","text":""},{"location":"04_module_coverage_estimates_documentation/#1-overview-what-why","title":"1. Overview (What &amp; Why)","text":""},{"location":"04_module_coverage_estimates_documentation/#what-does-this-module-do","title":"What does this module do?","text":"<p>This module estimates health service coverage by integrating three key data sources: adjusted health service volumes from HMIS (Module 2), population projections from the United Nations, and household survey data from MICS/DHS. It helps answer the question: \"What percentage of the target population received this health service?\"</p> <p>The module operates in two distinct parts.</p> <p>Part 1 calculates target population sizes (denominators) using multiple methods and automatically selects the best option for each health indicator by comparing results against survey benchmarks.</p> <p>Part 2 allows users to refine these selections, choose specific denominators based on programmatic knowledge, and project survey estimates forward in time using administrative data trends to fill gaps where surveys are unavailable.</p> <p>Together, these parts transform raw service counts into meaningful coverage estimates that can be analyzed for trends, compared across regions, and used for policy decisions.</p>"},{"location":"04_module_coverage_estimates_documentation/#why-is-it-needed-in-the-fastr-pipeline","title":"Why is it needed in the FASTR pipeline?","text":"<p>Understanding coverage is essential for monitoring health system performance and equity. While Module 2 provides adjusted service volumes, these numbers alone don't tell us whether services are reaching their intended populations. Coverage estimates provide context by comparing service delivery to population need.</p> <p>This module addresses some challenges in coverage estimation:</p> <ul> <li> <p>Multiple data sources: Integrates HMIS data with survey data</p> </li> <li> <p>Denominator uncertainty: Different methods for estimating target populations may yield different results; the module systematically evaluates options</p> </li> <li> <p>Temporal gaps: Surveys occur every 3-5 years; the module projects estimates for intervening years using administrative trends</p> </li> <li> <p>Subnational analysis: Enables coverage monitoring at national, provincial, and district levels</p> </li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#quick-summary","title":"Quick Summary","text":"Component Details Inputs M2_adjusted_data (national &amp; subnational) from Module 2Survey data (MICS/DHS) from GitHub repositoryPopulation data (UN WPP) from GitHub repository Outputs M4_denominators (national, admin2, admin3) - calculated target populationsM4_combined_results (national, admin2, admin3) - coverage estimates with all denominatorsM5_coverage_estimation (national, admin2, admin3) - final coverage with projections Purpose Estimate health service coverage by comparing service volumes to target populations, validated against survey benchmarks"},{"location":"04_module_coverage_estimates_documentation/#part-1-and-part-2-explained","title":"Part 1 and Part 2 Explained","text":"<p>Part 1: Denominator Calculation and Selection</p> <ul> <li> <p>Calculates target populations (denominators) using multiple approaches: HMIS-based (from ANC1, delivery, BCG, Penta1) and population-based (UN WPP)</p> </li> <li> <p>Compares coverage estimates from each denominator against survey data</p> </li> <li> <p>Automatically selects the \"best\" denominator for each indicator by minimizing error</p> </li> <li> <p>Outputs: Denominator datasets and combined results showing all options</p> </li> </ul> <p>Part 2: Denominator Selection and Survey Projection</p> <ul> <li> <p>Allows users to override automatic selections and choose specific denominators</p> </li> <li> <p>Calculates year-over-year coverage trends from administrative data</p> </li> <li> <p>Projects survey estimates forward using HMIS trends to fill temporal gaps</p> </li> <li> <p>Outputs: Final coverage estimates combining HMIS, survey, and projected values</p> </li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#2-how-it-works","title":"2. How It Works","text":""},{"location":"04_module_coverage_estimates_documentation/#high-level-workflow","title":"High-Level Workflow","text":""},{"location":"04_module_coverage_estimates_documentation/#part-1-denominator-calculation-and-selection","title":"Part 1: Denominator Calculation and Selection","text":"<p>Step 1: Load and Prepare Data Sources The module begins by loading three data sources and ensuring they're compatible. HMIS data is aggregated from monthly to annual totals. Survey data is harmonized (DHS prioritized over MICS) and forward-filled to create continuous time series. Population data is filtered to the target country.</p> <p>Step 2: Calculate Multiple Denominator Options For each health indicator, the module calculates several possible target populations:</p> <ul> <li> <p>Service-based denominators: Using HMIS volumes divided by survey coverage (e.g., if 10,000 women received ANC1 and survey says coverage is 80%, estimated pregnancies = 10,000/0.80 = 12,500)</p> </li> <li> <p>Population-based denominators: Using UN population projections and birth rates</p> </li> <li> <p>Each denominator is adjusted for demographic factors (pregnancy loss, stillbirths, mortality rates) to match the indicator's target age group</p> </li> </ul> <p>Step 3: Calculate Coverage for Each Denominator The module computes coverage by dividing the service volume by each denominator option. This produces multiple coverage estimates per indicator, each based on a different population assumption.</p> <p>Step 4: Compare to Survey Benchmarks Each coverage estimate is compared to survey data using squared error calculation. The survey serves as the \"truth\" benchmark since it's based on representative household sampling.</p> <p>Step 5: Select the Best Denominator The denominator producing the lowest error (closest match to survey) is automatically selected as \"best.\" The selection prioritizes HMIS-based denominators over population projections to ensure data is driven by observed service delivery.</p> <p>Step 6: Generate Outputs The module saves denominator datasets for transparency and combined results files showing coverage from all denominators plus the selected best option.</p> <p>Step 7: Repeat for Subnational Levels If subnational data is available, the process repeats for administrative level 2 (e.g., provinces) and level 3 (e.g., districts), with fallback mechanisms to handle missing local survey data.</p>"},{"location":"04_module_coverage_estimates_documentation/#part-2-denominator-selection-and-survey-projection","title":"Part 2: Denominator Selection and Survey Projection","text":"<p>Step 1: User Configuration Users review Part 1 results and configure denominator selections for each indicator. Options include using the automatic \"best\" selection or overriding with a specific denominator based on programmatic knowledge.</p> <p>Step 2: Filter to Selected Denominators The module filters Part 1's combined results to include only user-selected denominators, creating a focused dataset for analysis.</p> <p>Step 3: Calculate Coverage Trends Year-over-year changes (deltas) in HMIS-based coverage are calculated. This shows whether coverage is increasing, decreasing, or stable over time.</p> <p>Step 4: Identify Survey Baseline For each geographic area and indicator, the most recent survey observation is identified as the baseline anchor point for projections.</p> <p>Step 5: Project Survey Estimates Forward The module extends survey coverage estimates into years without surveys by applying HMIS trends. The projection uses: Last survey value + (Current year HMIS coverage - Survey year HMIS coverage). This preserves the survey calibration while incorporating observed trends.</p> <p>Step 6: Combine All Estimates The final output merges three types of estimates:</p> <ul> <li> <p>HMIS-based coverage: Direct calculation from service volumes and selected denominators</p> </li> <li> <p>Original survey values: Actual household survey observations</p> </li> <li> <p>Projected survey coverage: Survey estimates extended using HMIS trends</p> </li> </ul> <p>Step 7: Save Final Outputs Results are saved with standardized column structures for each administrative level, ready for visualization and reporting.</p>"},{"location":"04_module_coverage_estimates_documentation/#workflow-diagram","title":"Workflow Diagram","text":""},{"location":"04_module_coverage_estimates_documentation/#key-decision-points","title":"Key Decision Points","text":"<p>1. Which denominator to use? Part 1 automatically selects based on alignment with survey data, but users can override in Part 2. The choice affects whether coverage is anchored to service delivery patterns (HMIS-based) or demographic projections (population-based).</p> <p>2. How to handle survey gaps? Surveys occur infrequently (every 3-5 years). The module forward-fills survey values in Part 1 (assumes constant coverage until next survey) and uses projection in Part 2 (incorporates HMIS trends).</p> <p>3. Should subnational analysis use local or national survey data? When local survey data is unavailable, the module falls back to national values. This assumes national coverage rates apply locally, which may not hold in all contexts.</p> <p>4. How to adjust denominators for different target populations? Each health indicator targets a specific population (e.g., pregnant women for ANC, infants for vaccines). The module applies sequential demographic adjustments (pregnancy loss, stillbirths, mortality) to align denominators with target populations.</p>"},{"location":"04_module_coverage_estimates_documentation/#what-happens-to-the-data","title":"What Happens to the Data","text":"<p>Input Integration: The module combines three distinct data sources: facility-level service volumes from HMIS (aggregated annually by geographic area), household survey coverage estimates (harmonized across different survey years and forward-filled to create continuous time series), and population projections (filtered to extract age-specific target populations for each health indicator).</p> <p>Denominator Construction: Using the relationship between HMIS service volumes and survey-based coverage estimates, the module calculates HMIS-implied denominators that represent the population that would need to exist for the observed service volumes to match survey coverage rates. These denominators are adjusted for indicator-specific target populations through sequential demographic corrections accounting for pregnancy loss, stillbirths, and mortality.</p> <p>Coverage Calculation: The module calculates multiple coverage estimates by dividing service volumes by different denominator options (population-based, HMIS-implied, hybrid approaches). Each coverage estimate is then compared against survey benchmarks to identify which denominator produces the most plausible results for each indicator, balancing between HMIS data quality and population estimate accuracy.</p> <p>Temporal Projection: For years beyond the most recent survey, the module projects coverage estimates forward by combining the last observed survey value with HMIS-based trends. This produces complete coverage time series that leverage both the validity of survey data and the timeliness of routine HMIS reporting, with all estimates accompanied by metadata indicating data source and projection methodology.</p>"},{"location":"04_module_coverage_estimates_documentation/#analysis-outputs-and-visualization","title":"Analysis Outputs and Visualization","text":"<p>The FASTR analysis generates coverage estimate visualizations at multiple geographic levels:</p> <p>1. Coverage Calculated from HMIS Data (National)</p> <p>Time series showing coverage estimates calculated from HMIS service volumes at the national level, comparing HMIS-based estimates with survey benchmarks.</p> <p></p> <p>2. Coverage Calculated from HMIS Data (Admin Area 2)</p> <p>Coverage estimates disaggregated to admin area 2 level, enabling identification of subnational variation in service coverage.</p> <p></p> <p>3. Coverage Calculated from HMIS Data (Admin Area 3)</p> <p>Highly disaggregated coverage estimates at admin area 3 level for detailed geographic analysis where data quality permits.</p> <p></p> <p>Interpretation Guide: - Survey points: Shown as markers representing validated household survey coverage estimates - HMIS-based estimates: Line series showing coverage calculated from routine facility data - Projected coverage: Forward projections combining survey benchmarks with HMIS trends - Geographic disaggregation: Lower administrative levels enable targeting of interventions to areas with coverage gaps</p>"},{"location":"04_module_coverage_estimates_documentation/#3-detailed-reference","title":"3. Detailed Reference","text":""},{"location":"04_module_coverage_estimates_documentation/#part-1-denominator-calculation-technical-details","title":"Part 1: Denominator Calculation (Technical Details)","text":""},{"location":"04_module_coverage_estimates_documentation/#configuration-parameters","title":"Configuration Parameters","text":"<p>The module begins with several configurable parameters that control the analysis:</p> <pre><code>COUNTRY_ISO3 &lt;- \"ISO3\"                         # ISO3 country code (e.g., \"RWA\", \"UGA\", \"ZMB\")\nSELECTED_COUNT_VARIABLE &lt;- \"count_final_both\"  # Which adjusted count to use\nANALYSIS_LEVEL &lt;- \"NATIONAL_PLUS_AA2\"          # Geographic scope\n</code></pre> <p>Analysis Level Options:</p> <ul> <li><code>NATIONAL_ONLY</code>: National-level analysis only</li> <li><code>NATIONAL_PLUS_AA2</code>: National + administrative area 2 (e.g., provinces)</li> <li><code>NATIONAL_PLUS_AA2_AA3</code>: National + admin area 2 + admin area 3 (e.g., districts)</li> </ul> <p>Demographic Adjustment Rates: <pre><code>PREGNANCY_LOSS_RATE &lt;- 0.03      # 3% pregnancy loss\nTWIN_RATE &lt;- 0.015               # 1.5% twin births\nSTILLBIRTH_RATE &lt;- 0.02          # 2% stillbirths\nP1_NMR &lt;- 0.039                  # Neonatal mortality rate\nP2_PNMR &lt;- 0.028                 # Post-neonatal mortality rate\nINFANT_MORTALITY_RATE &lt;- 0.063   # Infant mortality rate\nUNDER5_MORTALITY_RATE &lt;- 0.103   # Under-5 mortality rate\n</code></pre></p> <p>Count Variable Options:</p> <ul> <li><code>count_final_none</code>: No adjustments (raw reported data)</li> <li><code>count_final_outlier</code>: Outlier adjustment only</li> <li><code>count_final_completeness</code>: Completeness adjustment only</li> <li><code>count_final_both</code>: Both adjustments (recommended)</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#input-data-sources","title":"Input Data Sources","text":"<p>Part 1 integrates three primary data sources:</p> <p>1. HMIS Adjusted Data (from Module 2)</p> <ul> <li>National: <code>M2_adjusted_data_national.csv</code></li> <li>Subnational: <code>M2_adjusted_data_admin_area.csv</code></li> <li>Contains service volumes by indicator, area, and time period</li> </ul> <p>2. Survey Data (DHS/MICS)</p> <ul> <li>Source: GitHub repository (unified survey dataset)</li> <li>Provides coverage benchmarks for comparison</li> <li>DHS data prioritized over MICS when both available</li> </ul> <p>3. Population Data (UN WPP)</p> <ul> <li>Source: GitHub repository</li> <li>Provides population-based denominators</li> <li>Includes total population, births, under-1, and under-5 populations</li> </ul> <p>Additional Data Context:</p> <p>Population Projections (UN WPP) Sourced from the United Nations World Population Prospects, these estimates provide age-specific and total population figures used to calculate denominators for coverage estimates. These projections account for demographic trends, including fertility, mortality, and migration.</p> <p>Survey Data - MICS The Multiple Indicator Cluster Surveys (MICS), conducted by UNICEF, provide household survey-based estimates for key health indicators, including coverage of maternal and child health services.</p> <p>Survey Data - DHS The Demographic and Health Surveys (DHS), conducted by USAID, provide survey data on health service utilization, including immunization rates and maternal care coverage.</p>"},{"location":"04_module_coverage_estimates_documentation/#core-functions-documentation","title":"Core Functions Documentation","text":"<code>process_hmis_adjusted_volume()</code> <p>Purpose: Prepares HMIS data for denominator calculation</p> <p>Input:</p> <ul> <li>Adjusted volume data from Module 2</li> <li>Selected count variable (e.g., <code>count_final_both</code>)</li> </ul> <p>Processing:</p> <ul> <li>Aggregates monthly data to annual totals</li> <li>Counts number of reporting months per year</li> <li>Pivots data to wide format (one column per indicator)</li> </ul> <p>Output:</p> <ul> <li><code>annual_hmis</code>: Annual service counts by area and year</li> <li><code>hmis_countries</code>: List of countries in dataset</li> <li><code>hmis_iso3</code>: ISO3 code(s) present</li> </ul> <p>Example Structure:</p> <pre><code>admin_area_1  admin_area_2  year  countanc1  countdelivery  ...  nummonth\nCountry_Name  Province_A    2020  12500      10200          ...  12\nCountry_Name  Province_A    2021  13000      10500          ...  11\n</code></pre> <code>process_survey_data()</code> <p>Purpose: Harmonizes and extends survey data for use as coverage benchmarks</p> <p>Input:</p> <ul> <li>Survey data (DHS/MICS)</li> <li>HMIS country names and ISO3 codes</li> <li>Optional national reference (for subnational fallback)</li> </ul> <p>Key Processing Steps:</p> <ol> <li>Harmonization</li> <li>Recodes indicator names (e.g., <code>polio1</code> \u2192 <code>opv1</code>, <code>vitamina</code> \u2192 <code>vitaminA</code>)</li> <li>Normalizes source labels (<code>dhs</code>, <code>mics</code>)</li> <li> <p>Filters by country and date range</p> </li> <li> <p>Source Prioritization</p> </li> <li>When both DHS and MICS exist for same year/area/indicator</li> <li>DHS is selected preferentially</li> <li> <p>Preserves source details for transparency</p> </li> <li> <p>Fallback Logic</p> </li> <li>If <code>sba</code> missing, uses <code>delivery</code> values</li> <li>If <code>pnc1_mother</code> missing, uses <code>pnc1</code> values</li> <li> <p>Subnational areas use national values when local data unavailable (for BCG, Penta1, Penta3)</p> </li> <li> <p>Forward-Filling</p> </li> <li>Creates complete time series for each area</li> <li>Carries forward last observed value (<code>na.locf</code>)</li> <li>Creates \"carry\" columns (e.g., <code>anc1carry</code>, <code>bcgcarry</code>)</li> </ol> <p>Output:</p> <ul> <li><code>carried</code>: Extended survey data with forward-filled values</li> <li><code>raw</code>: Raw survey observations (wide format)</li> <li><code>raw_long</code>: Raw survey observations (long format) with source details</li> </ul> <code>process_national_population_data()</code> <p>Purpose: Prepares UN WPP population estimates for denominator calculation</p> <p>Input:</p> <ul> <li>Population estimates (UN WPP)</li> <li>HMIS country identifiers</li> </ul> <p>Processing:</p> <ul> <li>Filters to national level and target country</li> <li>Extracts key population indicators:</li> <li><code>crudebr_unwpp</code>: Crude birth rate</li> <li><code>poptot_unwpp</code>: Total population</li> <li><code>totu1pop_unwpp</code>: Under-1 population</li> </ul> <p>Output:</p> <ul> <li><code>wide</code>: Population indicators in wide format</li> <li><code>raw_long</code>: Population data in long format with source tracking</li> </ul> <code>calculate_denominators()</code> <p>Purpose: Calculates all possible denominators from HMIS and population data. This is the core function that generates multiple denominator estimates.</p> <p>Input:</p> <ul> <li><code>hmis_data</code>: Annual service counts</li> <li><code>survey_data</code>: Survey reference values (carried forward)</li> <li><code>population_data</code>: UN WPP estimates (national only)</li> </ul> <p>Denominator Types Calculated:</p> <p>A. Service-Based Denominators (using HMIS numerator \u00f7 survey coverage):</p> <ol> <li>From ANC1:</li> <li><code>danc1_pregnancy</code>: Estimated pregnancies</li> <li><code>danc1_delivery</code>: Estimated deliveries</li> <li><code>danc1_birth</code>: Estimated births (live + stillbirths)</li> <li><code>danc1_livebirth</code>: Estimated live births</li> <li><code>danc1_dpt</code>: Eligible for DPT (adjusted for neonatal mortality)</li> <li><code>danc1_measles1</code>: Eligible for MCV1</li> <li> <p><code>danc1_measles2</code>: Eligible for MCV2</p> </li> <li> <p>From Delivery:</p> </li> <li><code>ddelivery_livebirth</code>, <code>ddelivery_birth</code>, <code>ddelivery_pregnancy</code></li> <li> <p><code>ddelivery_dpt</code>, <code>ddelivery_measles1</code>, <code>ddelivery_measles2</code></p> </li> <li> <p>From SBA (Skilled Birth Attendance):</p> </li> <li>Same structure as delivery denominators</li> <li><code>dsba_livebirth</code>, <code>dsba_birth</code>, <code>dsba_pregnancy</code></li> <li> <p><code>dsba_dpt</code>, <code>dsba_measles1</code>, <code>dsba_measles2</code></p> </li> <li> <p>From BCG (national only):</p> </li> <li> <p><code>dbcg_pregnancy</code>, <code>dbcg_livebirth</code>, <code>dbcg_dpt</code></p> </li> <li> <p>From Penta1:</p> </li> <li><code>dpenta1_dpt</code>, <code>dpenta1_measles1</code>, <code>dpenta1_measles2</code></li> </ol> <p>B. Population-Based Denominators (national only):</p> <ul> <li><code>dwpp_pregnancy</code>: From crude birth rate \u00d7 total population \u00f7 (1 + twin rate)</li> <li><code>dwpp_livebirth</code>: From crude birth rate \u00d7 total population</li> <li><code>dwpp_dpt</code>: Under-1 population</li> <li><code>dwpp_measles1</code>: Under-1 population adjusted for neonatal mortality</li> <li><code>dwpp_measles2</code>: Further adjusted for post-neonatal mortality</li> </ul> <p>C. Vitamin A and Full Immunization:</p> <p>For each livebirth denominator, additional denominators are automatically created:</p> <ul> <li><code>d*_vitaminA</code>: Livebirth \u00d7 (1 - U5MR) \u00d7 4.5 (children 6-59 months)</li> <li><code>d*_fully_immunized</code>: Livebirth \u00d7 (1 - IMR)</li> </ul> <p>Adjustment for Incomplete Reporting:</p> <p>When <code>nummonth &lt; 12</code>, population-based denominators are scaled:</p> <pre><code>denominator_adjusted = denominator \u00d7 (nummonth / 12)\n</code></pre> <p>Output:</p> <p>Data frame with all calculated denominators plus original HMIS and survey data</p> <code>classify_source_type()</code> <p>Purpose: Categorizes denominators to prevent circular references</p> <p>Logic:</p> <ul> <li><code>reference_based</code>: Denominator calculated from same indicator (e.g., <code>danc1_pregnancy</code> for ANC1)</li> <li><code>unwpp_based</code>: Denominator from UN WPP population data</li> <li><code>independent</code>: Denominator from a different service indicator</li> </ul> <p>Importance:</p> <p>This classification ensures that when selecting \"best\" denominators, we avoid using reference-based denominators (which would artificially show 100% coverage equal to the survey value).</p> <code>compare_coverage_to_survey()</code> <p>Purpose: Selects the best-performing denominator for each indicator</p> <p>Input:</p> <ul> <li>Coverage estimates from all denominators</li> <li>Survey reference values (forward-filled)</li> </ul> <p>Selection Algorithm:</p> <ol> <li>Calculate Coverage: For each denominator option</li> </ol> <pre><code>coverage = (service_volume / denominator) \u00d7 100\n</code></pre> <ol> <li>Calculate Error: Compare to survey benchmark</li> </ol> <pre><code>squared_error = (HMIS_coverage - survey_coverage)\u00b2\n</code></pre> <ol> <li> <p>Classify Source Type: Label each denominator as independent, reference-based, or UNWPP</p> </li> <li> <p>Selection Hierarchy:</p> </li> </ol> <pre><code>Priority 1: Independent denominators (non-reference, non-UNWPP) \u2192 lowest error\nPriority 2: Reference-based denominators (only if no independent available)\nPriority 3: UNWPP denominators (last resort fallback)\n</code></pre> <ol> <li>Geographic Consistency: Best denominator selected per geographic area \u00d7 indicator (not per year)</li> </ol> <p>Output:</p> <p>Coverage data filtered to only the best-performing denominator for each indicator, with ranking</p> <p>Key Design Decision:</p> <ul> <li>UNWPP denominators excluded from \"best\" selection by default</li> <li>Prevents over-reliance on population projections</li> <li>Ensures HMIS data drives coverage when available</li> <li>UNWPP used only when no HMIS-based options exist</li> </ul> <code>create_combined_results_table()</code> <p>Purpose: Merges coverage estimates and survey observations into unified output</p> <p>Input:</p> <ul> <li>Coverage comparison results (best denominator selected)</li> <li>Raw survey observations</li> <li>All coverage data (optional, includes all denominators)</li> </ul> <p>Output Structure:</p> <pre><code>admin_area_1  year  indicator_common_id  denominator_best_or_survey  value\nCountry_Name  2020  anc1                 best                        85.3\nCountry_Name  2020  anc1                 survey                      84.2\nCountry_Name  2020  anc1                 danc1_pregnancy             85.3\nCountry_Name  2020  anc1                 dwpp_pregnancy              82.1\n</code></pre> <p>Denominator Categories:</p> <ul> <li><code>best</code>: Selected optimal denominator</li> <li><code>survey</code>: Actual survey observation</li> <li><code>d*_*</code>: Individual denominator results (all options)</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#statistical-methods-algorithms","title":"Statistical Methods &amp; Algorithms","text":""},{"location":"04_module_coverage_estimates_documentation/#forward-filling-last-observation-carried-forward","title":"Forward-Filling (Last Observation Carried Forward)","text":"<p>Survey data typically has gaps (e.g., DHS every 5 years). To create continuous denominators:</p> <pre><code>na.locf(survey_value, na.rm = FALSE)\n</code></pre> <p>Example:</p> <pre><code>Year:   2015  2016  2017  2018  2019  2020\nRaw:    85.3  NA    NA    NA    87.2  NA\nFilled: 85.3  85.3  85.3  85.3  87.2  87.2\n</code></pre> <p>This assumes coverage remains constant until next observation.</p>"},{"location":"04_module_coverage_estimates_documentation/#squared-error-minimization","title":"Squared Error Minimization","text":"<p>To select the best denominator:</p> \\[ \\text{Best denominator} = \\arg \\min_d \\sum_{t} (C_{d,t} - S_t)^2 \\] <p>Where:</p> <ul> <li> <p>\\(C_{d,t}\\) = Coverage using denominator \\(d\\) in year \\(t\\)</p> </li> <li> <p>\\(S_t\\) = Survey coverage in year \\(t\\)</p> </li> <li> <p>Summation is across all years with survey data</p> </li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#conceptual-framework-demographic-cascades","title":"Conceptual Framework: Demographic Cascades","text":"<p>Before diving into the specific formulas, it's important to understand the conceptual flow of denominator calculations. Denominators are derived through sequential demographic adjustments that reflect the biological cascade from pregnancy to specific health service target populations.</p> <p>Illustrative Example: From Pregnancy to DPT-eligible Population</p> <p>Consider how an estimated 10,000 pregnancies translate to the population eligible for DPT vaccination:</p> <pre><code>Starting point (pregnancies):           10,000\n\u2192 After pregnancy loss (3%):            10,000 \u00d7 (1 - 0.03) = 9,700 deliveries\n\u2192 After twin adjustment (1.5% rate):    9,700 \u00d7 (1 - 0.015/2) = 9,627 births\n\u2192 After stillbirths (2%):               9,627 \u00d7 (1 - 0.02) = 9,435 live births\n\u2192 After neonatal deaths (3.9%):         9,435 \u00d7 (1 - 0.039) = 9,067 DPT-eligible children\n</code></pre> <p>This cascade demonstrates how each demographic factor sequentially reduces the population size as we move through life stages. The detailed mathematical formulas in the following sections follow this same logic, but work in both directions:</p> <ul> <li>Forward cascade: Starting from earlier indicators (ANC1, Delivery) and adjusting toward later target populations</li> <li>Backward cascade: Starting from later indicators (BCG, Penta1) and working backwards to estimate earlier populations</li> </ul> <p>The specific rates and formulas for each denominator source are provided in detail below.</p>"},{"location":"04_module_coverage_estimates_documentation/#hmis-based-denominator-calculations","title":"HMIS-based Denominator Calculations","text":"<p>Denominators Derived from ANC1</p> <p>Starting from ANC1 service counts and survey coverage, we calculate:</p> <p>Estimated pregnancies (base calculation):</p> \\[ d_{\\text{anc1-pregnancy}} = \\frac{\\text{count}_{\\text{anc1}} \\times 100}{\\text{coverage}_{\\text{anc1}}} \\] <p>Estimated deliveries (adjusted for pregnancy loss):</p> \\[ d_{\\text{anc1-delivery}} = d_{\\text{anc1-pregnancy}} \\times (1 - \\text{pregnancy loss rate}) \\] <p>Estimated births (adjusted for twin births):</p> \\[ d_{\\text{anc1-birth}} = d_{\\text{anc1-delivery}} / (1 - 0.5 \\times \\text{twin rate}) \\] <p>Estimated live births (adjusted for stillbirths):</p> \\[ d_{\\text{anc1-livebirth}} = d_{\\text{anc1-birth}} \\times (1 - \\text{stillbirth rate}) \\] <p>Population eligible for DPT/Penta vaccines (adjusted for neonatal mortality):</p> \\[ d_{\\text{anc1-dpt}} = d_{\\text{anc1-livebirth}} \\times (1 - \\text{neonatal mortality rate}) \\] <p>Population eligible for MCV1 (adjusted for post-neonatal mortality):</p> \\[ d_{\\text{anc1-measles1}} = d_{\\text{anc1-dpt}} \\times (1 - \\text{post-neonatal mortality rate}) \\] <p>Population eligible for MCV2 (adjusted for additional post-neonatal mortality):</p> \\[ d_{\\text{anc1-measles2}} = d_{\\text{anc1-dpt}} \\times (1 - 2 \\times \\text{post-neonatal mortality rate}) \\] <p>Denominators Derived from Delivery</p> <p>Starting from institutional delivery counts and survey coverage:</p> <p>Estimated live births (base calculation):</p> \\[ d_{\\text{delivery-livebirth}} = \\frac{\\text{count}_{\\text{delivery}} \\times 100}{\\text{coverage}_{\\text{delivery}}} \\] <p>Estimated births (adjusted for stillbirths):</p> \\[ d_{\\text{delivery-birth}} = d_{\\text{delivery-livebirth}} / (1 - \\text{stillbirth rate}) \\] <p>Estimated pregnancies (adjusted for twin births and pregnancy loss):</p> \\[ d_{\\text{delivery-pregnancy}} = d_{\\text{delivery-birth}} \\times (1 - 0.5 \\times \\text{twin rate}) / (1 - \\text{pregnancy loss rate}) \\] <p>Population eligible for DPT/Penta vaccines:</p> \\[ d_{\\text{delivery-dpt}} = d_{\\text{delivery-livebirth}} \\times (1 - \\text{neonatal mortality rate}) \\] <p>Population eligible for MCV1:</p> \\[ d_{\\text{delivery-measles1}} = d_{\\text{delivery-dpt}} \\times (1 - \\text{post-neonatal mortality rate}) \\] <p>Population eligible for MCV2:</p> \\[ d_{\\text{delivery-measles2}} = d_{\\text{delivery-dpt}} \\times (1 - 2 \\times \\text{post-neonatal mortality rate}) \\] <p>Note: Denominators derived from Skilled Birth Attendance (SBA) follow the same formulas as delivery denominators.</p> <p>Denominators Derived from BCG (National analysis only)</p> <p>Starting from BCG vaccination counts and survey coverage:</p> <p>Estimated live births (base calculation):</p> \\[ d_{\\text{bcg-livebirth}} = \\frac{\\text{count}_{\\text{bcg}} \\times 100}{\\text{coverage}_{\\text{bcg}}} \\] <p>Estimated pregnancies (working backwards through demographic adjustments):</p> \\[ d_{\\text{bcg-pregnancy}} = \\frac{d_{\\text{bcg-livebirth}}}{(1 - \\text{pregnancy loss rate}) \\times (1 + \\text{twin rate}) \\times (1 - \\text{stillbirth rate})} \\] <p>Population eligible for DPT/Penta vaccines:</p> \\[ d_{\\text{bcg-dpt}} = d_{\\text{bcg-livebirth}} \\times (1 - \\text{neonatal mortality rate}) \\] <p>Denominators Derived from Penta1</p> <p>Starting from Penta1 vaccination counts and survey coverage:</p> <p>Population eligible for DPT/Penta vaccines (base calculation):</p> \\[ d_{\\text{penta1-dpt}} = \\frac{\\text{count}_{\\text{penta1}} \\times 100}{\\text{coverage}_{\\text{penta1}}} \\] <p>Population eligible for MCV1:</p> \\[ d_{\\text{penta1-measles1}} = d_{\\text{penta1-dpt}} \\times (1 - \\text{post-neonatal mortality rate}) \\] <p>Population eligible for MCV2:</p> \\[ d_{\\text{penta1-measles2}} = d_{\\text{penta1-dpt}} \\times (1 - 2 \\times \\text{post-neonatal mortality rate}) \\] <p>Denominators Derived from Live Birth Counts</p> <p>When live birth data is directly reported in HMIS:</p> <p>Estimated live births (base calculation):</p> \\[ d_{\\text{livebirths-livebirth}} = \\frac{\\text{count}_{\\text{livebirth}} \\times 100}{\\text{coverage}_{\\text{livebirth}}} \\] <p>Estimated pregnancies (working backwards):</p> \\[ d_{\\text{livebirths-pregnancy}} = \\frac{d_{\\text{livebirths-livebirth}} \\times (1 - 0.5 \\times \\text{twin rate})}{(1 - \\text{stillbirth rate}) \\times (1 - \\text{pregnancy loss rate})} \\] <p>Estimated deliveries:</p> \\[ d_{\\text{livebirths-delivery}} = d_{\\text{livebirths-pregnancy}} \\times (1 - \\text{pregnancy loss rate}) \\] <p>Estimated births:</p> \\[ d_{\\text{livebirths-birth}} = d_{\\text{livebirths-livebirth}} / (1 - \\text{stillbirth rate}) \\] <p>Population eligible for DPT/Penta vaccines:</p> \\[ d_{\\text{livebirths-dpt}} = d_{\\text{livebirths-livebirth}} \\times (1 - \\text{neonatal mortality rate}) \\] <p>Population eligible for MCV1:</p> \\[ d_{\\text{livebirths-measles1}} = d_{\\text{livebirths-dpt}} \\times (1 - \\text{post-neonatal mortality rate}) \\] <p>Population eligible for MCV2:</p> \\[ d_{\\text{livebirths-measles2}} = d_{\\text{livebirths-dpt}} \\times (1 - 2 \\times \\text{post-neonatal mortality rate}) \\]"},{"location":"04_module_coverage_estimates_documentation/#unwpp-based-denominator-calculations","title":"UNWPP-based Denominator Calculations","text":"<p>Denominators Derived from UN World Population Prospects (WPP) (National analysis only)</p> <p>Instead of using service volumes, these denominators are calculated directly from population projections and demographic rates:</p> <p>Estimated pregnancies (from crude birth rate and total population):</p> \\[ d_{\\text{wpp-pregnancy}} = \\frac{\\text{Crude birth rate}}{1000} \\times \\text{Total population} \\times \\frac{1}{1 + \\text{twin rate}} \\] <p>Estimated live births (from crude birth rate):</p> \\[ d_{\\text{wpp-livebirth}} = \\frac{\\text{Crude birth rate}}{1000} \\times \\text{Total population} \\] <p>Population eligible for DPT/Penta vaccines (under-1 population):</p> \\[ d_{\\text{wpp-dpt}} = \\text{Total under-1 population from WPP} \\] <p>Population eligible for MCV1 (adjusted for neonatal mortality):</p> \\[ d_{\\text{wpp-measles1}} = d_{\\text{wpp-dpt}} \\times (1 - \\text{neonatal mortality rate}) \\] <p>Population eligible for MCV2 (adjusted for post-neonatal mortality):</p> \\[ d_{\\text{wpp-measles2}} = d_{\\text{wpp-dpt}} \\times (1 - \\text{neonatal mortality rate}) \\times (1 - 2 \\times \\text{post-neonatal mortality rate}) \\] <p>Adjustment for Incomplete Reporting:</p> <p>When HMIS data contains fewer than 12 months of reported data in a year, all UNWPP denominators are scaled to match the reporting period:</p> \\[ d_{\\text{adjusted}} = d_{\\text{wpp}} \\times \\frac{\\text{months reported}}{12} \\] <p>This adjustment ensures denominators are comparable to service volumes that may only represent partial-year reporting.</p> <p>Denominators Derived from Live Birth Estimates (Secondary Calculations)</p> <p>After all primary live birth denominators are calculated (from ANC1, Delivery, BCG, Penta1, Live Birth Counts, and WPP), the module generates additional denominators for specific interventions by applying age-specific mortality adjustments:</p> <p>Vitamin A Supplementation</p> <p>For each live birth denominator source, a corresponding Vitamin A denominator is calculated:</p> \\[ d_{\\text{source-vitaminA}} = d_{\\text{source-livebirth}} \\times (1 - \\text{under-5 mortality rate}) \\times 4.5 \\] <p>Where: - <code>source</code> represents any of: anc1, delivery, bcg, penta1, livebirths, or wpp - The factor 4.5 represents the approximate duration (in years) of the Vitamin A target age range, corresponding to children aged 6\u201359 months (\u2248 4.5 years). - Under-5 mortality rate accounts for child survival to the supplementation age range</p> <p>Fully Immunized Child (FIC)</p> <p>For each live birth denominator source, a corresponding FIC denominator is calculated:</p> \\[ d_{\\text{source-fully-immunized}} = d_{\\text{source-livebirth}} \\times (1 - \\text{infant mortality rate}) \\] <p>Where: - <code>source</code> represents any of: anc1, delivery, bcg, penta1, livebirths, or wpp - Infant mortality rate adjusts for survival to the age when full immunization status is assessed (typically 12 months)</p> <p>These secondary denominators are calculated automatically for all available live birth denominators, ensuring consistent methodology across different source indicators.</p> Output Files Specification <p>Part 1 generates six CSV files:</p> <p>Denominator Files</p> <p>1. M4_denominators_national.csv</p> <p>2. M4_denominators_admin2.csv</p> <p>3. M4_denominators_admin3.csv</p> <p>Structure:</p> <pre><code>admin_area_1, [admin_area_2/3], year, denominator, source_indicator, target_population, value\n</code></pre> <p>Fields:</p> <ul> <li><code>denominator</code>: Full denominator name (e.g., <code>danc1_livebirth</code>)</li> <li><code>source_indicator</code>: Service used (e.g., <code>source_anc1</code>, <code>source_wpp</code>)</li> <li><code>target_population</code>: Target group (e.g., <code>target_livebirth</code>, <code>target_dpt</code>)</li> <li><code>value</code>: Calculated denominator size</li> </ul> <p>Combined Results Files</p> <p>4. M4_combined_results_national.csv</p> <p>5. M4_combined_results_admin2.csv</p> <p>6. M4_combined_results_admin3.csv</p> <p>Structure:</p> <pre><code>admin_area_1, [admin_area_2/3], year, indicator_common_id, denominator_best_or_survey, value\n</code></pre> <p>Fields:</p> <ul> <li><code>indicator_common_id</code>: Health indicator (e.g., <code>anc1</code>, <code>penta3</code>)</li> <li><code>denominator_best_or_survey</code>: Either <code>best</code>, <code>survey</code>, or specific denominator name</li> <li><code>value</code>: Coverage percentage (0-100+)</li> </ul> <p>Special \"best\" Entry: Duplicates the selected optimal denominator for easy filtering</p> Data Safeguards and Validation <p>Part 1 includes multiple validation checks:</p> <ol> <li> <p>ISO3 Validation: Ensures survey and population data match HMIS country</p> </li> <li> <p>Geographic Matching: Validates admin area names between HMIS and survey</p> </li> <li>Reports match rate (e.g., \"15/20 regions match\")</li> <li> <p>Falls back to higher geographic level if mismatch detected</p> </li> <li> <p>Fallback Mechanisms:</p> </li> <li>Subnational \u2192 National if no local survey data</li> <li>SBA \u2192 Delivery if SBA missing</li> <li> <p>PNC1_mother \u2192 PNC1 if missing</p> </li> <li> <p>Edge Case Handling: Detects when admin_area_3 should be used as admin_area_2 in certain country contexts</p> </li> <li> <p>Empty Data Handling: Creates empty CSVs with correct structure when data unavailable</p> </li> <li> <p>Error Handling: Wraps survey processing in <code>tryCatch</code> to handle mismatches gracefully</p> </li> </ol> Indicators Supported <p>Part 1 processes the following health indicators:</p> <p>Maternal Health:</p> <ul> <li><code>anc1</code>: Antenatal care 1<sup>st</sup> visit</li> <li><code>anc4</code>: Antenatal care 4+ visits</li> <li><code>delivery</code>: Institutional delivery</li> <li><code>sba</code>: Skilled birth attendance</li> <li><code>pnc1</code>: Postnatal care (child)</li> <li><code>pnc1_mother</code>: Postnatal care (mother)</li> </ul> <p>Immunization:</p> <ul> <li><code>bcg</code>: BCG vaccine</li> <li><code>penta1</code>, <code>penta2</code>, <code>penta3</code>: Pentavalent vaccine</li> <li><code>measles1</code>, <code>measles2</code>: Measles-containing vaccine</li> <li><code>rota1</code>, <code>rota2</code>: Rotavirus vaccine</li> <li><code>opv1</code>, <code>opv2</code>, <code>opv3</code>: Oral polio vaccine</li> <li><code>fully_immunized</code>: Full immunization status</li> </ul> <p>Child Health:</p> <ul> <li><code>nmr</code>: Neonatal mortality rate (survey only)</li> <li><code>imr</code>: Infant mortality rate (survey only)</li> <li><code>vitaminA</code>: Vitamin A supplementation</li> </ul> Usage Notes and Best Practices <p>When to Use Which Count Variable</p> <ul> <li><code>count_final_none</code>: No adjustments (raw reported data)</li> <li><code>count_final_outlier</code>: Outlier adjustment only</li> <li><code>count_final_completeness</code>: Completeness adjustment only</li> <li><code>count_final_both</code>: Both adjustments (recommended)</li> </ul> <p>Interpreting \"best\" Denominators</p> <p>The \"best\" denominator may vary by indicator and area based on:</p> <ul> <li>Data availability (some services not universally reported)</li> <li>Reporting completeness (affects HMIS-based denominators)</li> <li>Population projection quality (affects WPP denominators)</li> <li>Survey coverage levels (extreme values reduce denominator options)</li> </ul> <p>Why Multiple Denominators?</p> <p>Different denominators serve different purposes:</p> <ul> <li>Independent denominators: Provide cross-validation between services</li> <li>Reference denominators: Show internal HMIS consistency (but excluded from \"best\" by default)</li> <li>WPP denominators: Offer population-based benchmarks</li> <li>Comparing multiple options reveals data quality issues</li> </ul> Troubleshooting Common Issues <p>Issue: No matching admin areas between HMIS and survey</p> <ul> <li>Solution: Check ISO3 code is correct; verify admin area naming conventions; module will fall back to national analysis</li> </ul> <p>Issue: All denominators show &gt;100% coverage</p> <ul> <li>Solution: May indicate under-reporting in survey or over-reporting in HMIS; check data quality from Module 2</li> </ul> <p>Issue: UNWPP selected as \"best\" for most indicators</p> <ul> <li>Solution: May indicate poor HMIS data quality or completeness; review Module 2 adjustments</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#part-2-denominator-selection-and-survey-projection-technical-details","title":"Part 2: Denominator Selection and Survey Projection (Technical Details)","text":""},{"location":"04_module_coverage_estimates_documentation/#purpose-and-objectives","title":"Purpose and Objectives","text":"<p>Part 2 serves three key purposes:</p> <ol> <li> <p>User-Driven Denominator Selection: While Part 1 automatically selects the \"best\" denominator by minimizing error against survey data, Part 2 allows users to override this selection and choose specific denominators based on programmatic knowledge or policy priorities</p> </li> <li> <p>Temporal Trend Analysis: Computes year-over-year changes (deltas) in coverage to understand service delivery trends over time</p> </li> <li> <p>Survey Projection: Projects survey-based coverage estimates forward in time using trends observed in administrative (HMIS) data, filling gaps where survey data is unavailable</p> </li> </ol>"},{"location":"04_module_coverage_estimates_documentation/#user-configuration","title":"User Configuration","text":"<p>Users configure Part 2 through two key parameter sets:</p> 1. Denominator Selection Configuration <p>At the top of the script, users specify which denominator to use for each indicator:</p> <pre><code>DENOMINATOR_SELECTION &lt;- list(\n  # PREGNANCY-RELATED INDICATORS\n  anc1 = \"best\",                    # Options: \"best\", \"danc1_pregnancy\", \"ddelivery_pregnancy\", \"dbcg_pregnancy\", \"dlivebirths_pregnancy\", \"dwpp_pregnancy\"\n  anc4 = \"best\",\n\n  # LIVE BIRTH-RELATED INDICATORS\n  delivery = \"best\",                # Options: \"best\", \"danc1_livebirth\", \"ddelivery_livebirth\", \"dbcg_livebirth\", \"dlivebirths_livebirth\", \"dwpp_livebirth\"\n  bcg = \"best\",\n  sba = \"best\",\n  pnc1_mother = \"best\",\n  pnc1 = \"best\",\n\n  # DPT-ELIGIBLE AGE GROUP INDICATORS\n  penta1 = \"best\",                  # Options: \"best\", \"danc1_dpt\", \"ddelivery_dpt\", \"dpenta1_dpt\", \"dbcg_dpt\", \"dlivebirths_dpt\", \"dwpp_dpt\"\n  penta2 = \"best\",\n  penta3 = \"best\",\n  opv1 = \"best\",\n  opv2 = \"best\",\n  opv3 = \"best\",\n\n  # MEASLES-ELIGIBLE AGE GROUP INDICATORS\n  measles1 = \"best\",                # Options: \"best\", \"danc1_measles1\", \"ddelivery_measles1\", \"dpenta1_measles1\", \"dbcg_measles1\", \"dlivebirths_measles1\", \"dwpp_measles1\"\n  measles2 = \"best\",\n\n  # ADDITIONAL INDICATORS\n  vitaminA = \"best\",                # Options: \"best\", \"danc1_vitaminA\", \"dbcg_vitaminA\", \"ddelivery_vitaminA\", \"dwpp_vitaminA\"\n  fully_immunized = \"best\"          # Options: \"best\", \"danc1_fully_immunized\", \"dbcg_fully_immunized\", \"ddelivery_fully_immunized\", \"dwpp_fully_immunized\"\n)\n</code></pre> <p>Denominator Options by Indicator Type:</p> <p>The available denominators vary by indicator type based on the appropriate target population:</p> <ul> <li>Pregnancy-based indicators (ANC1, ANC4): Use pregnancy-adjusted denominators</li> <li>Live birth-based indicators (Delivery, BCG, SBA, PNC): Use live birth-adjusted denominators</li> <li>DPT-eligible age group (Penta1-3, OPV1-3): Use DPT-adjusted denominators (children eligible for DPT)</li> <li>Measles-eligible age group (Measles1, Measles2): Use measles-adjusted denominators (children eligible for measles vaccine)</li> </ul> <p>Each denominator option combines a source (ANC1, Delivery, BCG, Penta1, or WPP) with an age-adjustment factor.</p> 2. Administrative Level Configuration <pre><code>RUN_NATIONAL &lt;- TRUE  # Always TRUE - national analysis is mandatory\nRUN_ADMIN2 &lt;- TRUE    # Enable/disable admin level 2 analysis\nRUN_ADMIN3 &lt;- TRUE    # Enable/disable admin level 3 analysis\n</code></pre> <p>The script automatically checks data availability and disables admin levels with no data.</p>"},{"location":"04_module_coverage_estimates_documentation/#core-functions-and-methods","title":"Core Functions and Methods","text":"Function 1: <code>coverage_deltas()</code> <p>Purpose: Calculates year-over-year changes in coverage for each indicator-denominator-geography combination.</p> <p>Algorithm:</p> <pre><code>coverage_deltas &lt;- function(coverage_df, lag_n = 1, complete_years = TRUE)\n</code></pre> <p>Process:</p> <ol> <li>Groups data by geography (admin areas), indicator, and denominator</li> <li>Optionally fills in missing years to create a complete time series</li> <li>Sorts data chronologically within each group</li> <li>Calculates delta as: \\(\\Delta\\text{coverage}_t = \\text{coverage}_t - \\text{coverage}_{t-1}\\)</li> </ol> <p>Mathematical Formulation: $$ \\Delta C_{i,d,g,t} = C_{i,d,g,t} - C_{i,d,g,t-1} $$</p> <p>where: - \\(C\\) = coverage estimate - \\(i\\) = indicator - \\(d\\) = denominator - \\(g\\) = geographic area - \\(t\\) = time (year)</p> <p>Input:</p> <ul> <li><code>coverage_df</code>: Data frame with coverage estimates</li> <li><code>lag_n</code>: Number of years to lag (default = 1 for year-over-year)</li> <li><code>complete_years</code>: Whether to fill missing years (default = TRUE)</li> </ul> <p>Output:</p> <p>Data frame with original coverage values plus a <code>delta</code> column showing year-over-year change.</p> <p>Example Output:</p> admin_area_1 indicator_common_id denominator year coverage delta Country A penta3 dpenta1_dpt 2018 75.2 NA Country A penta3 dpenta1_dpt 2019 78.5 3.3 Country A penta3 dpenta1_dpt 2020 80.1 1.6 Function 2: <code>project_survey_from_deltas()</code> <p>Purpose: Projects survey-based coverage estimates forward using administrative data trends.</p> <p>Algorithm:</p> <pre><code>project_survey_from_deltas &lt;- function(deltas_df, survey_raw_long)\n</code></pre> <p>Process:</p> <ol> <li>Identify Baseline: For each geography-indicator combination, find the most recent survey observation</li> <li>Extract the last observed survey year</li> <li> <p>Record the baseline coverage value at that year</p> </li> <li> <p>Attach Baseline to Each Denominator Path: Since Part 2 operates on specific denominator selections, attach the baseline to each denominator series</p> </li> <li> <p>Compute Cumulative Deltas: For years after the baseline year, calculate cumulative sum of deltas:</p> </li> </ol> <p>\\(\\(\\text{cumulative delta}_t = \\sum_{\\tau = \\text{baseline year} + 1}^{t} \\Delta C_\\tau\\)\\)</p> <ol> <li>Calculate Projection: Add cumulative delta to baseline value:</li> </ol> <p>\\(\\(\\text{Projected coverage}_t = \\text{Baseline coverage} + \\text{cumulative delta}_t\\)\\)</p> <p>Mathematical Formulation:</p> <p>For each indicator \\(i\\), denominator \\(d\\), and geography \\(g\\):</p> <ol> <li>Find baseline:</li> </ol> \\[ y_{\\text{baseline}} = \\max\\{t : S_{i,g,t} \\text{ exists}\\} \\] \\[ S_{\\text{baseline}} = S_{i,g,y_{\\text{baseline}}} \\] <ol> <li>For \\(t &gt; y_{\\text{baseline}}\\):</li> </ol> \\[ \\hat{S}_{i,d,g,t} = S_{\\text{baseline}} + \\sum_{\\tau = y_{\\text{baseline}} + 1}^{t} \\Delta C_{i,d,g,\\tau} \\] <p>where:</p> <ul> <li>\\(S\\) = survey-based coverage estimate</li> <li>\\(\\hat{S}\\) = projected survey coverage</li> <li>\\(\\Delta C\\) = year-over-year change in administrative coverage</li> </ul> <p>Assumptions:</p> <ul> <li>Trends observed in administrative data reflect true changes in service coverage</li> <li>The baseline survey provides an accurate reference point</li> <li>Administrative data trends can be applied to survey estimates</li> </ul> <p>Input:</p> <ul> <li><code>deltas_df</code>: Output from <code>coverage_deltas()</code> containing coverage changes</li> <li><code>survey_raw_long</code>: Raw survey data with years and values</li> </ul> <p>Output:</p> <p>Data frame with projected coverage for each year, indicator, denominator, and geography combination.</p> <p>Example Output:</p> admin_area_1 indicator_common_id denominator year baseline_year projected Country A penta3 dpenta1_dpt 2018 2018 75.0 Country A penta3 dpenta1_dpt 2019 2018 78.3 Country A penta3 dpenta1_dpt 2020 2018 79.9 Function 3: <code>build_final_results()</code> <p>Purpose: Combines HMIS coverage, projected survey estimates, and original survey values into a unified output dataset.</p> <p>Algorithm:</p> <pre><code>build_final_results &lt;- function(coverage_df, proj_df, survey_raw_df = NULL)\n</code></pre> <p>Process:</p> <ol> <li>Prepare HMIS Coverage: Extract coverage estimates from administrative data</li> <li> <p>Rename coverage column to <code>coverage_cov</code> for clarity</p> </li> <li> <p>Merge Projections: Join projected survey estimates</p> </li> <li>Match by geography, year, indicator, and denominator</li> <li> <p>Create <code>coverage_avgsurveyprojection</code> column</p> </li> <li> <p>Process Original Survey Data (if available):</p> </li> <li>Collapse multiple survey sources by taking mean value</li> <li>Preserve source metadata (source, source_detail)</li> <li> <p>Expand survey values across all denominators for that indicator</p> </li> <li> <p>Calculate Final Projections: Use an improved projection formula that anchors to the last survey value:</p> </li> </ol> <p>For years after the last survey year:</p> <p>$$    \\text{Projected coverage}t = \\text{Last survey value} + (C)    $$},t} - C_{\\text{HMIS, last survey year}</p> <p>This additive approach:    - Preserves the calibration to survey data    - Applies the HMIS trend (delta) to extend the estimate forward    - Avoids compounding errors from year-to-year deltas</p> <ol> <li>Combine Results: Merge all components using full outer join to preserve:</li> <li>Years with only HMIS data</li> <li>Years with only survey data</li> <li>Years with both data sources</li> </ol> <p>Mathematical Formulation:</p> <p>Let:</p> <ul> <li>\\(t_s\\) = year of last survey</li> <li>\\(S_{t_s}\\) = survey coverage at year \\(t_s\\)</li> <li>\\(C_{\\text{HMIS},t}\\) = HMIS-based coverage at year \\(t\\)</li> </ul> <p>For \\(t &gt; t_s\\):</p> \\[ \\hat{C}_t = S_{t_s} + (C_{\\text{HMIS},t} - C_{\\text{HMIS},t_s}) \\] <p>Input:</p> <ul> <li><code>coverage_df</code>: HMIS-based coverage estimates from selected denominators</li> <li><code>proj_df</code>: Projected survey estimates from <code>project_survey_from_deltas()</code></li> <li><code>survey_raw_df</code>: Original survey data (optional)</li> </ul> <p>Output:</p> <p>Comprehensive data frame with columns:</p> <ul> <li>Geographic identifiers (admin_area_1, admin_area_2, admin_area_3)</li> <li>year, indicator_common_id, denominator</li> <li><code>coverage_cov</code>: HMIS-based coverage</li> <li><code>coverage_original_estimate</code>: Original survey values</li> <li><code>coverage_avgsurveyprojection</code>: Projected survey coverage</li> <li><code>survey_raw_source</code>: Survey data source (e.g., \"DHS\", \"MICS\")</li> <li><code>survey_raw_source_detail</code>: Detailed source information</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#helper-functions","title":"Helper Functions","text":"Helper Function: <code>filter_by_denominator_selection()</code> <p>Purpose: Filters the combined results from Part 1 based on user's denominator selection.</p> <p>Algorithm:</p> <ol> <li>Iterate through each indicator in <code>DENOMINATOR_SELECTION</code></li> <li>For each indicator:</li> <li>If selection is \"best\": Keep rows where <code>denominator_best_or_survey == \"best\"</code></li> <li>If selection is a specific denominator: Keep rows where <code>denominator_best_or_survey == selected_denominator</code></li> <li>Convert selected rows to coverage format (rename columns, filter out survey entries)</li> <li>Combine results across all indicators</li> </ol> <p>Input:</p> <ul> <li><code>combined_results_df</code>: Output from Part 1 with all denominator options</li> <li><code>selection_list</code>: The DENOMINATOR_SELECTION configuration list</li> </ul> <p>Output:</p> <p>Filtered data frame containing only the user-selected denominators.</p> Helper Function: <code>extract_survey_from_combined()</code> <p>Purpose: Extracts raw survey values from Part 1 combined results.</p> <p>Algorithm:</p> <ol> <li>Filter for rows where <code>denominator_best_or_survey == \"survey\"</code></li> <li>Rename <code>value</code> column to <code>survey_value</code></li> <li>Select relevant columns dynamically based on admin levels present</li> </ol> <p>Input:</p> <p>Combined results data frame from Part 1</p> <p>Output:</p> <p>Survey data frame with columns: admin areas, year, indicator_common_id, survey_value</p>"},{"location":"04_module_coverage_estimates_documentation/#workflow-execution-steps","title":"Workflow Execution Steps","text":"<p>Part 2 executes the following workflow for each administrative level (national, admin2, admin3):</p> <p>Step 1: Load Data</p> <ul> <li>Load combined results from Part 1 for all admin levels</li> <li>Check which admin levels have data</li> <li>Extract survey data for use as projection baseline</li> <li>Display messages about data availability</li> </ul> <p>Step 2: For Each Admin Level</p> <p>Sub-step 1: Filter by Denominator Selection</p> <ul> <li>Apply user's denominator choices using <code>filter_by_denominator_selection()</code></li> <li>Message: Number of records selected</li> </ul> <p>Sub-step 2: Compute Deltas</p> <ul> <li>Calculate year-over-year coverage changes using <code>coverage_deltas()</code></li> <li>Creates complete time series with gaps filled</li> </ul> <p>Sub-step 3: Project Survey Values</p> <ul> <li>Use <code>project_survey_from_deltas()</code> to extend survey estimates</li> <li>Baseline is anchored to most recent survey</li> <li>Projections use cumulative deltas from HMIS trends</li> </ul> <p>Sub-step 4: Build Final Results</p> <ul> <li>Combine HMIS coverage, projections, and original surveys</li> <li>Calculate final projected estimates using additive formula</li> <li>Preserve all metadata</li> </ul> <p>Step 3: Standardize and Save Outputs</p> <ul> <li>Define required columns for each admin level</li> <li>Ensure all required columns exist (add as NA if missing)</li> <li>Order columns correctly</li> <li>Remove inappropriate admin level columns</li> <li>Save as CSV with UTF-8 encoding</li> <li>Create empty files for admin levels with no data</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#output-specifications","title":"Output Specifications","text":"<p>Part 2 produces three output files:</p>"},{"location":"04_module_coverage_estimates_documentation/#1-national-output-m5_coverage_estimation_nationalcsv","title":"1. National Output: <code>M5_coverage_estimation_national.csv</code>","text":"<p>Columns:</p> <ul> <li><code>admin_area_1</code>: Country name</li> <li><code>year</code>: Year of estimate</li> <li><code>indicator_common_id</code>: Standardized indicator code</li> <li><code>denominator</code>: Selected denominator source</li> <li><code>coverage_original_estimate</code>: Original survey-based coverage (NA for years without surveys)</li> <li><code>coverage_avgsurveyprojection</code>: Projected survey coverage using HMIS trends</li> <li><code>coverage_cov</code>: HMIS-based coverage estimate</li> <li><code>survey_raw_source</code>: Survey source (e.g., \"DHS 2018\")</li> <li><code>survey_raw_source_detail</code>: Additional source details</li> </ul> <p>Note: Does NOT include admin_area_2 or admin_area_3 columns.</p>"},{"location":"04_module_coverage_estimates_documentation/#2-admin-level-2-output-m5_coverage_estimation_admin2csv","title":"2. Admin Level 2 Output: <code>M5_coverage_estimation_admin2.csv</code>","text":"<p>Columns:</p> <p>Same as national, plus:</p> <ul> <li><code>admin_area_2</code>: Second-level administrative division name (e.g., province, region)</li> </ul> <p>Note: Does NOT include admin_area_3 column.</p>"},{"location":"04_module_coverage_estimates_documentation/#3-admin-level-3-output-m5_coverage_estimation_admin3csv","title":"3. Admin Level 3 Output: <code>M5_coverage_estimation_admin3.csv</code>","text":"<p>Columns:</p> <ul> <li><code>admin_area_1</code>: Country name</li> <li><code>admin_area_3</code>: Third-level administrative division name (e.g., district)</li> <li><code>year</code>: Year of estimate</li> <li><code>indicator_common_id</code>: Standardized indicator code</li> <li><code>denominator</code>: Selected denominator source</li> <li><code>coverage_original_estimate</code>: Original survey coverage</li> <li><code>coverage_avgsurveyprojection</code>: Projected survey coverage</li> <li><code>coverage_cov</code>: HMIS-based coverage</li> <li><code>survey_raw_source</code>: Survey source</li> <li><code>survey_raw_source_detail</code>: Source details</li> </ul> <p>Note: Does NOT include admin_area_2 column (skips straight from admin_area_1 to admin_area_3).</p>"},{"location":"04_module_coverage_estimates_documentation/#methodological-considerations","title":"Methodological Considerations","text":""},{"location":"04_module_coverage_estimates_documentation/#1-denominator-selection-strategy","title":"1. Denominator Selection Strategy","text":"<p>When to use \"best\":</p> <ul> <li>Uncertain about which denominator is most appropriate</li> <li>Want to rely on data-driven selection from Part 1</li> <li>Starting point for analysis</li> </ul> <p>When to specify a denominator:</p> <ul> <li>Programmatic knowledge suggests a specific denominator is most accurate</li> <li>Policy requirements dictate use of specific population estimates</li> <li>Conducting sensitivity analyses</li> <li>Known issues with certain data sources</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#2-projection-methodology","title":"2. Projection Methodology","text":"<p>The projection approach in Part 2 uses an additive delta method rather than multiplicative or direct replacement:</p> <p>Advantages:</p> <ul> <li>Preserves the level calibration from survey data</li> <li>Smoothly extends survey estimates using administrative trends</li> <li>Avoids compounding errors from year-to-year changes</li> <li>Maintains consistency when HMIS coverage is stable</li> </ul> <p>Limitations:</p> <ul> <li>Assumes HMIS trends reflect true coverage changes</li> <li>May diverge from reality if administrative data quality declines</li> <li>Projections become less reliable further from baseline survey</li> <li>Does not account for systematic biases in HMIS data</li> </ul> <p>Best Practice: Projections should be validated against new survey data when available, and the baseline should be updated with the most recent survey.</p>"},{"location":"04_module_coverage_estimates_documentation/#3-handling-missing-data","title":"3. Handling Missing Data","text":"<p>Part 2 implements several strategies for missing data:</p> <ul> <li>Complete time series: The <code>coverage_deltas()</code> function can fill missing years, creating a continuous series</li> <li>Survey gaps: Projections extend estimates forward, but years before the first survey remain NA</li> <li>Admin level gaps: Script automatically detects and skips admin levels with no data</li> <li>Missing denominators: If a selected denominator doesn't exist for an indicator, that indicator-denominator combination is omitted</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#4-multi-level-analysis-consistency","title":"4. Multi-Level Analysis Consistency","text":"<p>Part 2 processes each administrative level independently:</p> <ul> <li>National: Aggregated country-level estimates</li> <li>Admin 2: Provincial/regional estimates (may not sum to national due to different denominators)</li> <li>Admin 3: District-level estimates</li> </ul> <p>Important: Estimates across levels may not be directly comparable if different denominators are selected or if data quality varies by level.</p>"},{"location":"04_module_coverage_estimates_documentation/#example-use-case","title":"Example Use Case","text":"<p>Scenario: Analyst wants to estimate Penta3 coverage for 2015-2024 using Penta1-derived denominators.</p> <p>Configuration:</p> <pre><code>DENOMINATOR_SELECTION &lt;- list(\n  penta3 = \"dpenta1_dpt\"  # Override \"best\" to use Penta1-based denominator\n)\n</code></pre> <p>Input Data:</p> <ul> <li>Part 1 combined results with multiple denominators calculated</li> <li>Survey data available for 2018 (75.0% coverage)</li> <li>HMIS data available for all years 2015-2024</li> </ul> <p>Process:</p> <ol> <li>Filter: Extract only Penta3 estimates using dpenta1_dpt denominator</li> <li>Compute Deltas: Calculate year-over-year changes in HMIS-based coverage</li> <li>Project:</li> <li>Baseline year: 2018 (last survey)</li> <li>Baseline value: 75.0%</li> <li>For 2019: 75.0% + (HMIS 2019 - HMIS 2018)</li> <li>For 2020: 75.0% + (HMIS 2020 - HMIS 2018)</li> <li>Continue through 2024</li> </ol> <p>Output:</p> <ul> <li>Years 2015-2017: Only HMIS coverage available</li> <li>Year 2018: Survey value (75.0%) and HMIS coverage</li> <li>Years 2019-2024: Projected coverage based on HMIS trends</li> </ul>"},{"location":"04_module_coverage_estimates_documentation/#validation-and-quality-checks","title":"Validation and Quality Checks","text":"<p>Users should validate Part 2 outputs by:</p> <ol> <li>Checking projection reasonableness:</li> <li>Are projected values within plausible ranges (0-100%)?</li> <li> <p>Do trends make programmatic sense?</p> </li> <li> <p>Comparing denominators:</p> </li> <li>Run Part 2 with different denominator selections</li> <li> <p>Assess sensitivity of results to denominator choice</p> </li> <li> <p>Validating against new surveys:</p> </li> <li>When new survey data becomes available, compare projections to actual values</li> <li> <p>Update baseline and re-run if necessary</p> </li> <li> <p>Reviewing HMIS trends:</p> </li> <li>Large deltas may indicate data quality issues</li> <li> <p>Sudden changes should be investigated</p> </li> <li> <p>Admin level consistency:</p> </li> <li>Check if subnational trends align with national patterns</li> <li>Investigate large discrepancies</li> </ol>"},{"location":"04_module_coverage_estimates_documentation/#integration-with-part-1","title":"Integration with Part 1","text":"<p>Part 2 builds directly on Part 1 outputs:</p> Part 1 Output Part 2 Use Combined results with all denominators Input for denominator filtering \"Best\" denominator selections Default option if user doesn't specify Coverage estimates Basis for delta calculations Survey values Baseline for projections Source metadata Preserved in final output <p>Workflow Connection:</p> <pre><code>Part 1: Calculate denominators \u2192 Select \"best\" \u2192 Output combined results\n                                                          \u2193\nPart 2: User selects denominators \u2192 Calculate trends \u2192 Project surveys \u2192 Final estimates\n</code></pre>"},{"location":"04_module_coverage_estimates_documentation/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>Issue: \"No data in admin2 combined results\"</p> <ul> <li>Cause: Part 1 didn't process admin level 2, or no subnational data exists</li> <li>Solution: Set <code>RUN_ADMIN2 &lt;- FALSE</code> or check Part 1 inputs</li> </ul> <p>Issue: Projections show implausible values (&gt;100% or &lt;0%)</p> <ul> <li>Cause: Large errors in HMIS data or inappropriate denominator</li> <li>Solution: Review denominator selection, check HMIS data quality, consider different denominator</li> </ul> <p>Issue: Missing denominators in output</p> <ul> <li>Cause: Selected denominator not calculated in Part 1 for that indicator</li> <li>Solution: Check Part 1 denominator options, verify indicator-denominator compatibility</li> </ul> <p>Issue: Gaps in projected coverage</p> <ul> <li>Cause: Missing HMIS data for some years</li> <li>Solution: Review Module 2 outputs, check data completeness adjustments</li> </ul> <p>Last updated: 10-11-2025 Contact: FASTR Project Team</p>"},{"location":"disclaimer/","title":"Disclaimer and Copyright","text":"<p>\u00a9 2024 International Bank for Reconstruction and Development / The World Bank</p> <p>1818 H Street NW Washington DC 20433 Telephone: 202-473-1000 Internet: www.worldbank.org</p> <p>This work is a product of the staff of The World Bank with external contributions. The findings, interpretations, and conclusions expressed in this work do not necessarily reflect the views of The World Bank, its Board of Executive Directors, or the governments they represent.</p> <p>The World Bank does not guarantee the accuracy, completeness, or currency of the data included in this work and does not assume responsibility for any errors, omissions, or discrepancies in the information, or liability with respect to the use of or failure to use the information, methods, processes, or conclusions set forth. The boundaries, colors, denominations, and other information shown on any map in this work do not imply any judgment on the part of The World Bank concerning the legal status of any territory or the endorsement or acceptance of such boundaries.</p> <p>Nothing herein shall constitute or be construed or considered to be a limitation upon or waiver of the privileges and immunities of The World Bank, all of which are specifically reserved.</p>"},{"location":"disclaimer/#rights-and-permissions","title":"Rights and Permissions","text":"<p>The material in this work is subject to copyright. Because The World Bank encourages dissemination of its knowledge, this work may be reproduced, in whole or in part, for noncommercial purposes as long as full attribution to this work is given.</p> <p>Any queries on rights and licenses, including subsidiary rights, should be addressed to World Bank Publications, The World Bank Group, 1818 H Street NW, Washington, DC 20433, USA; fax: 202-522-2625; e-mail: pubrights@worldbank.org.</p>"},{"location":"executive_summary/","title":"Executive Summary","text":""},{"location":"executive_summary/#fastr-rmncah-n-service-use-monitoring","title":"FASTR RMNCAH-N Service Use Monitoring","text":"<p>This document describes a four-module analytical approach for monitoring reproductive, maternal, newborn, child, and adolescent health and nutrition (RMNCAH-N) service delivery using routine health management information system (HMIS) data. The methodology connects to DHIS2 systems to retrieve facility-level data, applies statistical quality assessment and adjustment procedures, and generates data quality metrics, service utilization estimates, and population coverage indicators.</p>"},{"location":"executive_summary/#background","title":"Background","text":"<p>Health management information systems in low- and middle-income countries generate routine facility-level service delivery data on a monthly basis. However, these data are frequently affected by reporting incompleteness, statistical outliers, and internal inconsistencies that limit their analytical utility. Traditional household surveys (DHS, MICS) provide validated coverage estimates but are conducted infrequently (typically every 3-5 years), creating gaps in the availability of timely data for monitoring service delivery trends, detecting disruptions, and tracking progress toward health system goals.</p> <p>This methodology addresses these constraints through a four-module analytical pipeline that systematically assesses and adjusts for data quality issues in routine HMIS data, then combines adjusted facility-level data with periodic survey benchmarks to generate continuous time series of coverage estimates at subnational geographic levels.</p>"},{"location":"executive_summary/#analytical-approach","title":"Analytical Approach","text":""},{"location":"executive_summary/#module-1-data-quality-assessment","title":"Module 1: Data Quality Assessment","text":"<p>Applies statistical methods to identify outliers using median absolute deviation, tracks reporting completeness at facility and indicator levels, and validates logical consistency between related indicators (e.g., ANC1 \u2265 ANC4). Generates facility-level quality scores and flags for downstream use in data adjustment and analysis procedures.</p>"},{"location":"executive_summary/#module-2-data-quality-adjustments","title":"Module 2: Data Quality Adjustments","text":"<p>Generates four parallel versions of the dataset: (1) original unadjusted data, (2) outlier-adjusted data only, (3) missing data imputed only, and (4) both adjustments applied. Outlier adjustment replaces flagged values with 6-month rolling medians (with fallback to forward/backward fill or same-month-prior-year values). Missing data imputation follows the same hierarchical approach. All four versions are retained to support sensitivity analysis.</p>"},{"location":"executive_summary/#module-3-service-utilization-disruption-analysis","title":"Module 3: Service Utilization &amp; Disruption Analysis","text":"<p>Applies statistical process control methods (control charts with robust regression) to identify months where service volumes deviate significantly from expected patterns after accounting for seasonality and trends. Uses panel regression models at national, regional, and district levels to quantify the magnitude of service shortfalls or surpluses during flagged disruption periods. Outliers identified in Module 1 are excluded from the analysis.</p>"},{"location":"executive_summary/#module-4-coverage-estimation-projection","title":"Module 4: Coverage Estimation &amp; Projection","text":"<p>Part 1 - Denominators: Calculates target population denominators by combining HMIS service counts with survey-reported coverage rates. Multiple denominator options are derived from different HMIS indicators (ANC1, deliveries, BCG, Penta1) and UN population estimates. Denominators are adjusted for biological factors including pregnancy loss, stillbirths, twin births, and mortality rates. Optimal denominators are selected per indicator by minimizing squared error between calculated coverage and survey benchmarks.</p> <p>Part 2 - Projections: Generates coverage projections for years beyond the most recent survey by applying annual percent changes observed in HMIS data to the survey baseline. For each indicator-year-location, three values are produced: actual survey estimates where available, HMIS-anchored projections, and direct HMIS-based coverage. Analysis is conducted at national, admin area 2, and admin area 3 levels where survey data permits.</p>"},{"location":"executive_summary/#module-integration","title":"Module Integration","text":"<p>The four modules operate sequentially with outputs from each module serving as inputs to subsequent stages:</p> <ul> <li>Module 2 applies quality flags generated by Module 1 to determine which observations require adjustment</li> <li>Module 3 accepts all four adjustment scenarios from Module 2 as input; users specify which scenario to apply in disruption analysis</li> <li>Module 4 Part 1 loads all four scenarios from Module 2; users specify which scenario to use as the numerator in coverage calculations</li> <li>Module 4 Part 2 loads all denominator options and coverage estimates from Part 1; users specify the preferred denominator per indicator for projection calculations</li> </ul>"},{"location":"executive_summary/#methodological-characteristics","title":"Methodological Characteristics","text":"<p>The analytical approach exhibits several characteristics relevant to implementation and interpretation:</p> <p>Multiple Scenarios: The retention of four parallel data adjustment scenarios (none, outliers only, missing data only, both) supports sensitivity analysis and assessment of how data quality assumptions affect results.</p> <p>Geographic Disaggregation: Analyses are conducted at national, admin area 2, and admin area 3 levels where data quality and sample sizes permit, subject to minimum thresholds for statistical reliability.</p> <p>Temporal Resolution: Monthly HMIS data enables higher-frequency monitoring than periodic household surveys, while survey anchoring maintains comparability with established benchmarks and accounts for systematic biases in routine reporting.</p> <p>Parameter Configuration: All statistical thresholds, window sizes, and adjustment methods are configurable parameters documented in technical specifications, allowing adaptation to country-specific contexts and data characteristics.</p>"}]}
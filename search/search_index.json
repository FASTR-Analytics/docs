{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FASTR Methodology Portal","text":"<p>Welcome! This site collects the background materials and module-level methodology guidance for the FASTR toolkit. Each chapter below is a standalone reference, and you can jump to whichever module you need.</p>"},{"location":"#chapters","title":"Chapters","text":"<ul> <li>Background Documentation</li> <li>Module 1 \u00b7 Data Quality Assessment</li> <li>Module 2 \u00b7 Data Quality Adjustments</li> <li>Module 3 \u00b7 Service Utilization</li> <li>Module 4 \u00b7 Coverage Estimates</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install MkDocs (<code>pip install mkdocs</code>).</li> <li>From this directory run <code>mkdocs serve</code> to preview locally or <code>mkdocs build</code> to create a production-ready site in the <code>site/</code> folder.</li> <li>Deploy the generated <code>site/</code> contents to any static hosting service.</li> </ol> <p>Feel free to update the chapters or navigation entries; the site configuration automatically pulls in the Markdown files listed above.</p>"},{"location":"00_background_documentation/","title":"Background Documentation","text":"<p>This section contains background information.</p>"},{"location":"01_module_data_quality_assessment_documentation/","title":"Module 1: Data Quality Assessment (DQA)","text":""},{"location":"01_module_data_quality_assessment_documentation/#1-overview-what-why","title":"1. Overview (What &amp; Why)","text":""},{"location":"01_module_data_quality_assessment_documentation/#what-does-this-module-do","title":"What does this module do?","text":"<p>The Data Quality Assessment (DQA) module evaluates the reliability of Health Management Information System (HMIS) data from health facilities. It acts as a quality control checkpoint in the FASTR pipeline, examining monthly facility reports to identify data issues before the information is used for decision-making.</p> <p>The module assesses data quality through three complementary lenses: detecting outliers (unusually high values that may indicate reporting errors), assessing completeness (whether facilities consistently submit their reports), and measuring consistency (whether related health indicators align with expected patterns). These assessments are combined into an overall DQA score that provides a single measure of data reliability.</p> <p>Routinely reported health facility data are an important source for health indicators at the facility and population levels. Health facilities report on events such as immunizations given or live births attended by a skilled provider. As with any data, quality is an issue. The FASTR approach conducts an analysis of monthly data by facility and by indicator to assess data quality. Results are presented as annual estimates but may comprise a partial year of data given the availability of data at the time the analysis is conducted (e.g., an analysis conducted in June 2024 may contain data from January-May 2024, and this will be presented as the analysis for 2024).</p>"},{"location":"01_module_data_quality_assessment_documentation/#why-is-it-needed-in-the-fastr-pipeline","title":"Why is it needed in the FASTR pipeline?","text":"<p>Data quality directly impacts the reliability of health indicators and coverage estimates. Before calculating service utilization rates or estimating population coverage, we must ensure the underlying facility data is trustworthy. This module identifies problematic data patterns that could skew results, allowing analysts to make informed decisions about data adjustments or exclusions in subsequent pipeline steps.</p>"},{"location":"01_module_data_quality_assessment_documentation/#quick-summary","title":"Quick Summary","text":"Component Details Inputs - <code>hmis_[COUNTRY].csv</code> containing facility service volumes by month and indicator- Geographic/administrative area identifiers- Standardized indicator names Outputs - Outlier flags and lists- Completeness status by facility-indicator-month- Consistency results at geographic level- Overall DQA scores Purpose Evaluate HMIS data reliability through outlier detection, completeness assessment, and consistency checking to ensure trustworthy inputs for coverage estimation"},{"location":"01_module_data_quality_assessment_documentation/#2-how-it-works","title":"2. How It Works","text":""},{"location":"01_module_data_quality_assessment_documentation/#high-level-workflow","title":"High-Level Workflow","text":"<p>The module follows a logical sequence of quality checks, building from individual data points to an overall quality score:</p> <p>Step 1: Load and Prepare Data The module reads monthly facility reports and organizes them for analysis. It converts dates to a standard format and identifies which geographic areas and health indicators are present in the dataset.</p> <p>Step 2: Detect Outliers For each health facility and indicator (like pentavalent vaccine doses or antenatal care visits), the module identifies unusually high values that might indicate data entry errors. It uses two methods: statistical outliers (values far from the facility's typical volume) and proportional outliers (a single month accounting for most of the year's services).</p> <p>Step 3: Assess Completeness The module checks whether facilities are consistently reporting data. It creates a complete timeline for each facility and indicator, identifying months with missing reports. Facilities that stop reporting for 6+ months are flagged as inactive rather than incomplete.</p> <p>Step 4: Measure Consistency Related indicators should follow predictable patterns. For example, more women should receive their first antenatal visit (ANC1) than their fourth visit (ANC4). The module calculates ratios between paired indicators at the district level (to account for patients visiting multiple facilities) and flags relationships that don't meet expectations.</p> <p>Step 5: Validate Indicator Availability Before running consistency checks, the module verifies that the required indicator pairs actually exist in the dataset. Missing indicators are handled gracefully, with the analysis adapting to available data.</p> <p>Step 6: Calculate DQA Scores For a defined set of core indicators (typically pentavalent vaccine, first antenatal visit, and outpatient visits), the module combines the three quality dimensions. A facility-month receives a perfect DQA score only if all core indicators are complete, free of outliers, and meet consistency benchmarks.</p> <p>Step 7: Export Results The module produces several output files containing outlier lists, completeness flags, consistency results, and final DQA scores. These outputs inform subsequent modules and provide actionable insights for data quality improvement.</p>"},{"location":"01_module_data_quality_assessment_documentation/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    INPUT: Monthly Facility Data                 \u2502\n\u2502         (facility_id, period_id, indicator, count)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   Data Loading &amp; Preparation  \u2502\n         \u2502  \u2022 Convert dates              \u2502\n         \u2502  \u2022 Detect geography           \u2502\n         \u2502  \u2022 Create composite indicators\u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502    Outlier Detection          \u2502\n         \u2502  \u2022 Calculate median &amp; MAD     \u2502\n         \u2502  \u2022 Flag statistical outliers  \u2502\n         \u2502  \u2022 Flag proportional outliers \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   Completeness Assessment     \u2502\n         \u2502  \u2022 Generate full time series  \u2502\n         \u2502  \u2022 Identify missing reports   \u2502\n         \u2502  \u2022 Flag inactive facilities   \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   Consistency Analysis        \u2502\n         \u2502  \u2022 Aggregate to district level\u2502\n         \u2502  \u2022 Calculate indicator ratios \u2502\n         \u2502  \u2022 Apply benchmarks           \u2502\n         \u2502  \u2022 Assign to facilities       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502      DQA Scoring              \u2502\n         \u2502  \u2022 Filter core indicators     \u2502\n         \u2502  \u2022 Combine quality dimensions \u2502\n         \u2502  \u2022 Calculate composite score  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         OUTPUTS                                \u2502\n\u2502  \u2022 Outlier flags &amp; lists                                       \u2502\n\u2502  \u2022 Completeness status                                         \u2502\n\u2502  \u2022 Consistency results                                         \u2502\n\u2502  \u2022 DQA scores                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#key-decision-points","title":"Key Decision Points","text":"<p>When is a value considered an outlier? An observation is flagged as an outlier if it meets EITHER of two criteria AND the count exceeds 100: - It's more than 10 times the Median Absolute Deviation (MAD) away from the facility's typical volume, OR - It represents more than 80% of that facility's annual total for that indicator</p> <p>Why measure consistency at the district level instead of facility level? Patients often visit different facilities within their local district for different services. A woman might get her first antenatal visit at one health center but deliver at a district hospital. Measuring consistency at the district level accounts for this patient movement and provides a more accurate picture of service utilization patterns.</p> <p>What happens when required indicators are missing? The module adapts to available data. If consistency pairs cannot be evaluated (e.g., ANC4 data doesn't exist), the DQA score is calculated using only completeness and outlier checks. The analysis continues with the dimensions that can be assessed.</p> <p>How are inactive facilities handled? If a facility doesn't report for 6 or more consecutive months at the start or end of their reporting period, those months are flagged as \"inactive\" rather than \"incomplete.\" This prevents penalizing facilities that haven't yet started reporting or have permanently closed.</p>"},{"location":"01_module_data_quality_assessment_documentation/#what-happens-to-the-data","title":"What Happens to the Data","text":"<p>Transformation Overview:</p> <p>The module transforms raw facility reports into quality-flagged datasets:</p> <ol> <li>Input Format: Monthly rows with facility ID, period, indicator name, and count</li> <li>Enrichment: Adds calculated fields like median volume, MAD residuals, proportional contributions</li> <li>Completion: Generates explicit rows for missing months (turning implicit gaps into explicit records)</li> <li>Aggregation: Rolls up facility data to district level for consistency calculations</li> <li>Flagging: Adds binary quality flags (outlier yes/no, complete yes/no, consistent yes/no)</li> <li>Scoring: Combines flags into continuous scores (0-1) and binary pass/fail indicators</li> <li>Output Format: Multiple files optimized for different use cases (quick outlier review, full analysis, downstream modules)</li> </ol> <p>The data flows through the module in long format (one row per facility-indicator-period combination) and emerges with quality dimensions that subsequent modules use to weight, adjust, or exclude observations.</p>"},{"location":"01_module_data_quality_assessment_documentation/#3-detailed-reference","title":"3. Detailed Reference","text":"<p>This section provides technical details for implementers, developers, and analysts who need to understand the underlying methodology.</p>"},{"location":"01_module_data_quality_assessment_documentation/#configuration-parameters","title":"Configuration Parameters","text":"<p>The module uses several configurable parameters that control analysis behavior:</p> Geographic Settings <pre><code># Country identifier\nCOUNTRY_ISO3 &lt;- \"GIN\"  # ISO3 country code\n\n# Geographic level for consistency analysis\nGEOLEVEL &lt;- \"admin_area_3\"  # Admin level (1=national, 2=region, 3=district, etc.)\n</code></pre>  The `GEOLEVEL` parameter determines the aggregation level for consistency analysis. Lower administrative levels (3-4) capture local patterns but may have sparse data. Higher levels (2) provide more stable estimates but may mask local inconsistencies.   Outlier Detection Parameters <pre><code># Proportion threshold for outlier detection\nOUTLIER_PROPORTION_THRESHOLD &lt;- 0.8  # Flag if single month &gt; 80% of annual total\n\n# Minimum count to consider for outlier flagging\nMINIMUM_COUNT_THRESHOLD &lt;- 100  # Only flag outliers with count &gt;= 100\n\n# Number of Median Absolute Deviations for statistical outlier detection\nMADS &lt;- 10  # Flag if value &gt; 10 MADs from median\n</code></pre>  **Tuning Guidance:** - **More sensitive detection**: Lower `OUTLIER_PROPORTION_THRESHOLD` to 0.6-0.7, reduce `MADS` to 8 - **Less sensitive detection**: Increase `OUTLIER_PROPORTION_THRESHOLD` to 0.9, increase `MADS` to 12-15 - **Small facilities**: Lower `MINIMUM_COUNT_THRESHOLD` to 50 - **Large facilities only**: Increase `MINIMUM_COUNT_THRESHOLD` to 200+   DQA Indicator Selection <pre><code># Core indicators used for DQA scoring\nDQA_INDICATORS &lt;- c(\"penta1\", \"anc1\", \"opd\")\n\n# Consistency pairs to evaluate\nCONSISTENCY_PAIRS_USED &lt;- c(\"penta\", \"anc\")\n</code></pre>  **Standard Indicator Sets:** - **Maternal-child focus**: `c(\"anc1\", \"anc4\", \"delivery\", \"penta1\", \"penta3\")` - **Immunization focus**: `c(\"bcg\", \"penta1\", \"penta3\", \"measles1\")` - **Comprehensive**: `c(\"penta1\", \"anc1\", \"opd\", \"delivery\", \"pnc1\")`   Consistency Benchmark Ranges <pre><code>all_consistency_ranges &lt;- list(\n  pair_penta    = c(lower = 0.95, upper = Inf),  # Penta1 &gt;= 0.95 * Penta3\n  pair_anc      = c(lower = 0.95, upper = Inf),  # ANC1 &gt;= 0.95 * ANC4\n  pair_delivery = c(lower = 0.7, upper = 1.3),   # 0.7 &lt;= BCG/Delivery &lt;= 1.3\n  pair_malaria  = c(lower = 0.9, upper = 1.1)    # Malaria indicators within 10%\n)\n</code></pre>  The ranges reflect programmatic expectations. For example, ANC1 should always be at least 95% of ANC4 (more women start care than complete four visits). The 5% tolerance accounts for data entry variations. BCG, as a birth dose vaccine, should approximately equal facility deliveries, with 30% tolerance for variation."},{"location":"01_module_data_quality_assessment_documentation/#inputoutput-specifications","title":"Input/Output Specifications","text":""},{"location":"01_module_data_quality_assessment_documentation/#input-file-structure","title":"Input File Structure","text":"<p>Required File: <code>hmis_[COUNTRY_ISO3].csv</code></p> <p>Required Columns: - <code>facility_id</code> (character/integer): Unique identifier for each health facility - <code>period_id</code> (integer): Time period in YYYYMM format (e.g., 202401 for January 2024) - <code>indicator_common_id</code> (character): Standardized indicator names (e.g., \"penta1\", \"anc1\", \"opd\") - <code>count</code> (numeric): Service volume or count for the indicator - <code>admin_area_1</code> through <code>admin_area_8</code> (character): Geographic/administrative area columns</p> <p>Format Example: <pre><code>facility_id,period_id,indicator_common_id,count,admin_area_1,admin_area_2,admin_area_3\nFAC001,202401,penta1,45,Guinea,Conakry,Ratoma\nFAC001,202401,anc1,67,Guinea,Conakry,Ratoma\nFAC001,202402,penta1,52,Guinea,Conakry,Ratoma\n</code></pre></p> <p>Data Requirements: - At least 12 months of data recommended for robust outlier detection - Missing values represented as NA or absent rows (both handled) - Zero counts should be explicit zeros, not missing - Geographic columns detected automatically (columns 2-8 are optional)</p>"},{"location":"01_module_data_quality_assessment_documentation/#output-files","title":"Output Files","text":"M1_output_outlier_list.csv - Flagged Outliers Only  **Purpose**: Quick reference list of only the observations flagged as outliers  **Columns:** - `facility_id`: Facility identifier - `admin_area_[2-8]`: Geographic areas (dynamically included based on data) - `indicator_common_id`: Health indicator name - `period_id`: Time period (YYYYMM) - `year`, `month`: Extracted time components - `count`: Reported service volume - `median_volume`: Facility-indicator median - `mad_volume`: Median Absolute Deviation - `mad_residual`: Standardized residual - `pc`: Proportion of annual total - `outlier_mad`: MAD-based outlier flag - `outlier_pc`: Proportion-based outlier flag - `outlier_flag`: Final combined outlier flag  **Use Case**: Data managers reviewing specific outliers for investigation or correction   M1_output_outliers.csv - All Records with Outlier Flags  **Purpose**: Complete dataset with outlier metrics for all facility-indicator-period combinations  **Columns**: Same as outlier_list.csv but includes all observations  **Use Case**: - Input for Module 2 (Data Quality Adjustments) - Statistical analysis of outlier patterns - Generating visualizations of outlier prevalence   M1_output_completeness.csv - Completeness Status  **Purpose**: Completeness flags for all facility-indicator-period combinations, including explicitly created records for missing months  **Columns:** - `facility_id`: Facility identifier - `admin_area_[2-8]`: Geographic areas - `indicator_common_id`: Health indicator name - `period_id`: Time period (YYYYMM) - `year`, `month`, `quarter_id`: Time components - `count`: Reported volume (NA if missing) - `completeness_flag`: 0=Incomplete, 1=Complete, 2=Inactive (removed from output)  **Special Features**: - Contains explicit rows for non-reporting months - Inactive periods (6+ months at start/end) excluded - Full time series for each facility-indicator combination  **Use Case**: - Calculating completeness percentages - Identifying reporting gaps - Trend analysis of reporting behavior   M1_output_consistency_geo.csv - Geographic-Level Consistency  **Purpose**: Consistency ratios calculated at the specified geographic level (e.g., district)  **Columns:** - `admin_area_1` through `admin_area_[X]`: Geographic identifiers up to specified GEOLEVEL - `period_id`: Time period (YYYYMM) - `ratio_type`: Name of consistency pair (e.g., \"pair_penta\", \"pair_anc\") - `consistency_ratio`: Calculated ratio value - `sconsistency`: Binary flag (1=consistent, 0=inconsistent, NA=cannot calculate)  **Format**: Long format with one row per geographic area-period-ratio type  **Use Case**: - Understanding district-level service delivery patterns - Identifying geographic areas with consistency issues - Creating consistency heatmaps by zone   M1_output_consistency_facility.csv - Facility-Level Consistency  **Purpose**: Geographic consistency results expanded to facility level, pivoted to wide format  **Columns:** - `facility_id`: Facility identifier - `period_id`: Time period - `pair_[X]`: One column per consistency pair with flag values  **Format**: Wide format with one row per facility-period  **Use Case**: - Input for DQA scoring - Merging consistency flags with facility-level analyses - Facility-specific quality reports   M1_output_dqa.csv - Final DQA Scores  **Purpose**: Composite data quality scores by facility and time period  **Columns:** - `facility_id`: Facility identifier - `admin_area_[2-8]`: Geographic areas - `period_id`: Time period (YYYYMM) - `completeness_outlier_score`: Proportion of DQA indicators passing completeness &amp; outlier checks (0-1) - `consistency_score`: Proportion of consistency pairs passing benchmarks (0-1, or NA if no pairs) - `dqa_mean`: Average of component scores (0-1) - `dqa_score`: Binary overall pass/fail (1 = all checks passed, 0 = any check failed)  **Use Case**: - Filtering data for subsequent modules (e.g., only use facility-months with dqa_score=1) - Tracking data quality trends over time - Identifying facilities needing data quality improvement support"},{"location":"01_module_data_quality_assessment_documentation/#key-functions-documentation","title":"Key Functions Documentation","text":"load_and_preprocess_data()  **Signature**: `load_and_preprocess_data(file_path)`  **Purpose**: Loads HMIS data and prepares it for analysis by creating necessary date fields and composite indicators  **Parameters:** - `file_path` (character): Path to HMIS CSV file  **Returns**: List containing: - `data`: Preprocessed dataframe with date field added - `geo_cols`: Vector of detected geographic column names  **Process:** 1. Reads CSV file with HMIS data 2. Converts `period_id` (YYYYMM format) to Date objects for temporal ordering 3. Detects all administrative area columns (admin_area_1 through admin_area_8) 4. Creates composite malaria indicator if component indicators exist:    - Combines `rdt_positive` + `micro_positive` into `rdt_positive_plus_micro`    - This composite is used for malaria consistency checks  **Example:** <pre><code>inputs &lt;- load_and_preprocess_data(\"hmis_GIN.csv\")\ndata &lt;- inputs$data\ngeo_cols &lt;- inputs$geo_cols\n</code></pre> validate_consistency_pairs()  **Signature**: `validate_consistency_pairs(consistency_params, data)`  **Purpose**: Validates that required indicator pairs exist in the dataset before running consistency analysis  **Parameters:** - `consistency_params`: List containing consistency_pairs and consistency_ranges - `data`: The HMIS dataset  **Returns**: Updated consistency_params with only valid pairs (empty list if no valid pairs)  **Process:** 1. Checks which indicators are available in the dataset 2. Removes consistency pairs where one or both indicators are missing 3. Issues warnings about removed pairs 4. Returns empty list if no valid pairs remain  **Example Output:** <pre><code>Warning: Skipping pair_delivery - indicator 'delivery' not found in data\nWarning: Skipping pair_malaria - indicator 'rdt_positive_plus_micro' not found in data\nRemaining consistency pairs: pair_penta, pair_anc\n</code></pre> outlier_analysis()  **Signature**: `outlier_analysis(data, geo_cols, outlier_params)`  **Purpose**: Identifies statistical outliers in facility service volumes using dual detection methods  **Parameters:** - `data`: HMIS data with facility_id, indicator_common_id, period_id, count - `geo_cols`: Vector of geographic column names - `outlier_params`: List containing:   - `outlier_pc_threshold`: Proportion threshold (default 0.8)   - `count_threshold`: Minimum count threshold (default 100)  **Returns**: Dataframe with outlier flags and diagnostic metrics for each facility-indicator-period  **Calculated Fields:** - `median_volume`: Median count by facility-indicator - `mad_volume`: MAD calculated on values &gt;= median - `mad_residual`: Standardized residual (|count - median| / MAD) - `outlier_mad`: Binary flag (1 if mad_residual &gt; MADS) - `pc`: Proportional contribution to annual total - `outlier_pc`: Binary flag (1 if pc &gt; threshold) - `outlier_flag`: Final flag (1 if either method flags AND count &gt; minimum threshold)  **Algorithm Steps:**  **Step 1**: Calculate median volume for each facility-indicator combination  **Step 2**: Compute MAD using only values equal to or above the median - Avoids bias from facilities with many low-volume months - Standardizes residuals by dividing (count - median) by MAD - Flags outlier_mad = 1 if mad_residual &gt; MADS parameter  **Step 3**: Calculate proportional contribution - For each facility-indicator-year, sum total annual count - Calculate pc = count / annual_total - Flags outlier_pc = 1 if pc &gt; OUTLIER_PROPORTION_THRESHOLD  **Step 4**: Combine flags - Final outlier_flag = 1 if (outlier_mad = 1 OR outlier_pc = 1) AND count &gt; MINIMUM_COUNT_THRESHOLD - This ensures only substantial volumes are flagged   process_completeness()  **Signature**: `process_completeness(outlier_data_main)`  **Purpose**: Main orchestration function that generates complete time series and assigns completeness flags for all indicators  **Parameters:** - `outlier_data_main`: Outlier analysis results (contains all facility-indicator-period combinations with counts)  **Returns**: Long format dataset with completeness flags for all facility-indicator-period combinations  **Process:** 1. Identifies first and last reporting period for each indicator globally 2. Calls `generate_full_series_per_indicator()` for each indicator 3. Applies completeness tagging logic (complete/incomplete/inactive) 4. Merges with geographic metadata 5. Combines results across all indicators 6. Removes inactive periods (completeness_flag = 2)  **Output Structure:** - Explicit rows for both reported and non-reported periods - Completeness flag: 0 (incomplete), 1 (complete), 2 (inactive - removed) - Full time series from first to last reporting period per indicator   generate_full_series_per_indicator()  **Signature**: `generate_full_series_per_indicator(outlier_data, indicator_id, timeframe)`  **Purpose**: Creates a complete monthly time series for a specific indicator, filling in gaps where facilities did not report  **Parameters:** - `outlier_data`: data.table with outlier results - `indicator_id`: Specific indicator to process (e.g., \"penta1\") - `timeframe`: Data table with first_pid and last_pid for each indicator  **Returns**: Complete time series with explicit rows for both reported and non-reported periods  **Process:** 1. Subsets data to specific indicator 2. Generates monthly sequence from first to last period_id for that indicator 3. Creates complete facility-period grid (all facilities \u00d7 all months) using `CJ()` cross join 4. Merges with actual reported data 5. Missing counts indicate non-reporting periods 6. Applies inactive detection algorithm  **Inactive Detection Algorithm:** <pre><code># A facility is flagged inactive (offline_flag = 2) if:\n# 1. Missing 6+ consecutive months BEFORE first report, OR\n# 2. Missing 6+ consecutive months AFTER last report\n\noffline_flag := fifelse(\n  (missing_group == 1 &amp; missing_count &gt;= 6 &amp; !first_report_idx) |\n  (missing_group == max(missing_group) &amp; missing_count &gt;= 6 &amp; !last_report_idx),\n  2L, 0L\n)\n</code></pre>  **Example Timeline:** <pre><code>Facility A reporting pattern for indicator \"penta1\":\nPeriod:  202001 202002 202003 202004 202005 202006 202007 202008 202009 202010\nCount:   NA     NA     NA     NA     50     30     NA     NA     40     35\nFlag:    2      2      2      2      1      1      0      0      1      1\n         [----Inactive----] [---Active period with gaps---]\n\nExplanation:\n- First 4 months: Inactive (6+ months missing before first report at 202005)\n- 202005-202006: Complete (reported)\n- 202007-202008: Incomplete (gaps in active period)\n- 202009-202010: Complete (reported)\n</code></pre> geo_consistency_analysis()  **Signature**: `geo_consistency_analysis(data, geo_cols, geo_level, consistency_params)`  **Purpose**: Calculates consistency ratios at the geographic level to account for patients seeking services across multiple facilities within a district/ward  **Parameters:** - `data`: Outlier data (with outliers already flagged) - `geo_cols`: Vector of geographic column names - `geo_level`: Geographic level for aggregation (e.g., \"admin_area_3\") - `consistency_params`: List with consistency_pairs and consistency_ranges  **Returns**: Long format dataframe with geographic-level consistency results  **Process:** 1. Excludes outliers (sets count to NA where outlier_flag = 1) 2. Aggregates data to specified geographic level by period (sums across facilities) 3. Reshapes to wide format (one column per indicator) 4. Calculates ratio for each indicator pair 5. Flags consistency based on predefined ranges  **Output Columns:** - Geographic identifiers (up to specified level) - `period_id`: Time period - `ratio_type`: Name of the consistency pair (e.g., \"pair_penta\") - `consistency_ratio`: Calculated ratio value - `sconsistency`: Binary flag (1 = consistent, 0 = inconsistent, NA = cannot calculate)  **Example Output:** <pre><code>admin_area_2  admin_area_3  period_id  ratio_type    consistency_ratio  sconsistency\nDistrict_A    Ward_1        202401     pair_penta    1.05               1\nDistrict_A    Ward_1        202401     pair_anc      0.88               0\nDistrict_A    Ward_2        202401     pair_penta    0.97               1\n</code></pre>  **Rationale**: Measuring consistency at the geographic level accounts for patient movement between facilities and provides a more accurate picture of service utilization patterns across a community.   expand_geo_consistency_to_facilities()  **Signature**: `expand_geo_consistency_to_facilities(facility_metadata, geo_consistency_results, geo_level)`  **Purpose**: Assigns geographic-level consistency results to individual facilities  **Parameters:** - `facility_metadata`: Facility list with geographic assignments - `geo_consistency_results`: Output from geo_consistency_analysis() - `geo_level`: Geographic level used in consistency analysis  **Returns**: Facility-level dataset with consistency flags  **Process:** - Extracts facility list with their geographic assignments - Performs left join to replicate geo-level consistency scores to all facilities in that area - Uses many-to-many relationship to handle multiple periods and ratio types  **Rationale**: Since consistency is measured at the geographic level (accounting for patient movement between facilities), all facilities within the same district/ward receive the same consistency scores.   dqa_with_consistency()  **Signature**: `dqa_with_consistency(completeness_data, consistency_data, outlier_data, geo_cols, dqa_rules)`  **Purpose**: Calculates comprehensive DQA scores including consistency checks when consistency pairs are available  **Parameters:** - `completeness_data`: Output from process_completeness() - `consistency_data`: Wide-format facility consistency results - `outlier_data`: Output from outlier_analysis() - `geo_cols`: Vector of geographic column names - `dqa_rules`: List specifying required values for each dimension  **DQA Rules Configuration:** <pre><code>dqa_rules &lt;- list(\n  completeness = 1,   # Must be complete (flag = 1)\n  outlier_flag = 0,   # Must NOT be an outlier (flag = 0)\n  sconsistency = 1    # Must be consistent (flag = 1)\n)\n</code></pre>  **Scoring Algorithm:**  **1. Completeness-Outlier Score** (per facility-period): - Each DQA indicator scores 0-2 points (1 for completeness + 1 for no outlier) - Maximum possible = 2 \u00d7 number of DQA indicators - Score = Total Points / Maximum Points  **2. Consistency Score** (per facility-period): - Only counts pairs where both indicators exist (NA pairs excluded from denominator) - Score = Number of passing pairs / Number of available pairs - If no pairs available, score = 0  **3. Mean DQA Score:** - Average of completeness-outlier score and consistency score - Formula: `(completeness_outlier_score + consistency_score) / 2`  **4. Binary DQA Score:** - 1 if ALL available consistency pairs pass AND completeness-outlier score is perfect - 0 otherwise  **Handling Missing Indicators:** The function intelligently handles cases where some consistency indicators are missing: - NA values in consistency pairs are NOT replaced with 0 - Only available pairs contribute to the denominator - This prevents penalizing facilities for indicators they don't provide  **Example Calculation:** <pre><code>Facility X in period 202401:\n- DQA Indicators: penta1, anc1, opd (3 indicators)\n- Completeness: All 3 complete \u2192 3 points\n- Outliers: None \u2192 3 points\n- Total: 6/6 \u2192 completeness_outlier_score = 1.0\n\nConsistency Pairs:\n- pair_penta (penta1/penta3): Pass (1)\n- pair_anc (anc1/anc4): Fail (0)\n- pair_delivery: NA (bcg not a DQA indicator)\n\nConsistency calculation:\n- Available pairs: 2 (penta, anc)\n- Passing pairs: 1 (penta)\n- consistency_score = 1/2 = 0.5\n\nFinal scores:\n- dqa_mean = (1.0 + 0.5) / 2 = 0.75\n- dqa_score = 0 (not all pairs passed)\n</code></pre> dqa_without_consistency()  **Signature**: `dqa_without_consistency(completeness_data, outlier_data, geo_cols, dqa_rules)`  **Purpose**: Calculates DQA scores using only completeness and outlier checks when consistency data is unavailable or no valid consistency pairs exist  **When Used:** - No consistency pairs defined in configuration - All consistency pairs have missing indicators - Dataset doesn't contain paired indicators  **Scoring:** - Uses only completeness and outlier components - `dqa_mean` = `completeness_outlier_score` - `dqa_score` = 1 if all completeness and outlier checks pass, 0 otherwise  **Output Structure:** <pre><code>dqa_results &lt;- data.frame(\n  facility_id,\n  admin_area_X,              # Dynamic geographic columns\n  period_id,\n  completeness_outlier_score, # Range: 0-1\n  dqa_mean,                   # Range: 0-1 (equals completeness_outlier_score)\n  dqa_score                   # Binary: 0 or 1\n)\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#statistical-methods-algorithms","title":"Statistical Methods &amp; Algorithms","text":"Median Absolute Deviation (MAD) Calculation  The MAD is a robust measure of variability that is less sensitive to outliers than standard deviation.  **Standard MAD Algorithm:** 1. Compute the median of the dataset 2. Calculate absolute deviations: |value - median| for each data point 3. Find the median of these absolute deviations  **FASTR Modification:** The module calculates MAD using only values at or above the median, making it more sensitive to high outliers while avoiding bias from facilities with many low-volume months.  **Outlier Degree Calculation:** $$ \\text{MAD Residual} = \\frac{|\\text{volume} - \\text{median volume}|}{\\text{MAD}} $$  **Outlier Classification:** - If MAD Residual &gt; 10 (configurable via `MADS` parameter), the value is flagged as a statistical outlier  **Example:** <pre><code>Facility ABC, Indicator: penta1\nMonthly counts: 20, 25, 22, 28, 24, 26, 150, 23, 27, 25, 21, 24\n\nStep 1: Calculate median = 24.5\nStep 2: Values &gt;= median: 25, 28, 24.5, 26, 150, 27, 25, 24.5\nStep 3: Absolute deviations from median: 0.5, 3.5, 0, 1.5, 125.5, 2.5, 0.5, 0\nStep 4: MAD = median(0, 0, 0.5, 0.5, 1.5, 2.5, 3.5, 125.5) = 1.0\nStep 5: For count=150: MAD residual = |150 - 24.5| / 1.0 = 125.5\nStep 6: 125.5 &gt; 10, therefore 150 is flagged as an outlier\n</code></pre> Proportional Outlier Detection  This method identifies months where a single observation represents an unusually large proportion of the annual total for a facility-indicator combination.  **Algorithm:** 1. For each facility-indicator-year, sum the total annual count 2. Calculate the proportion: `pc = monthly_count / annual_total` 3. Flag as outlier if `pc &gt; OUTLIER_PROPORTION_THRESHOLD` (default 0.8)  **Rationale:** A facility reporting 80% of its annual volume in a single month likely indicates a data entry error (e.g., cumulative reporting instead of monthly, extra digit entered).  **Example:** <pre><code>Facility XYZ, Indicator: anc1, Year: 2024\nMonthly counts: 15, 18, 12, 16, 890, 14, 17, 13, 16, 15, 14, 12\nAnnual total: 1052\n\nFor May (count=890):\npc = 890 / 1052 = 0.846\n0.846 &gt; 0.8, therefore May is flagged as a proportional outlier\n</code></pre> Consistency Ratio Benchmarks  The module applies programmatically defined benchmarks for indicator pairs:  **ANC Consistency:** $$ \\text{ANC Consistency} = \\begin{cases} 1, &amp; \\frac{\\text{ANC1 Volume}}{\\text{ANC4 Volume}} \\geq 0.95 \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$  **Interpretation**: More women should start antenatal care (ANC1) than complete four visits (ANC4). The ratio is expected to be \u2265 0.95, allowing up to 5% tolerance for data variations.  **Penta Consistency:** $$ \\text{Penta Consistency} = \\begin{cases} 1, &amp; \\frac{\\text{Penta1 Volume}}{\\text{Penta3 Volume}} \\geq 0.95 \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$  **Interpretation**: More children should receive the first pentavalent dose (Penta1) than complete the three-dose series (Penta3).  **BCG/Delivery Consistency:** $$ \\text{BCG/Delivery Consistency} = \\begin{cases} 1, &amp; 0.7 \\leq \\frac{\\text{BCG Volume}}{\\text{Delivery Volume}} \\leq 1.3 \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$  **Interpretation**: BCG is a birth dose vaccine, so BCG vaccinations should approximately equal facility deliveries. The wider range (\u00b130%) accounts for infants born elsewhere receiving BCG at the facility or facility-born infants receiving BCG elsewhere.  **Implementation Detail:** Consistency is assessed at the district/ward level (specified by `GEOLEVEL`) to account for patients visiting multiple facilities within their local area for different services.   Completeness Calculation  For a given indicator in a given month:  $$ \\text{Completeness} = \\frac{\\text{Number of reporting facilities}}{\\text{Number of expected facilities}} \\times 100 $$  **Expected Facilities Definition:** A facility is expected to report for an indicator if it has ever reported for that indicator within the analysis timeframe AND is not flagged as inactive.  **Inactive Facility Definition:** A facility is flagged as inactive for periods where it did not report for six or more consecutive months before its first report or after its last report.  **Example:** <pre><code>District has 20 facilities that have ever reported penta1 data in 2024\nIn March 2024:\n- 18 facilities submitted penta1 data\n- 2 facilities did not submit (but are not inactive)\n\nCompleteness = 18 / 20 \u00d7 100 = 90%\n</code></pre>  **Important Note**: A high level of completeness does not necessarily indicate that the HMIS is representative of all service delivery in the country, as some services may not be delivered in facilities or some facilities may not report. For countries where DHIS2 does not store zeros, indicator completeness may be underestimated if there are many low-volume facilities.   DQA Composite Score Calculation  The DQA score combines three quality dimensions for a defined set of core indicators.  **Component Scores:**  **1. Completeness-Outlier Score:** $$ \\text{Completeness-Outlier Score} = \\frac{\\sum (\\text{completeness pass} + \\text{outlier pass})}{2 \\times \\text{number of DQA indicators}} $$  **2. Consistency Score:** $$ \\text{Consistency Score} = \\frac{\\text{Number of pairs passing benchmarks}}{\\text{Number of available pairs}} $$  **3. Mean DQA Score:** $$ \\text{DQA Mean} = \\frac{\\text{Completeness-Outlier Score} + \\text{Consistency Score}}{2} $$  **4. Binary DQA Score:** $$ \\text{DQA Score} = \\begin{cases} 1, &amp; \\text{if all checks passed} \\\\ 0, &amp; \\text{otherwise} \\end{cases} $$  **Passing Criteria for Binary Score:** - ALL DQA indicators must be complete (completeness_flag = 1) - ALL DQA indicators must be free of outliers (outlier_flag = 0) - ALL available consistency pairs must pass benchmarks (sconsistency = 1)  **Example Calculation:** <pre><code>Facility 123, Period 202403\nDQA Indicators: penta1, anc1, opd\n\nCompleteness: penta1=1, anc1=1, opd=1 \u2192 3 points\nOutliers: penta1=0, anc1=0, opd=0 \u2192 3 points\nCompleteness-Outlier Score = 6 / (2\u00d73) = 1.0\n\nConsistency Pairs:\n- pair_penta: 1 (pass)\n- pair_anc: 1 (pass)\nConsistency Score = 2 / 2 = 1.0\n\nDQA Mean = (1.0 + 1.0) / 2 = 1.0\nDQA Score = 1 (all checks passed)\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#code-examples","title":"Code Examples","text":"Example 1: Running the Module with Default Settings <pre><code># Set working directory\nsetwd(\"/path/to/module/directory\")\n\n# Load required libraries\nlibrary(zoo)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(data.table)\n\n# The module will automatically:\n# 1. Load hmis_GIN.csv\n# 2. Run all analyses with default parameters\n# 3. Generate output CSV files in the working directory\n\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre> Example 2: Adjusting Outlier Detection Sensitivity <pre><code># Make outlier detection more sensitive (lower thresholds)\nOUTLIER_PROPORTION_THRESHOLD &lt;- 0.6   # Flag if &gt;60% of annual volume (was 80%)\nMINIMUM_COUNT_THRESHOLD &lt;- 50         # Consider counts &gt;=50 (was 100)\nMADS &lt;- 8                             # Flag at 8 MADs (was 10)\n\n# Run the module\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre>  **Use Case**: Countries with generally low service volumes where the default thresholds are too conservative.   Example 3: Different Geographic Level for Consistency <pre><code># Use district level (admin_area_2) instead of sub-district (admin_area_3)\nGEOLEVEL &lt;- \"admin_area_2\"\n\n# This affects consistency analysis aggregation level\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre>  **Use Case**: Sub-district level has sparse data or too few facilities per area, making district-level aggregation more stable.   Example 4: Custom DQA Indicators <pre><code># Focus DQA on maternal health indicators only\nDQA_INDICATORS &lt;- c(\"anc1\", \"anc4\", \"delivery\", \"pnc1\")\n\n# Only evaluate anc consistency pair\nCONSISTENCY_PAIRS_USED &lt;- c(\"anc\")\n\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre>  **Use Case**: Specialized analysis focusing on a specific service area.   Example 5: Running for Different Country <pre><code># Configure for Rwanda\nCOUNTRY_ISO3 &lt;- \"RWA\"\nPROJECT_DATA_HMIS &lt;- \"hmis_RWA.csv\"\nGEOLEVEL &lt;- \"admin_area_3\"\n\n# Adjust for Rwanda-specific indicators if needed\nDQA_INDICATORS &lt;- c(\"penta1\", \"anc1\", \"opd\", \"fp_new\")\n\nsource(\"01_module_data_quality_assessment.R\")\n</code></pre> Example 6: Programmatic Use of Outputs <pre><code># After running the module, work with outputs\n\n# Load DQA results\ndqa_results &lt;- read.csv(\"M1_output_dqa.csv\")\n\n# Filter to high-quality facility-months only\nhigh_quality &lt;- dqa_results %&gt;%\n  filter(dqa_score == 1)\n\n# Calculate percentage of facility-months passing DQA by district\nquality_by_district &lt;- dqa_results %&gt;%\n  group_by(admin_area_2, period_id) %&gt;%\n  summarize(\n    total_facility_months = n(),\n    passing_quality = sum(dqa_score == 1),\n    pct_passing = 100 * passing_quality / total_facility_months\n  )\n\n# Identify facilities with consistently poor quality (never passing)\npoor_quality_facilities &lt;- dqa_results %&gt;%\n  group_by(facility_id) %&gt;%\n  summarize(\n    months_analyzed = n(),\n    months_passed = sum(dqa_score == 1),\n    pct_passed = 100 * months_passed / months_analyzed\n  ) %&gt;%\n  filter(pct_passed == 0)\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#troubleshooting","title":"Troubleshooting","text":"Problem: Module skips consistency analysis  **Symptoms:** - Console message: \"No valid consistency pairs found\" - M1_output_consistency_geo.csv has only headers - DQA scores calculated without consistency component  **Diagnosis:** Check that both indicators in each pair exist in your dataset: <pre><code># Load your data\ndata &lt;- read.csv(\"hmis_[COUNTRY].csv\")\n\n# Check available indicators\nprint(unique(data$indicator_common_id))\n\n# Compare with required pairs\n# For pair_penta: need \"penta1\" and \"penta3\"\n# For pair_anc: need \"anc1\" and \"anc4\"\n# For pair_delivery: need \"bcg\" and \"delivery\" (or \"sba\")\n</code></pre>  **Solutions:** 1. Adjust `CONSISTENCY_PAIRS_USED` to only include pairs with available indicators 2. Modify indicator names in your data to match expected names 3. Accept that DQA will be calculated without consistency component   Problem: All facilities flagged as outliers  **Symptoms:** - Very high percentage of outlier_flag = 1 in M1_output_outliers.csv - Most observations in outlier_list.csv  **Diagnosis:** Your thresholds may be too sensitive for your data context.  **Solutions:** 1. Increase MAD threshold: <pre><code>MADS &lt;- 15  # Increase from default 10\n</code></pre>  2. Increase proportion threshold: <pre><code>OUTLIER_PROPORTION_THRESHOLD &lt;- 0.9  # Increase from 0.8\n</code></pre>  3. Increase minimum count threshold (focus on larger facilities): <pre><code>MINIMUM_COUNT_THRESHOLD &lt;- 200  # Increase from 100\n</code></pre>  4. Review the data: Check if there are genuine quality issues requiring data cleaning rather than parameter adjustment   Problem: No DQA results generated  **Symptoms:** - M1_output_dqa.csv is empty or has only headers - Console message: \"Skipping DQA analysis - none of the required indicators found\"  **Diagnosis:** None of the indicators specified in `DQA_INDICATORS` exist in your dataset.  **Solution:** Check which DQA indicators are missing: <pre><code># Load data\ndata &lt;- read.csv(\"hmis_[COUNTRY].csv\")\n\n# Check which DQA indicators are missing\navailable_indicators &lt;- unique(data$indicator_common_id)\nmissing_indicators &lt;- setdiff(DQA_INDICATORS, available_indicators)\nprint(paste(\"Missing DQA indicators:\", paste(missing_indicators, collapse=\", \")))\n\n# Available DQA indicators\navailable_dqa &lt;- intersect(DQA_INDICATORS, available_indicators)\nprint(paste(\"Available DQA indicators:\", paste(available_dqa, collapse=\", \")))\n</code></pre>  Then update `DQA_INDICATORS` to include only available indicators: <pre><code>DQA_INDICATORS &lt;- c(\"penta1\", \"anc1\")  # Only use what's available\n</code></pre> Problem: Consistency ratios seem incorrect  **Symptoms:** - All consistency flags are 0 (inconsistent) - Consistency ratios are unexpectedly high or low  **Diagnosis:** The geographic aggregation level may be inappropriate for your data.  **Investigation:** <pre><code># Load geographic consistency results\ngeo_cons &lt;- read.csv(\"M1_output_consistency_geo.csv\")\n\n# Check distribution of consistency ratios\nsummary(geo_cons$consistency_ratio)\n\n# Check sample sizes at geographic level\noutliers &lt;- read.csv(\"M1_output_outliers.csv\")\ngeo_summary &lt;- outliers %&gt;%\n  group_by(admin_area_3, period_id) %&gt;%\n  summarize(\n    n_facilities = n_distinct(facility_id),\n    total_volume = sum(count, na.rm = TRUE)\n  )\nsummary(geo_summary$n_facilities)\n</code></pre>  **Solutions:** 1. If geographic areas have very few facilities (1-2), use higher level: <pre><code>GEOLEVEL &lt;- \"admin_area_2\"  # Use district instead of sub-district\n</code></pre>  2. If ratios are generally below 0.95 for ANC/Penta pairs, this may indicate genuine programmatic issues (high dropout) rather than data quality problems  3. Review the consistency benchmark ranges - they may need adjustment for your context: <pre><code># Example: Allow higher dropout (lower ratio) for Penta\nall_consistency_ranges$pair_penta &lt;- c(lower = 0.85, upper = Inf)\n</code></pre> Problem: Completeness percentages seem low  **Symptoms:** - High proportion of completeness_flag = 0 in M1_output_completeness.csv  **Diagnosis:** This could be legitimate (poor reporting) or an artifact of how your DHIS2 stores zero values.  **Investigation:** <pre><code># Load completeness data\ncompleteness &lt;- read.csv(\"M1_output_completeness.csv\")\n\n# Check pattern: Are there explicit zeros or just missing values?\noutliers &lt;- read.csv(\"M1_output_outliers.csv\")\ntable(is.na(outliers$count), outliers$count == 0)\n\n# Check completeness by indicator\ncomp_by_indicator &lt;- completeness %&gt;%\n  group_by(indicator_common_id) %&gt;%\n  summarize(\n    pct_complete = 100 * mean(completeness_flag == 1),\n    pct_incomplete = 100 * mean(completeness_flag == 0)\n  )\nprint(comp_by_indicator)\n</code></pre>  **Considerations:** 1. If your DHIS2 doesn't store zeros, low-volume facilities may appear incomplete when they legitimately had no services to report 2. Completeness percentages should be interpreted in context - 70% completeness may be acceptable depending on the health system 3. Use the completeness_flag in subsequent modules to weight estimates appropriately   Problem: Error reading input file  **Symptoms:** - Error: \"Cannot open file 'hmis_[COUNTRY].csv'\" - Module crashes during data loading  **Solutions:** 1. Check file path and working directory: <pre><code>getwd()  # Verify working directory\nlist.files()  # Check if HMIS file is present\n</code></pre>  2. Verify file name matches `PROJECT_DATA_HMIS` parameter  3. Check file format (CSV, proper encoding, comma-separated)  4. Ensure required columns exist: <pre><code># After loading\nnames(data)  # Should include: facility_id, period_id, indicator_common_id, count\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#usage-notes","title":"Usage Notes","text":"Data Type Handling  **period_id Flexibility:** The module accepts `period_id` in multiple formats: - Integer: `202401` - String: `\"202401\"` - Numeric: `202401.0`  All formats are internally converted to Date objects for correct chronological ordering: <pre><code># Internal conversion\nas.Date(sprintf(\"%04d-%02d-01\", year, month))\n</code></pre>  This ensures proper temporal ordering even with gaps in reporting periods.  **Count Values:** - Numeric values required (integers or decimals) - Zero counts should be explicit `0`, not `NA` - Missing counts represented as `NA` or absent rows  **Geographic Columns:** - Character type recommended - Can contain spaces and special characters - Case-sensitive in some operations   Missing Value Strategy  The module uses context-specific approaches to missing values:  **Outlier Analysis:** - NA values excluded from median/MAD calculations - Only non-NA values contribute to statistics - Prevents bias from sparse reporting  **Completeness:** - Explicit NA in count column indicates non-reporting - Assigned completeness_flag = 0 (incomplete) - Distinguished from inactive periods (flag = 2, removed)  **Consistency:** - NA ratios (from division by zero) kept as NA, not converted to 0 - NA pairs excluded from consistency scoring denominator - Prevents penalizing facilities for unavailable indicators  **DQA Scoring:** - NA consistency pairs excluded from denominator - Only available pairs affect consistency score - Allows partial scoring when some indicators missing   Memory Considerations  For large datasets (&gt;1 million rows), the module implements several optimizations:  **data.table Usage:** - Completeness processing uses `data.table` for in-place operations - Significantly faster and more memory-efficient than `dplyr` for large data  **Filtering Strategy:** - Filters to relevant indicators before expensive operations - Reduces memory footprint during calculations  **Object Management:** - Removes intermediate objects after use - Prevents memory accumulation during sequential processing  **Recommendations for Large Datasets:** - Allocate at least 8GB RAM for countries with &gt;1000 facilities - Consider processing by year if multi-year datasets cause memory issues - Monitor memory usage: `pryr::mem_used()` at various stages   Performance Optimization Opportunities  **Current Implementation:** The completeness analysis processes indicators sequentially using `lapply()`.  **Potential Enhancement:** For datasets with many indicators, parallelization could improve performance:  <pre><code># Future enhancement (not in current code)\nlibrary(parallel)\n\n# Detect available cores\nn_cores &lt;- detectCores() - 1\n\n# Parallel processing of indicators\ncompleteness_list &lt;- mclapply(\n  unique(outlier_data_main$indicator_common_id),\n  function(ind) generate_full_series_per_indicator(\n    outlier_data = outlier_data_main,\n    indicator_id = ind,\n    timeframe = indicator_timeframe\n  ),\n  mc.cores = n_cores\n)\n\n# Combine results\ncompleteness_data &lt;- rbindlist(completeness_list)\n</code></pre>  **Expected Speedup:** - 3-4x faster with 4 cores on datasets with 10+ indicators - Most beneficial for countries with many indicators and long time series   Dynamic Indicator Selection  The module intelligently adapts to available data:  **Delivery Indicator Selection:** <pre><code># Automatically chooses between \"delivery\" and \"sba\" for BCG consistency pair\nif (\"delivery\" %in% available_indicators) {\n  PAIR_DELIVERY_B &lt;- \"delivery\"\n} else if (\"sba\" %in% available_indicators) {\n  PAIR_DELIVERY_B &lt;- \"sba\"  # Skilled birth attendant\n} else {\n  PAIR_DELIVERY_B &lt;- \"delivery\"  # Default fallback\n}\n</code></pre>  **DQA Indicator Validation:** <pre><code># Only use DQA indicators that exist in the dataset\ndqa_indicators_to_use &lt;- intersect(DQA_INDICATORS, unique(data$indicator_common_id))\n\n# If none found, skip DQA analysis with informative message\nif (length(dqa_indicators_to_use) == 0) {\n  print(\"Skipping DQA analysis - none of the required indicators found\")\n}\n</code></pre>  **Consistency Pair Validation:** The module checks each consistency pair and removes those with missing indicators, providing clear warnings about which pairs were skipped.   Error Handling and Fallbacks  The module includes robust error handling:  **Missing Consistency Pairs:** - If no valid pairs exist, skips consistency analysis - Uses `dqa_without_consistency()` for scoring - Outputs dummy files with proper headers  **Missing Geographic Levels:** - Falls back to lowest available admin level if specified `GEOLEVEL` not found - Issues warning about the fallback  **Empty Results:** - Creates CSV files with proper headers even when no data - Ensures downstream processes don't break  **Missing Indicators:** - Validates all indicator requirements before analysis - Warns about removed pairs - Continues with available indicators   Interpretation Guidelines  **Outlier Flags:** - outlier_flag = 1 suggests potential data quality issues, but require investigation - Not all flagged outliers are errors (genuine service campaigns can trigger flags) - Use mad_residual and pc values to prioritize review  **Completeness:** - Completeness % varies by health system context - 80-90%+ is generally good, but depends on country - Trend over time more informative than absolute percentage - Low completeness for specific indicators may reflect genuine service gaps  **Consistency:** - sconsistency = 0 may indicate data quality issues OR programmatic performance issues (e.g., high dropout) - Requires programmatic knowledge to interpret - Geographic patterns can help distinguish systematic issues from random errors  **DQA Scores:** - dqa_score = 1 indicates data passed all checks, suitable for unadjusted use - dqa_score = 0 requires further investigation - dqa_mean provides nuanced view (0.75 = mostly good, 0.25 = mostly poor)"},{"location":"01_module_data_quality_assessment_documentation/#data-quality-metrics-summary","title":"Data Quality Metrics Summary","text":"Metric Type Range Interpretation outlier_flag Binary 0 or 1 1 = Outlier detected by either method and count &gt; threshold outlier_mad Binary 0 or 1 1 = Statistical outlier (MAD-based) outlier_pc Binary 0 or 1 1 = Proportional outlier (&gt;80% of annual volume) mad_residual Continuous 0 to \u221e Standardized deviation from median (higher = more extreme) pc Continuous 0 to 1 Proportion of annual volume (closer to 1 = more concentrated) completeness_flag Categorical 0, 1, 2 0=Incomplete (missing), 1=Complete (reported), 2=Inactive (removed) sconsistency Binary 0, 1, NA 1=Consistent (passes benchmark), 0=Inconsistent, NA=Cannot calculate consistency_ratio Continuous 0 to \u221e Ratio of paired indicators (interpretation depends on pair) completeness_outlier_score Continuous 0 to 1 Proportion of DQA indicators passing completeness &amp; outlier checks consistency_score Continuous 0 to 1 Proportion of consistency pairs passing benchmarks dqa_mean Continuous 0 to 1 Average of component scores (overall quality measure) dqa_score Binary 0 or 1 1 = All checks passed (high quality), 0 = Any check failed"},{"location":"01_module_data_quality_assessment_documentation/#analysis-outputs-and-visualization","title":"Analysis Outputs and Visualization","text":"<p>The FASTR analysis generates six main visual outputs:</p> <p>1. Outliers Heatmap</p> <p>Heatmap table with zones as rows and health indicators as columns, color-coded by outlier percentage.</p> <p></p> <p>2. Indicator Completeness</p> <p>Heatmap table with zones as rows and health indicators as columns, color-coded by completeness percentage.</p> <p></p> <p>3. Indicator Completeness Over Time</p> <p>Horizontal timeline charts showing completeness trends for each indicator over the analysis period.</p> <p></p> <p>4. Internal Consistency</p> <p>Heatmap table with zones as rows and consistency benchmark categories as columns, color-coded by performance.</p> <p></p> <p>5. Overall DQA Score</p> <p>Heatmap table with zones as rows and time periods as columns, color-coded by DQA score percentage.</p> <p></p> <p>6. Mean DQA Score</p> <p>Heatmap table with zones as rows and time periods as columns, color-coded by average DQA score.</p> <p></p> <p>Color Coding System: - Green: 90% or above (completeness/consistency), Below 1% (outliers) - Yellow: 80% to 89% (completeness), 1% to 2% (outliers) - Red: Below 80% (completeness/consistency), 3% or above (outliers)</p>"},{"location":"01_module_data_quality_assessment_documentation/#execution-workflow","title":"Execution Workflow","text":"<p>The module follows this carefully orchestrated sequence:</p> <pre><code>1. DATA LOADING &amp; PREPROCESSING\n   \u251c\u2500 Load HMIS CSV file\n   \u251c\u2500 Convert period_id to dates\n   \u251c\u2500 Detect geographic columns\n   \u2514\u2500 Create composite malaria indicator (if applicable)\n\n2. CONFIGURATION &amp; VALIDATION\n   \u251c\u2500 Detect available indicators\n   \u251c\u2500 Dynamically select delivery indicator (delivery vs sba)\n   \u251c\u2500 Build consistency pairs based on available indicators\n   \u251c\u2500 Validate consistency pairs\n   \u2514\u2500 Filter DQA indicators to available ones\n\n3. OUTLIER ANALYSIS\n   \u251c\u2500 Calculate median and MAD by facility-indicator\n   \u251c\u2500 Flag MAD-based outliers (&gt;10 MADs)\n   \u251c\u2500 Flag proportion-based outliers (&gt;80% of annual volume)\n   \u2514\u2500 Combine flags (either method + count &gt; 100)\n\n4. COMPLETENESS ANALYSIS\n   \u251c\u2500 Identify reporting timeframe per indicator\n   \u251c\u2500 Generate full time series (all facilities \u00d7 all months)\n   \u251c\u2500 Tag reporting status (complete/incomplete/inactive)\n   \u2514\u2500 Remove inactive periods (6+ months before first/after last report)\n\n5. CONSISTENCY ANALYSIS (if applicable)\n   \u251c\u2500 Exclude outliers from data\n   \u251c\u2500 Aggregate to geographic level (e.g., district)\n   \u251c\u2500 Calculate ratios for indicator pairs\n   \u251c\u2500 Flag consistency based on predefined ranges\n   \u251c\u2500 Expand geo-level results to facilities\n   \u2514\u2500 Pivot to wide format (one column per pair)\n\n6. DQA SCORING\n   \u251c\u2500 Filter to DQA indicators only\n   \u251c\u2500 Merge completeness, outlier, and consistency results\n   \u251c\u2500 Calculate component scores:\n   \u2502  \u251c\u2500 Completeness-outlier score (0-1)\n   \u2502  \u2514\u2500 Consistency score (0-1, if applicable)\n   \u251c\u2500 Calculate mean DQA score\n   \u2514\u2500 Assign binary DQA pass/fail flag\n\n7. EXPORT RESULTS\n   \u251c\u2500 M1_output_outlier_list.csv (outliers only)\n   \u251c\u2500 M1_output_outliers.csv (all records with flags)\n   \u251c\u2500 M1_output_completeness.csv (completeness flags)\n   \u251c\u2500 M1_output_consistency_geo.csv (geo-level consistency)\n   \u251c\u2500 M1_output_consistency_facility.csv (facility-level consistency)\n   \u2514\u2500 M1_output_dqa.csv (final DQA scores)\n</code></pre>"},{"location":"01_module_data_quality_assessment_documentation/#version-history","title":"Version History","text":"<ul> <li>2025-11-08: Restructured documentation for user-friendliness; separated overview/workflow from technical reference</li> <li>2025-09-03: Updated methodology descriptions and statistical formulas</li> <li>Previous: Initial module development and documentation</li> </ul> <p>Module Author: CB - FASTR PROJECT</p> <p>Last Code Edit: 2025-11-08</p> <p>Last Documentation Edit: 2025-11-08</p>"},{"location":"02_module_data_quality_adjustments_documentation/","title":"Module 2: Data Quality Adjustment","text":""},{"location":"02_module_data_quality_adjustments_documentation/#1-overview-what-why","title":"1. Overview (What &amp; Why)","text":""},{"location":"02_module_data_quality_adjustments_documentation/#what-does-this-module-do","title":"What does this module do?","text":"<p>The Data Quality Adjustment module systematically corrects two common problems in routine health facility data: outliers (extreme values caused by reporting errors or data entry mistakes) and missing data (from incomplete reporting). Rather than simply deleting problematic data, this module replaces questionable values with statistically sound estimates based on each facility's own historical patterns.</p> <p>The module uses sophisticated temporal smoothing techniques that analyze trends over time. By calculating rolling averages and examining facility-specific historical patterns, it preserves the underlying trends in the data while correcting anomalous values. This approach ensures that adjusted data remains grounded in real service delivery patterns rather than arbitrary replacements.</p> <p>To accommodate different analytical needs, the module produces four parallel versions of the data: one with no adjustments (the original data), one with only outlier corrections, one with only missing data filled in, and one with both types of corrections applied. This multi-scenario approach allows analysts to understand how sensitive their results are to different data quality assumptions and choose the most appropriate version for their analysis.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#why-is-it-needed-in-the-fastr-pipeline","title":"Why is it needed in the FASTR pipeline?","text":"<p>Routine health management information system (HMIS) data often contains errors and gaps that can seriously distort trends, mask true patterns, and lead to incorrect conclusions. A single extreme outlier can make service volumes appear to spike dramatically, while missing data can make it seem like services stopped entirely. These issues are particularly problematic when:</p> <ul> <li>Tracking progress toward health goals and targets</li> <li>Comparing facilities or regions to identify high and low performers</li> <li>Allocating resources based on service delivery patterns</li> <li>Detecting genuine changes in health service utilization versus data quality issues</li> </ul> <p>By systematically addressing these data quality issues before analysis, this module ensures that downstream calculations and decisions are based on reliable, consistent data rather than artifacts of poor data quality.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#quick-summary","title":"Quick Summary","text":"Aspect Details Purpose Replace outlier values and fill missing data using facility-specific historical patterns Key Inputs Raw HMIS data (<code>hmis_AFG.csv</code>)Outlier flags from Module 1 (<code>M1_output_outliers.csv</code>)Completeness flags from Module 1 (<code>M1_output_completeness.csv</code>) Key Outputs Facility-level adjusted data (<code>M2_adjusted_data.csv</code>)Subnational aggregated data (<code>M2_adjusted_data_admin_area.csv</code>)National aggregated data (<code>M2_adjusted_data_national.csv</code>)Exclusion metadata (<code>M2_low_volume_exclusions.csv</code>) Scenarios Produced None (original data), Outliers only, Completeness only, Both adjustments"},{"location":"02_module_data_quality_adjustments_documentation/#2-how-it-works","title":"2. How It Works","text":""},{"location":"02_module_data_quality_adjustments_documentation/#high-level-workflow","title":"High-Level Workflow","text":"<p>The module follows a systematic seven-step process to clean and adjust health facility data:</p> <p>Step 1: Load and Prepare Data The module brings together three datasets: the raw facility service volumes, the outlier flags identifying suspicious values (from Module 1), and the completeness flags showing which months had incomplete reporting (from Module 1). It also identifies certain sensitive indicators like deaths that should never be adjusted.</p> <p>Step 2: Identify Low-Volume Indicators Before making any adjustments, the module checks each indicator to see if it has meaningful variation. Indicators that never have values above 100 across the entire dataset are flagged and excluded from outlier adjustment, since outlier detection isn't meaningful for consistently low-count indicators.</p> <p>Step 3: Adjust Outlier Values For each value flagged as an outlier, the module calculates what the value \"should have been\" based on that facility's historical pattern. It uses a hierarchy of methods, preferring to use surrounding months when possible (like averaging the 3 months before and 3 months after), but falling back to other approaches if needed (like using the same month from the previous year for seasonal indicators).</p> <p>Step 4: Fill Missing and Incomplete Data For months where data is missing or marked as incomplete, the module imputes (fills in) values using the same rolling average approach. This ensures that temporary reporting gaps don't create artificial drops to zero in the data.</p> <p>Step 5: Create Multiple Scenarios The module runs the adjustment logic four different ways: with no adjustments (baseline), only outlier corrections, only completeness corrections, and both types of corrections together. This allows analysts to see how different choices affect their results.</p> <p>Step 6: Aggregate to Geographic Levels After adjustments are complete, the facility-level data is aggregated (summed up) to create subnational and national-level datasets. Each geographic level maintains all four scenarios, so analysts can work at whichever level they need.</p> <p>Step 7: Export Results The module saves four CSV files: one for facility-level data, one for subnational areas, one for national totals, and one documenting which indicators were excluded from adjustment and why.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#visual-workflow","title":"Visual Workflow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INPUTS                                                      \u2502\n\u2502                                                             \u2502\n\u2502  Raw HMIS Data          Outlier Flags      Completeness    \u2502\n\u2502  (service volumes)      (from Module 1)    Flags           \u2502\n\u2502                                            (from Module 1)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 STEP 1: Data Preparation                                    \u2502\n\u2502  \u2022 Merge datasets by facility, indicator, and month         \u2502\n\u2502  \u2022 Identify excluded indicators (deaths)                    \u2502\n\u2502  \u2022 Check for low-volume indicators                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 STEP 2: Calculate Rolling Averages                          \u2502\n\u2502  \u2022 Centered 6-month average (3 before + 3 after)            \u2502\n\u2502  \u2022 Forward 6-month average (next 6 months)                  \u2502\n\u2502  \u2022 Backward 6-month average (previous 6 months)             \u2502\n\u2502  \u2022 Facility-specific historical mean                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 STEP 3: Apply Adjustment Hierarchy                          \u2502\n\u2502                                                             \u2502\n\u2502  For Outliers:                 For Missing/Incomplete:      \u2502\n\u2502  1. Use centered average       1. Use centered average      \u2502\n\u2502  2. Use forward average        2. Use forward average       \u2502\n\u2502  3. Use backward average       3. Use backward average      \u2502\n\u2502  4. Use same month last year   4. Use historical mean       \u2502\n\u2502  5. Use historical mean                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 STEP 4: Create Four Scenarios                               \u2502\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502   None      \u2502  \u2502   Outliers   \u2502  \u2502  Completeness   \u2502   \u2502\n\u2502  \u2502 (original)  \u2502  \u2502   Only       \u2502  \u2502     Only        \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                             \u2502\n\u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502                   \u2502     Both     \u2502                          \u2502\n\u2502                   \u2502  Adjustments \u2502                          \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 STEP 5: Geographic Aggregation                              \u2502\n\u2502                                                             \u2502\n\u2502  Facility Level \u2192 Subnational Level \u2192 National Level        \u2502\n\u2502  (individual)     (provinces/         (country total)       \u2502\n\u2502                    districts)                               \u2502\n\u2502                                                             \u2502\n\u2502  Each level maintains all 4 scenarios                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 OUTPUTS                                                     \u2502\n\u2502                                                             \u2502\n\u2502  M2_adjusted_data.csv           (facility level)            \u2502\n\u2502  M2_adjusted_data_admin_area.csv (subnational)             \u2502\n\u2502  M2_adjusted_data_national.csv  (national)                 \u2502\n\u2502  M2_low_volume_exclusions.csv   (metadata)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"02_module_data_quality_adjustments_documentation/#key-decision-points","title":"Key Decision Points","text":"<p>Which values should be adjusted? The module adjusts two types of problematic values: - Values flagged as outliers by Module 1's statistical detection algorithms - Values from months marked as incomplete or entirely missing</p> <p>However, certain indicators are NEVER adjusted: - Death counts (under-5 deaths, maternal deaths, neonatal deaths) because these represent discrete events that should not be smoothed - Low-volume indicators (those that never exceed 100) where outlier detection isn't meaningful</p> <p>How are replacement values calculated? The module follows a hierarchy from most to least reliable:</p> <ol> <li>Best option: Use the average of surrounding months (3 before and 3 after) if available</li> <li>Good option: If surrounding months aren't available, use either future months or past months</li> <li>Acceptable option: For seasonal indicators, use the same month from last year (e.g., June 2022 to replace June 2023)</li> <li>Fallback option: Use the facility's overall average for that indicator if nothing else works</li> </ol> <p>This hierarchy ensures that replacements are as contextually relevant as possible, using the most recent and similar data available.</p> <p>Which scenario should analysts use? By producing four scenarios, the module allows different use cases:</p> <ul> <li>None: Use for validation or when data quality is already excellent</li> <li>Outliers only: Use when completeness is good but occasional extreme values are problematic</li> <li>Completeness only: Use when you trust the reported values but reporting is sporadic</li> <li>Both: Use when both data quality issues are prevalent (most common choice)</li> </ul>"},{"location":"02_module_data_quality_adjustments_documentation/#what-happens-to-the-data","title":"What Happens to the Data","text":"<p>Before adjustment, a facility's monthly service volume data might look like this:</p> <pre><code>Month      Deliveries   Issue\nJan 2023   78           OK\nFeb 2023   450          OUTLIER (data entry error)\nMar 2023   [missing]    INCOMPLETE REPORTING\nApr 2023   82           OK\nMay 2023   85           OK\n</code></pre> <p>After adjustment (using the \"both\" scenario), the same data becomes:</p> <pre><code>Month      Original   Adjusted   Method Used\nJan 2023   78         78         No adjustment needed\nFeb 2023   450        82         Replaced using 6-month average\nMar 2023   [missing]  80         Filled using 6-month average\nApr 2023   82         82         No adjustment needed\nMay 2023   85         85         No adjustment needed\n</code></pre> <p>The adjusted data maintains realistic values that fit the facility's typical pattern, making it suitable for trend analysis, comparison with other facilities, and resource planning.</p>"},{"location":"02_module_data_quality_adjustments_documentation/#3-detailed-reference","title":"3. Detailed Reference","text":"Configuration Parameters  ### Excluded Indicators  Some indicators are excluded from all adjustments due to their sensitive nature:  <pre><code>EXCLUDED_FROM_ADJUSTMENT &lt;- c(\"u5_deaths\", \"maternal_deaths\", \"neonatal_deaths\")\n</code></pre>  **Rationale**: Death counts should not be smoothed or imputed as they represent discrete events that may have genuine temporal variation. Adjusting these could mask important epidemiological patterns or outbreak signals.  ### Low Volume Exclusions  Indicators are also automatically excluded from **outlier adjustment** if they have zero observations above 100 across the entire dataset. This prevents meaningless outlier detection on indicators with consistently low counts.  **Exclusion Logic**: <pre><code>volume_check &lt;- raw_data[, .(\n  above_100 = sum(count &gt; 100, na.rm = TRUE),\n  total = .N\n), by = indicator_common_id]\n\nno_outlier_adj &lt;- volume_check[above_100 == 0, indicator_common_id]\n</code></pre>  This information is saved to `M2_low_volume_exclusions.csv` for transparency.  ### Rolling Window Configuration  The module uses a **6-month window** for all rolling averages. This choice balances:  **Advantages**: - Captures medium-term trends - Reduces impact of short-term fluctuations - Sufficient data points for stable averages - Works well for both stable and seasonal indicators  **Trade-offs**: - May not capture rapid changes in service delivery - Could over-smooth in cases of genuine programmatic shifts - Requires at least 6 valid observations for optimal centered average   Input/Output Specifications  ### Input Files  The module requires three input files from previous processing steps:  | File | Source | Description | Key Variables | |------|--------|-------------|---------------| | `hmis_AFG.csv` | Raw HMIS data | Facility-level service volumes | `facility_id`, `indicator_common_id`, `period_id`, `count`, admin area columns | | `M1_output_outliers.csv` | Module 1 | Outlier flags for each facility-month-indicator | `facility_id`, `indicator_common_id`, `period_id`, `outlier_flag` | | `M1_output_completeness.csv` | Module 1 | Completeness flags for each facility-month-indicator | `facility_id`, `indicator_common_id`, `period_id`, `completeness_flag` |  ### Input Data Structure  **Raw HMIS Data (`hmis_AFG.csv`)**: <pre><code>facility_id | admin_area_1 | admin_area_2 | admin_area_3 | period_id | indicator_common_id | count\n------------|--------------|--------------|--------------|-----------|---------------------|-------\nFAC001      | AFG          | Kabul        | District_A   | 202301    | anc1                | 145\nFAC001      | AFG          | Kabul        | District_A   | 202302    | anc1                | 152\nFAC001      | AFG          | Kabul        | District_A   | 202303    | anc1                | 890  # Outlier\n</code></pre>  **Outlier Flags (`M1_output_outliers.csv`)**: <pre><code>facility_id | indicator_common_id | period_id | outlier_flag\n------------|---------------------|-----------|-------------\nFAC001      | anc1                | 202301    | 0\nFAC001      | anc1                | 202302    | 0\nFAC001      | anc1                | 202303    | 1           # Flagged as outlier\n</code></pre>  **Completeness Flags (`M1_output_completeness.csv`)**: <pre><code>facility_id | indicator_common_id | period_id | completeness_flag\n------------|---------------------|-----------|------------------\nFAC001      | anc1                | 202301    | 1             # Complete\nFAC001      | anc1                | 202302    | 0             # Incomplete\nFAC001      | anc1                | 202303    | 1             # Complete\n</code></pre>  ### Output Files  The module generates four output files:  | File | Level | Description | Key Columns | |------|-------|-------------|-------------| | `M2_adjusted_data.csv` | Facility | Adjusted volumes for all scenarios at facility level | `facility_id`, admin areas (excl. admin_area_1), `period_id`, `indicator_common_id`, `count_final_*` | | `M2_adjusted_data_admin_area.csv` | Subnational | Aggregated adjusted volumes at subnational admin areas | Admin areas (excl. admin_area_1), `period_id`, `indicator_common_id`, `count_final_*` | | `M2_adjusted_data_national.csv` | National | Aggregated adjusted volumes at national level | `admin_area_1`, `period_id`, `indicator_common_id`, `count_final_*` | | `M2_low_volume_exclusions.csv` | Metadata | Indicators excluded from outlier adjustment due to low volumes | `indicator_common_id`, `low_volume_exclude` |  ### Output Data Structure  **Facility-Level Output** (`M2_adjusted_data.csv`): <pre><code>facility_id | admin_area_2 | admin_area_3 | period_id | indicator_common_id | count_final_none | count_final_outliers | count_final_completeness | count_final_both\n------------|--------------|--------------|-----------|---------------------|------------------|----------------------|--------------------------|------------------\nFAC001      | Kabul        | District_A   | 202301    | anc1                | 145              | 145                  | 145                      | 145\nFAC001      | Kabul        | District_A   | 202302    | anc1                | 152              | 152                  | 148                      | 148\nFAC001      | Kabul        | District_A   | 202303    | anc1                | 890              | 148                  | 890                      | 148\n</code></pre>  Each `count_final_*` column represents a different adjustment scenario: - `count_final_none`: No adjustments applied (original values) - `count_final_outliers`: Only outlier adjustment applied - `count_final_completeness`: Only completeness adjustment applied - `count_final_both`: Both outlier and completeness adjustments applied   Key Functions Documentation  ### Required Libraries  The module depends on the following R packages:  -   `data.table` - High-performance data manipulation and aggregation -   `zoo` - Rolling window calculations (`frollmean` for rolling averages) -   `lubridate` - Date handling and manipulation  ### 1. `apply_adjustments()`  Core function that implements the adjustment logic for a single scenario.  **Purpose**: Replaces outlier and/or incomplete values using rolling averages and historical patterns.  **Parameters**: - `raw_data` (data.table): Original HMIS data with service counts - `completeness_data` (data.table): Completeness flags from Module 1 - `outlier_data` (data.table): Outlier flags from Module 1 - `adjust_outliers` (logical): Whether to apply outlier adjustment - `adjust_completeness` (logical): Whether to apply completeness adjustment  **Returns**: data.table with adjusted values in `count_working` column and adjustment metadata  **Key Operations**: 1. Merges input datasets by `facility_id`, `indicator_common_id`, and `period_id` 2. Converts `period_id` to dates for temporal ordering 3. Calculates rolling averages (centered, forward, backward) for valid values 4. Applies adjustment hierarchy based on data availability 5. Tracks adjustment method used for each replaced value  ### 2. `apply_adjustments_scenarios()`  Wrapper function that runs adjustments across all four scenarios.  **Purpose**: Applies the adjustment logic under different combinations of outlier and completeness adjustments.  **Parameters**: - `raw_data` (data.table): Original HMIS data - `completeness_data` (data.table): Completeness flags - `outlier_data` (data.table): Outlier flags  **Returns**: data.table with four `count_final_*` columns, one per scenario  **Scenarios Processed**: 1. `none`: No adjustments (baseline) 2. `outliers`: Outlier adjustment only 3. `completeness`: Completeness adjustment only 4. `both`: Sequential outlier then completeness adjustment  **Processing Logic**: - Calls `apply_adjustments()` once per scenario - Preserves original values for excluded indicators (deaths) - Merges all scenario results into a single wide-format table   Statistical Methods &amp; Algorithms  ### Outlier Adjustment Methodology  Outlier adjustment is applied to any facility-month value flagged in Module 1 (`outlier_flag == 1`). The goal is to replace these outlier values using valid historical data from the same facility and indicator.  **Statistical Approach**: Rolling averages are used to estimate expected values. A rolling average (also called moving average) is the mean of a set of time periods surrounding the target period. This technique smooths short-term fluctuations and highlights longer-term trends.  **Valid Values Definition**: Only values meeting ALL of the following criteria are used in calculations: - `count &gt; 0` (positive non-zero values) - `!is.na(count)` (non-missing) - `outlier_flag == 0` (not flagged as outlier)  **Implementation**: The module uses `frollmean()` from the `zoo` package for efficient rolling calculations: <pre><code>data_adj[, valid_count := fifelse(outlier_flag == 0L &amp; !is.na(count), count, NA_real_)]\ndata_adj[, `:=`(\n  roll6   = frollmean(valid_count, 6, na.rm = TRUE, align = \"center\"),\n  fwd6    = frollmean(valid_count, 6, na.rm = TRUE, align = \"left\"),\n  bwd6    = frollmean(valid_count, 6, na.rm = TRUE, align = \"right\"),\n  fallback= mean(valid_count, na.rm = TRUE)\n), by = .(facility_id, indicator_common_id)]\n</code></pre>  ### Adjustment Hierarchy for Outliers  The adjustment process follows this **hierarchical order** (stopping at the first available method):  1.  **Centered 6-Month Average (`roll6`)**     -   Uses the three months before and three months after the outlier month     -   Provides a balanced average based on nearby trends     -   Applied when enough valid values exist on both sides of the month     -   Method tag: `roll6`  2.  **Forward-Looking 6-Month Average (`fwd6`)**     -   Used if the centered average can't be calculated (e.g. early in the time series)     -   Takes the average of the next six valid months     -   Method tag: `forward`  3.  **Backward-Looking 6-Month Average (`bwd6`)**     -   Used if neither `roll6` nor `fwd6` are available     -   Takes the average of the six most recent valid months before the outlier     -   Method tag: `backward`  4.  **Same Month from Previous Year**     -   If no valid 6-month average exists, the value from the **same calendar month in the previous year** is used (e.g., Jan 2023 for Jan 2024)     -   Only applied if that previous value is valid (not an outlier, and &gt; 0)     -   Particularly useful for seasonal indicators (e.g., malaria, respiratory infections)     -   Method tag: `same_month_last_year`     -   **Implementation**:     <pre><code>data_adj[, `:=`(mm = month(date), yy = year(date))]\ndata_adj &lt;- data_adj[, {\n  for (i in which(outlier_flag == 1L &amp; is.na(adj_method))) {\n    j &lt;- which(mm == mm[i] &amp; yy == yy[i] - 1 &amp; outlier_flag == 0L &amp; !is.na(count))\n    if (length(j) == 1L) {\n      count_working[i] &lt;- count[j]\n      adj_method[i]    &lt;- \"same_month_last_year\"\n      adjust_note[i]   &lt;- format(date[j], \"%b-%Y\")\n    }\n  }\n  .SD\n}, by = .(facility_id, indicator_common_id)]\n</code></pre>  5.  **Mean of All Historical Values (Fallback)**     -   If all previous methods fail, the mean of all valid historical values for that facility-indicator is used     -   Provides a facility-specific baseline when no temporal pattern is available     -   Method tag: `fallback`  **Edge Case**: If no valid replacement can be found from any of these methods, the original outlier value is retained.  ### Completeness Adjustment Methodology  Completeness adjustment is applied to any facility-month where: - The month is flagged as incomplete (`completeness_flag != 1`) in Module 1, OR - The value is missing (`is.na(count_working)`)  **Statistical Approach**: The same rolling average methodology is applied, but the definition of \"valid values\" differs slightly:  **Valid Values for Completeness Adjustment**: - `!is.na(count_working)` (non-missing, possibly already adjusted for outliers) - `outlier_flag == 0` (not flagged as outlier in original data)  **Key Difference from Outlier Adjustment**: - Completeness adjustment can use values that were already adjusted for outliers (when scenarios include both adjustments) - No same-month-last-year method is used (only rolling averages and fallback)  **Implementation**: <pre><code>data_adj[, valid_count := fifelse(!is.na(count_working) &amp; outlier_flag == 0L, count_working, NA_real_)]\ndata_adj[, `:=`(\n  roll6   = frollmean(valid_count, 6, na.rm = TRUE, align = \"center\"),\n  fwd6    = frollmean(valid_count, 6, na.rm = TRUE, align = \"left\"),\n  bwd6    = frollmean(valid_count, 6, na.rm = TRUE, align = \"right\"),\n  fallback= mean(valid_count, na.rm = TRUE)\n), by = .(facility_id, indicator_common_id)]\n</code></pre>  ### Adjustment Hierarchy for Completeness  The replacement follows this **hierarchical order**:  1.  **Centered 6-Month Average (`roll6`)**     -   Uses three valid months before and after the missing or incomplete month     -   Preferred method when sufficient surrounding data exists     -   Method tag: `roll6`  2.  **Forward-Looking 6-Month Average (`fwd6`)**     -   Used if the centered average cannot be calculated (e.g., at start of time series)     -   Method tag: `forward`  3.  **Backward-Looking 6-Month Average (`bwd6`)**     -   Used if no centered or forward-looking values are available (e.g., at end of time series)     -   Method tag: `backward`  4.  **Mean of All Historical Values (Fallback)**     -   If no rolling averages can be calculated, uses the mean of all valid values for that facility-indicator     -   Provides a facility-specific baseline     -   Method tag: `fallback`  **Edge Case**: If no valid replacement is found, the value remains missing (`NA`).  ### Scenario Processing Logic  The module processes all four adjustment scenarios simultaneously using the `apply_adjustments_scenarios()` function:  **Scenario 1: None** (`count_final_none`) - `adjust_outliers = FALSE`, `adjust_completeness = FALSE` - Original raw data with no modifications - Serves as baseline for comparison  **Scenario 2: Outliers** (`count_final_outliers`) - `adjust_outliers = TRUE`, `adjust_completeness = FALSE` - Only outlier values are replaced - Missing/incomplete values remain as-is - Use case: When completeness is high but outliers are a concern  **Scenario 3: Completeness** (`count_final_completeness`) - `adjust_outliers = FALSE`, `adjust_completeness = TRUE` - Only missing/incomplete values are imputed - Outliers are retained in the data - Use case: When data quality is good but reporting is sporadic  **Scenario 4: Both** (`count_final_both`) - `adjust_outliers = TRUE`, `adjust_completeness = TRUE` - **Sequential processing**: Outliers adjusted first, then completeness - Most comprehensive adjustment - Use case: When both data quality issues are prevalent  **Processing Order for \"Both\" Scenario**: 1. Outlier adjustment creates `count_working` with outliers replaced 2. Completeness adjustment then operates on `count_working`, using the already-adjusted values 3. This ensures completeness imputation uses cleaned (non-outlier) values when available  **Important**: After scenario-specific adjustments, excluded indicators (deaths) are reset to their original values: <pre><code>dat[indicator_common_id %in% EXCLUDED_FROM_ADJUSTMENT, count_working := count]\n</code></pre>  ### Aggregation Methods  All geographic aggregations use **simple sums**:  <pre><code>sum(count_final_both, na.rm = TRUE)\n</code></pre>  **Rationale**: - Service volumes are additive (e.g., total deliveries = sum of facility deliveries) - Missing values (`NA`) are treated as zero in aggregation - Consistent with standard HMIS reporting practices  **Caution**: If many facilities have `NA` values after adjustment, subnational/national totals may be underestimated. The `count_final_none` scenario provides a reference point for assessing impact.  ### Handling Missing Data in Calculations  The module applies `na.rm = TRUE` in all rolling calculations:  <pre><code>frollmean(valid_count, 6, na.rm = TRUE, align = \"center\")\n</code></pre>  **Implication**: Rolling averages are calculated from available valid values only. If fewer than 6 values exist, the average is computed from whatever is available. If no valid values exist, the result is `NA`.   Code Examples  ### Example 1: Outlier Adjustment  **Scenario**: A facility reports an unusually high ANC1 visit count in March 2023.  **Data**: <pre><code>period_id | count | outlier_flag | Surrounding valid values\n----------|-------|--------------|-------------------------\n202301    | 145   | 0            | valid\n202302    | 152   | 0            | valid\n202303    | 890   | 1            | OUTLIER\n202304    | 148   | 0            | valid\n202305    | 155   | 0            | valid\n202306    | 147   | 0            | valid\n</code></pre>  **Adjustment Calculation** (centered 6-month average): - Valid values: [145, 152, 148, 155, 147] (excludes outlier 890) - Average: (145 + 152 + 148 + 155 + 147) / 5 = 149.4 - **Adjusted value**: 149.4  **Method used**: `roll6`  ### Example 2: Completeness Adjustment  **Scenario**: A facility fails to report malaria tests in February 2023.  **Data**: <pre><code>period_id | count | completeness_flag | Surrounding valid values\n----------|-------|-------------------|-------------------------\n202301    | 45    | 1                 | valid\n202302    | NA    | 0                 | INCOMPLETE\n202303    | 48    | 1                 | valid\n202304    | 52    | 1                 | valid\n202305    | 50    | 1                 | valid\n</code></pre>  **Adjustment Calculation** (centered 6-month average): - Valid values: [45, 48, 52, 50, ...] - Average: 48.75 (using available surrounding months) - **Imputed value**: 48.75  **Method used**: `roll6`  ### Example 3: Seasonal Indicator with Same-Month-Last-Year  **Scenario**: Malaria cases show strong seasonality, and a June 2023 outlier needs adjustment.  **Data**: <pre><code>period_id | count | outlier_flag | Notes\n----------|-------|--------------|-------\n202206    | 234   | 0            | June 2022 (valid)\n202306    | 1850  | 1            | June 2023 (OUTLIER)\n</code></pre>  **Adjustment Logic**: 1. Centered, forward, and backward rolling averages unavailable (insufficient data) 2. Same-month-last-year method activated 3. June 2022 value = 234 (valid) 4. **Adjusted value**: 234  **Method used**: `same_month_last_year`  ### Example 4: Scenario Comparison  **Facility**: FAC001 **Indicator**: Institutional deliveries **Period**: Q1 2023  **Original Data**: <pre><code>Month    | Count | Outlier? | Complete?\n---------|-------|----------|----------\nJan 2023 | 78    | No       | Yes\nFeb 2023 | 450   | Yes      | Yes       # Outlier\nMar 2023 | NA    | -        | No        # Incomplete\n</code></pre>  **Scenario Results**:  | Month    | None | Outliers | Completeness | Both | |----------|------|----------|--------------|------| | Jan 2023 | 78   | 78       | 78           | 78   | | Feb 2023 | 450  | 82*      | 450          | 82*  | | Mar 2023 | NA   | NA       | 80**         | 80** |  *Adjusted using rolling average **Imputed using rolling average  **Interpretation**: - **None**: Raw data with obvious issues - **Outliers**: February corrected, but March remains missing - **Completeness**: March filled in, but February outlier retained - **Both**: Most complete and clean dataset  ### Example 5: Geographic Aggregation  **Subnational Aggregation Code**: <pre><code>adjusted_data_admin_area_final &lt;- adjusted_data_export[\n  ,\n  .(\n    count_final_none         = sum(count_final_none,         na.rm = TRUE),\n    count_final_outliers     = sum(count_final_outliers,     na.rm = TRUE),\n    count_final_completeness = sum(count_final_completeness, na.rm = TRUE),\n    count_final_both         = sum(count_final_both,         na.rm = TRUE)\n  ),\n  by = c(geo_admin_area_sub, \"indicator_common_id\", \"period_id\")\n]\n</code></pre>  **National Aggregation Code**: <pre><code>adjusted_data_national_final &lt;- adjusted_data_export[\n  ,\n  .(\n    count_final_none         = sum(count_final_none,         na.rm = TRUE),\n    count_final_outliers     = sum(count_final_outliers,     na.rm = TRUE),\n    count_final_completeness = sum(count_final_completeness, na.rm = TRUE),\n    count_final_both         = sum(count_final_both,         na.rm = TRUE)\n  ),\n  by = .(admin_area_1, indicator_common_id, period_id)\n]\n</code></pre> Troubleshooting  ### Common Issues  **Issue 1: All values remain unadjusted** - **Possible causes**:   - Indicator is in the excluded list (deaths)   - Indicator flagged as low-volume   - No outlier or completeness flags in input data - **Solution**: Check `M2_low_volume_exclusions.csv` and verify Module 1 outputs contain flags  **Issue 2: Adjusted values seem unreasonable** - **Possible causes**:   - Insufficient valid historical data for rolling averages   - Genuine program changes being smoothed out   - Seasonal patterns not captured by 6-month window - **Solution**:   - Review facility-specific time series plots   - Consider using \"outliers only\" scenario if completeness is good   - Validate against program implementation records  **Issue 3: Many NA values after adjustment** - **Possible causes**:   - Facility has very sparse data   - No valid values available for any adjustment method   - Early months in time series lack historical data - **Solution**:   - Expected for facilities with limited reporting history   - Consider facility-level data quality filtering   - National/subnational aggregates will sum available values  **Issue 4: Subnational/national totals don't match expectations** - **Possible causes**:   - NA values treated as zero in aggregation   - Different scenarios produce different totals   - Low reporting completeness overall - **Solution**:   - Compare `count_final_none` vs `count_final_both` to assess adjustment impact   - Review Module 1 completeness statistics   - Consider data quality threshold for inclusion  ### Quality Assurance Checks  The module includes several quality checks:  1. **Low Volume Exclusions**: Automatically identifies and excludes indicators with zero high-value observations 2. **Adjustment Tracking**: Counts and reports number of values adjusted by each method 3. **Excluded Indicators**: Ensures deaths are never adjusted 4. **Console Logging**: Provides detailed progress and summary statistics  **Example Console Output**: <pre><code>Running adjustments...\n -&gt; Adjusting outliers...\n     Roll6 adjusted: 1,245\n     Forward-filled: 89\n     Backward-filled: 67\n     Same-month LY: 34\n     Fallback mean: 12\n -&gt; Adjusting for completeness...\n     Roll6 filled: 2,103\n     Forward-filled: 234\n     Backward-filled: 178\n     Fallback mean: 45\n</code></pre> Usage Notes &amp; Recommendations  ### Choosing the Right Scenario  | Situation | Recommended Scenario | Rationale | |-----------|---------------------|-----------| | High data quality, minimal issues | `none` | No adjustment needed | | Sporadic outliers, good completeness | `outliers` | Address quality without imputation | | Good quality, poor reporting frequency | `completeness` | Fill gaps while preserving actual values | | Poor quality and completeness | `both` | Comprehensive cleaning | | Uncertainty about data quality | Compare all scenarios | Sensitivity analysis |  ### Validation Steps  After running this module, consider:  1. **Compare scenarios**: Examine differences between `count_final_none` and `count_final_both` 2. **Review exclusions**: Check `M2_low_volume_exclusions.csv` for unexpected indicators 3. **Aggregate analysis**: Ensure subnational and national totals are reasonable 4. **Temporal plots**: Visualize trends before/after adjustment to identify over-smoothing 5. **Facility-level spot checks**: Review adjustments for a sample of facilities  ### Limitations  1. **Rolling windows assume stability**: Adjustments work best when service delivery is relatively stable. Genuine program changes (e.g., new campaigns) may be incorrectly smoothed.  2. **No adjustment uncertainty**: The module provides point estimates without confidence intervals. Adjusted values should be treated as estimates.  3. **Facility-specific adjustments**: No cross-facility borrowing of information. Facilities with very sparse data may have unstable adjustments.  4. **Seasonal patterns**: While same-month-last-year helps, strong within-year seasonality may not be fully captured by 6-month windows.  5. **NA treatment in aggregation**: Missing values are treated as zero when summing to higher geographic levels, which may underestimate totals if missingness is high.  ### Best Practices  1. **Always produce all four scenarios**: Even if you plan to use only one, having all scenarios allows for sensitivity analysis and validation  2. **Document your scenario choice**: When using adjusted data for analysis, clearly document which scenario was used and why  3. **Cross-validate with program data**: Compare adjusted trends with known programmatic events (campaigns, stockouts, facility closures)  4. **Consider data recency**: Be cautious with adjustments for the most recent months, which have less surrounding data for rolling averages  5. **Monitor excluded indicators**: Review the exclusion files to ensure appropriate indicators are being adjusted   <p>Last edit 2025 November 8</p>"},{"location":"03_module_service_utilization_documentation/","title":"Module 3: Service Utilization","text":""},{"location":"03_module_service_utilization_documentation/#1-overview-what-why","title":"1. Overview (What &amp; Why)","text":""},{"location":"03_module_service_utilization_documentation/#what-does-this-module-do","title":"What does this module do?","text":"<p>The Service Utilization module analyzes health service delivery patterns to detect and quantify disruptions in service volumes over time. It identifies when health services deviate significantly from expected patterns and measures the magnitude of these disruptions at national, provincial, and district levels.</p> <p>Using statistical process control methods and regression analysis, the module compares actual service volumes against historical trends and seasonal patterns. This helps distinguish between normal fluctuations (like expected increases in malaria cases during rainy season) and genuine disruptions that require investigation (like sudden drops in antenatal care during a pandemic or conflict).</p> <p>The analysis produces quantified estimates of service shortfalls and surpluses, enabling evidence-based resource allocation, policy decisions, and health system monitoring.</p>"},{"location":"03_module_service_utilization_documentation/#why-is-it-needed-in-the-fastr-pipeline","title":"Why is it needed in the FASTR pipeline?","text":"<p>Service utilization data reflects how populations access essential healthcare, but this data can fluctuate due to various factors: seasonal patterns, policy changes, external shocks (pandemics, natural disasters, conflicts), data quality issues, or service availability problems. Without systematic analysis, it's difficult to know whether observed changes represent normal variation or significant disruptions requiring action.</p> <p>This module provides objective, data-driven identification of service delivery problems and quantifies their impact. It enables health system managers to detect emerging issues early, target resources to affected areas, and monitor recovery after disruptions. The module's outputs support dashboard monitoring, impact assessments, and policy evaluation.</p>"},{"location":"03_module_service_utilization_documentation/#quick-summary","title":"Quick Summary","text":"Aspect Details Inputs <code>hmis_SLE.csv</code> (raw service data), <code>M1_output_outliers.csv</code> (outlier flags), <code>M2_adjusted_data.csv</code> (adjusted counts) Outputs <code>M3_chartout.csv</code> (disruption flags), <code>M3_disruptions_analysis_*.csv</code> (quantified impacts), <code>M3_all_indicators_shortfalls_*.csv</code> (shortfall/surplus summaries) Purpose Detect and quantify service delivery disruptions to support evidence-based health system monitoring and resource allocation Analysis Method Two-stage: (1) Control charts detect disruptions, (2) Panel regression quantifies impacts Geographic Levels National, Province, District, Ward (configurable) Runtime 5-60 minutes depending on configuration"},{"location":"03_module_service_utilization_documentation/#2-how-it-works","title":"2. How It Works","text":""},{"location":"03_module_service_utilization_documentation/#high-level-workflow","title":"High-Level Workflow","text":"<p>The module operates in two sequential stages, each with a distinct purpose:</p> <p>Stage 1: Control Chart Analysis - Identifies unusual patterns in service volumes</p> <ol> <li> <p>Prepare the data: Load health service data, remove previously identified outliers, aggregate to the appropriate geographic level, and fill in missing months using interpolation.</p> </li> <li> <p>Model expected patterns: For each combination of health indicator and geographic area, use robust statistical methods to estimate what service volumes should look like based on historical trends and seasonal patterns (e.g., accounting for predictable increases in malaria cases during rainy season).</p> </li> <li> <p>Detect deviations: Compare actual service volumes to expected patterns and identify significant deviations using multiple detection rules:</p> </li> <li>Sharp disruptions: Single months with extreme deviations</li> <li>Sustained drops: Gradual declines over several months</li> <li>Sustained dips: Periods consistently below expected levels</li> <li>Sustained rises: Periods consistently above expected levels</li> <li> <p>Missing data patterns: Gaps in reporting that may signal problems</p> </li> <li> <p>Flag disrupted periods: Mark months where any disruption pattern is detected, ensuring recent months are always flagged for review.</p> </li> </ol> <p>Stage 2: Disruption Analysis - Quantifies the impact of identified disruptions</p> <ol> <li> <p>Apply regression models: Use panel regression at multiple geographic levels (national, provincial, district) to estimate how much service volumes changed during flagged disruption periods, controlling for trends and seasonality.</p> </li> <li> <p>Calculate shortfalls and surpluses: Compare predicted volumes to actual volumes to quantify the magnitude of disruptions in absolute numbers and percentages.</p> </li> <li> <p>Generate outputs: Create summary files showing disruption impacts at each geographic level, ready for visualization and reporting.</p> </li> </ol>"},{"location":"03_module_service_utilization_documentation/#visual-workflow-diagram","title":"Visual Workflow Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      STAGE 1: CONTROL CHART ANALYSIS                    \u2502\n\u2502                     (Identifies WHEN disruptions occur)                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  Load Data \u2192 Remove Outliers \u2192 Aggregate by Geography                  \u2502\n\u2502       \u2502                                                                 \u2502\n\u2502       \u25bc                                                                 \u2502\n\u2502  Fill Missing Months \u2192 Filter Low-Volume Periods                       \u2502\n\u2502       \u2502                                                                 \u2502\n\u2502       \u25bc                                                                 \u2502\n\u2502  For each indicator \u00d7 geographic area:                                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502 1. Fit robust model: count ~ seasonality + time trend   \u2502         \u2502\n\u2502   \u2502 2. Smooth predictions to reduce noise                   \u2502         \u2502\n\u2502   \u2502 3. Calculate how far actual values deviate from expected\u2502         \u2502\n\u2502   \u2502 4. Apply detection rules (sharp, sustained, missing)    \u2502         \u2502\n\u2502   \u2502 5. Flag disrupted months                                \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502       \u2502                                                                 \u2502\n\u2502       \u25bc                                                                 \u2502\n\u2502  OUTPUT: M3_chartout.csv (disruption flags by month/area/indicator)    \u2502\n\u2502                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     STAGE 2: DISRUPTION ANALYSIS                        \u2502\n\u2502                    (Quantifies HOW MUCH impact occurred)                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  Join disruption flags to facility-level data                          \u2502\n\u2502       \u2502                                                                 \u2502\n\u2502       \u25bc                                                                 \u2502\n\u2502  Run panel regressions at multiple levels:                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502 National Level: Overall country impact                  \u2502         \u2502\n\u2502   \u2502 Province Level: Regional variation in impacts           \u2502         \u2502\n\u2502   \u2502 District Level: Local hotspots (optional)               \u2502         \u2502\n\u2502   \u2502 Ward Level: Finest geographic detail (optional)         \u2502         \u2502\n\u2502   \u2502                                                          \u2502         \u2502\n\u2502   \u2502 Each model estimates:                                   \u2502         \u2502\n\u2502   \u2502  \u2022 Expected service volume (adjusting for trends)       \u2502         \u2502\n\u2502   \u2502  \u2022 Effect of disruption periods                         \u2502         \u2502\n\u2502   \u2502  \u2022 Statistical significance                             \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502       \u2502                                                                 \u2502\n\u2502       \u25bc                                                                 \u2502\n\u2502  Calculate shortfalls and surpluses:                                   \u2502\n\u2502   \u2022 Absolute difference (number of services)                           \u2502\n\u2502   \u2022 Percentage difference (% change from expected)                     \u2502\n\u2502       \u2502                                                                 \u2502\n\u2502       \u25bc                                                                 \u2502\n\u2502  OUTPUTS:                                                              \u2502\n\u2502   \u2022 M3_disruptions_analysis_*.csv (4 geographic levels)                \u2502\n\u2502   \u2022 M3_all_indicators_shortfalls_*.csv (summary statistics)            \u2502\n\u2502   \u2022 M3_service_utilization.csv (data for visualization)                \u2502\n\u2502                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"03_module_service_utilization_documentation/#key-decision-points","title":"Key Decision Points","text":"<p>Geographic Level Selection: The module can analyze disruptions at different geographic scales. You can choose to run analysis at national and provincial levels only (faster, suitable for routine monitoring) or include district and ward levels (slower, provides detailed local information for targeted interventions).</p> <p>Sensitivity Settings: The module uses configurable thresholds to determine what constitutes a \"disruption.\" More sensitive settings (lower thresholds) will flag smaller deviations, useful for early warning systems. More conservative settings (higher thresholds) will only flag major disruptions, useful for focusing on critical issues.</p> <p>Data Completeness Approach: The module accepts different versions of service counts from Module 2, allowing you to choose whether to adjust for reporting completeness or use raw counts.</p>"},{"location":"03_module_service_utilization_documentation/#what-happens-to-the-data","title":"What Happens to the Data","text":"<p>Input Transformation: The module starts with facility-level service counts (e.g., number of deliveries at each clinic each month) and aggregates them to geographic areas (provinces, districts). Outliers identified in Module 1 are removed to prevent anomalous data points from skewing the analysis.</p> <p>Pattern Detection: Using robust statistical methods, the module learns what \"normal\" looks like for each service and area based on historical patterns. It then identifies months where actual volumes deviate significantly from these patterns, accounting for predictable variations like seasonal changes.</p> <p>Impact Quantification: For months flagged as disrupted, the module uses regression models to estimate what service volumes would have been without the disruption. By comparing predicted to actual volumes, it calculates how many services were missed (shortfalls) or how much service delivery increased (surpluses).</p> <p>Output Generation: The final outputs provide disruption impacts at multiple geographic scales, enabling users to see both national-level summaries and local-level details. All calculations preserve the original data while adding predicted values and disruption metrics.</p>"},{"location":"03_module_service_utilization_documentation/#3-detailed-reference","title":"3. Detailed Reference","text":"Configuration Parameters  ### Core Analysis Parameters  | Parameter | Default | Type | Description | Tuning Guidance | |-----------|---------|------|-------------|-----------------| | `COUNTRY_ISO3` | \"SLE\" | String | Three-letter country code | Set to your country code | | `SELECTEDCOUNT` | \"count_final_both\" | String | Data column used for analysis | Options: `count_final_none`, `count_final_completeness`, `count_final_both` | | `VISUALIZATIONCOUNT` | \"count_final_both\" | String | Data column used for visualization | Should match or complement `SELECTEDCOUNT` |  ### Control Chart Parameters  | Parameter | Default | Type | Description | Tuning Guidance | |-----------|---------|------|-------------|-----------------| | `SMOOTH_K` | 7 | Integer (odd) | Rolling median window size in months | Larger values = smoother trends, less sensitivity. Must be odd number (e.g., 5, 7, 9, 11) | | `MADS_THRESHOLD` | 1.5 | Numeric | MAD units threshold for sharp disruptions | Lower = more sensitive (e.g., 1.0), higher = more conservative (e.g., 2.0) | | `DIP_THRESHOLD` | 0.90 | Numeric | Proportion threshold for sustained dips | 0.90 = flag if below 90% of expected (10% drop). Use 0.80 for 20% drop threshold | | `DIFFPERCENT` | 10 | Numeric | Percentage threshold for plotting disruptions | If actual differs from predicted by &gt;10%, use predicted value in visualizations |  **Note**: `RISE_THRESHOLD` is automatically calculated as `1 / DIP_THRESHOLD` (default: ~1.11) to mirror dip detection symmetrically.  ### Geographic Analysis Parameters  | Parameter | Default | Type | Description | Tuning Guidance | |-----------|---------|------|-------------|-----------------| | `CONTROL_CHART_LEVEL` | Auto-set | String | Geographic level for control charts | Automatically set based on `RUN_DISTRICT_MODEL` and `RUN_ADMIN_AREA_4_ANALYSIS` | | `RUN_DISTRICT_MODEL` | FALSE | Logical | Whether to run admin_area_3 regressions | Set TRUE for district-level analysis (increases runtime) | | `RUN_ADMIN_AREA_4_ANALYSIS` | FALSE | Logical | Whether to run admin_area_4 analysis | Set TRUE for finest-level analysis (very slow for large datasets) |  ### Data Source Parameters  | Parameter | Default | Type | Description | |-----------|---------|------|-------------| | `PROJECT_DATA_HMIS` | \"hmis_SLE.csv\" | String | Filename for raw HMIS data |  ### Parameter Selection Guide  **For High-Sensitivity Analysis** (detecting smaller disruptions): - `MADS_THRESHOLD = 1.0` - `DIP_THRESHOLD = 0.95` (5% drop) - `SMOOTH_K = 5` (less smoothing)  **For Conservative Analysis** (only major disruptions): - `MADS_THRESHOLD = 2.0` - `DIP_THRESHOLD = 0.80` (20% drop) - `SMOOTH_K = 9` or `11` (more smoothing)  **For Faster Runtime**: - `RUN_DISTRICT_MODEL = FALSE` - `RUN_ADMIN_AREA_4_ANALYSIS = FALSE` - `CONTROL_CHART_LEVEL = \"admin_area_2\"`   Input/Output Specifications  ### Input Requirements  #### Primary Inputs  1. **`hmis_SLE.csv`** (or country-specific HMIS file)    - Raw HMIS service utilization data    - Required columns: `facility_id`, `admin_area_1`, `admin_area_2`, `admin_area_3`, `admin_area_4`, `indicator_common_id`, `period_id`, service count columns  2. **`M1_output_outliers.csv`**    - Output from Module 1 (Data Quality Assessment)    - Contains `outlier_flag` to exclude anomalous data points    - Required columns: `facility_id`, `indicator_common_id`, `period_id`, `outlier_flag`  3. **`M2_adjusted_data.csv`**    - Output from Module 2 (Data Quality Adjustments)    - Contains adjusted service counts with different completeness assumptions    - Required columns: `facility_id`, `indicator_common_id`, `period_id`, `count_final_none`, `count_final_completeness`, `count_final_both`  #### Data Requirements  - **Temporal coverage**: Minimum 12 months of data for seasonal modeling - **Data completeness**: Missing months are filled using interpolation - **Geographic completeness**: Data at specified administrative levels - **Count data**: Non-negative integer counts (predictions are bounded at zero)  ### Outputs  #### 1. Control Chart Results  **`M3_chartout.csv`** - **Purpose**: Contains flagged disruptions from the control chart analysis - **Columns**:   - `admin_area_*`: Geographic identifier (level depends on `CONTROL_CHART_LEVEL`)   - `indicator_common_id`: Health service indicator code   - `period_id`: Time period in YYYYMM format   - `tagged`: Binary flag (1 = disruption detected, 0 = normal)   - `count_original`: Actual service volume   - `count_predict`: Predicted volume from regression   - `count_smooth`: Smoothed prediction   - `residual`: Deviation from expected   - `robust_control`: Standardized residual   - `tag_sharp`, `tag_sustained`, `tag_sustained_dip`, `tag_sustained_rise`, `tag_missing`: Individual disruption flags - **Use**: Identifies which months require further investigation for each indicator-geography combination  **`M3_service_utilization.csv`** - **Purpose**: Pass-through copy of adjusted data for visualization - **Source**: Direct copy of `M2_adjusted_data.csv` - **Use**: Provides baseline data for plotting actual service volumes  **`M3_memory_log.txt`** - **Purpose**: Tracks memory usage throughout execution - **Use**: Diagnostics for performance optimization and troubleshooting  #### 2. Disruption Analysis Results  **`M3_disruptions_analysis_admin_area_1.csv`** (National level - always generated) - **Columns**:   - `admin_area_1`: Country name   - `indicator_common_id`: Health service indicator   - `period_id`: Time period (YYYYMM)   - `count_sum`: Actual service volume (sum across all facilities)   - `count_expect_sum`: Expected service volume (sum of predictions)   - `count_expected_if_above_diff_threshold`: Value for plotting (expected if |difference| &gt; DIFFPERCENT, otherwise actual)  **`M3_disruptions_analysis_admin_area_2.csv`** (Province level - always generated) - **Additional column**: `admin_area_2` (province/region name) - **Same structure** as admin_area_1 file but disaggregated by province  **`M3_disruptions_analysis_admin_area_3.csv`** (District level - conditional) - **Generated when**: `RUN_DISTRICT_MODEL = TRUE` - **Additional columns**: `admin_area_2`, `admin_area_3` - **Same structure** as above but disaggregated by district  **`M3_disruptions_analysis_admin_area_4.csv`** (Ward level - conditional) - **Generated when**: `RUN_ADMIN_AREA_4_ANALYSIS = TRUE` - **Additional columns**: `admin_area_2`, `admin_area_3`, `admin_area_4` - **Warning**: Very large file size for countries with many wards  #### 3. Shortfall/Surplus Summary Files  **`M3_all_indicators_shortfalls_admin_area_*.csv`** (one for each geographic level)  - **Purpose**: Pre-calculated shortfall and surplus metrics for reporting - **Common columns**:   - Geographic identifier(s): `admin_area_*`   - `indicator_common_id`: Health service indicator   - `period_id`: Time period (YYYYMM)   - `count_sum`: Actual service volume   - `count_expect_sum`: Expected service volume   - `shortfall_absolute`: Absolute number of missing services (if negative disruption)   - `shortfall_percent`: Percentage shortfall relative to expected   - `surplus_absolute`: Absolute number of excess services (if positive disruption)   - `surplus_percent`: Percentage surplus relative to expected  **Note**: If optional geographic levels are disabled, empty placeholder files are created for compatibility with downstream processes.  #### Temporary Files (Automatically Cleaned)  During execution, the module creates temporary batch files for memory management: - `M3_temp_controlchart_batch_*.csv` - `M3_temp_indicator_batch_*.csv` - `M3_temp_province_batch_*.csv` - `M3_temp_district_batch_*.csv` - `M3_temp_admin4_batch_*.csv`  These are automatically deleted upon successful completion. If the script crashes, these files may remain and will be cleaned up on the next run.   Key Functions Documentation  ### `robust_control_chart(panel_data, selected_count)`  **Purpose**: Identifies anomalies in service utilization using robust regression and MAD-based control limits.  **Inputs**: - `panel_data`: Time series data for a specific indicator-geography combination - `selected_count`: Column name containing service volume counts to analyze  **Process**: 1. Fits a robust linear model (using `MASS::rlm()`) with seasonal controls and time trends 2. Applies rolling median smoothing to predicted values to reduce noise 3. Calculates residuals and standardizes them using Median Absolute Deviation (MAD) 4. Applies rule-based tagging logic to identify different disruption types 5. Flags recent months automatically to ensure timely detection  **Outputs**: - `count_predict`: Predicted service volume from robust regression - `count_smooth`: Smoothed predictions using rolling median - `residual`: Difference between actual and smoothed values - `robust_control`: Standardized residual (residual/MAD) - `tagged`: Binary flag (1 = disruption detected, 0 = normal variation) - Additional flags: `tag_sharp`, `tag_sustained`, `tag_sustained_dip`, `tag_sustained_rise`, `tag_missing`  **Key Features**: - Handles missing data gracefully with interpolation - Uses robust regression to minimize influence of outliers - Employs multiple disruption detection rules for different patterns - Ensures non-negative predictions (counts cannot be negative)  ### Panel Regression Models  The disruption analysis uses fixed-effects panel regression models (`fixest::feols()`) at multiple geographic levels.  **Country-wide Model** (Admin Area 1): <pre><code>count ~ date + factor(month) + tagged\n</code></pre> Clustered standard errors at district level (`admin_area_3`)  **Province-level Models** (Admin Area 2): <pre><code>count ~ date + factor(month) + tagged | admin_area_2\n</code></pre> Separate regression for each province, clustered at district level  **District-level Models** (Admin Area 3 - optional): <pre><code>count ~ date + factor(month) + tagged | admin_area_3\n</code></pre> Separate regression for each district, clustered at ward level (`admin_area_4`)  **Ward-level Models** (Admin Area 4 - optional): <pre><code>count ~ date + factor(month) + tagged | admin_area_4\n</code></pre> Separate regression for each ward/finest unit  ### Supporting Functions  **`mem_usage(msg)`**: Tracks and logs memory consumption throughout execution  **Data Processing**: - Batch processing with disk-based temporary files for memory efficiency - Efficient data.table operations for large datasets - Progressive aggregation and merging strategies   Statistical Methods &amp; Algorithms  ### Control Chart Analysis  Service volumes are aggregated at the specified geographic level (configurable via `CONTROL_CHART_LEVEL`). The pipeline removes outliers (`outlier_flag == 1`), fills in missing months, and filters low-volume months (&lt;50% of global mean volume).  A robust regression model estimates expected service volumes per indicator \u00d7 geographic area (`panelvar`). A centered rolling median is applied to smooth the predicted values. Residuals (actual - smoothed) are standardized using MAD. Disruptions are identified using a rule-based tagging system.  #### Disruption Detection Rules  Each rule is controlled by user-defined parameters, allowing customization of the sensitivity and behavior of the detection logic:  **Sharp Disruptions**  Flags a single month when the standardized residual (residual divided by MAD) exceeds a threshold:  $$ \\left| \\frac{\\text{residual}}{\\text{MAD}} \\right| \\geq \\text{MADS_THRESHOLD} $$  - **Parameter:** `MADS_THRESHOLD` (default: `1.5`) - Lower values make the detection more sensitive to sudden spikes or dips.  **Sustained Drops**  Flags a sustained drop if: - Three consecutive months show mild deviations (standardized residual \u2265 1), and - The final month also exceeds the `MADS_THRESHOLD`.  This captures slower, compounding declines.  **Sustained Dips**  Flags periods where the actual volume falls consistently below a defined proportion of expected volume (smoothed prediction):  $$ \\text{count\\_original} &lt; \\text{DIP\\_THRESHOLD} \\times \\text{count\\_smooth} $$  - **Parameter:** `DIP_THRESHOLD` (default: `0.90`) - Users can adjust this to detect deeper or shallower dips (e.g., `0.80` for a 20% drop).  **Sustained Rises**  Symmetric to dips, flags periods of consistent overperformance:  $$ \\text{count\\_original} &gt; \\text{RISE\\_THRESHOLD} \\times \\text{count\\_smooth} $$  - **Parameter:** `RISE_THRESHOLD` (default: `1 / DIP_THRESHOLD`, e.g., `1.11`) - Users can adjust this to detect upward surges in volume.  **Missing Data**  Flags when 2 or more of the past 3 months have missing (`NA`) or zero service volume. - **Fixed rule**.  **Recent Tail Override**  Automatically flags all months in the last 6 months of data to ensure recent trends are reviewed, even if model-based tagging is not conclusive. - **Fixed rule**.  **Final Flag:**  A month is assigned `tagged = 1` if **any** of the following conditions are met: - `tag_sharp == 1` - `tag_sustained == 1` - `tag_sustained_dip == 1` - `tag_sustained_rise == 1` - `tag_missing == 1` - It falls within the most recent 6 months (`last_6_months == 1`)  ### Robust Regression Model  **Model fitting:** - If \u226512 observations and &gt;12 unique dates: $$Y_{it} = \\beta_0 + \\sum \\gamma_m \\cdot \\text{month}_m + \\beta_1 \\cdot \\text{date} + \\epsilon_{it}$$  - If only \u226512 observations: $$Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\epsilon_{it}$$  - If insufficient data: use the median of observed values.  **Apply rolling median smoothing** to predictions: $$ \\text{count\\_smooth}_{it} = \\text{Median}(\\text{count\\_predict}_{t-k}, \\dots, \\text{count\\_predict}_t, \\dots, \\text{count\\_predict}_{t+k}) $$  - **Parameter:** `SMOOTH_K` (default: 7, must be odd) - Larger `SMOOTH_K` smooths more; smaller retains more variation.  **Calculate residuals:** $$ \\text{residual}_{it} = \\text{count\\_original}_{it} - \\text{count\\_smooth}_{it} $$  **Standardize residuals using MAD:** $$ \\text{robust\\_control}_{it} = \\text{residual}_{it} / \\text{MAD}_i $$  ### Disruption Analysis Regression Models  Once anomalies are identified and saved in `M3_chartout.csv`, the disruption analysis quantifies their impact using regression models. These models estimate how much service utilization changed during the flagged disruption periods by adjusting for long-term trends and seasonal variations.  For each indicator, we estimate:  $$ Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\sum_{m=1}^{12} \\gamma_m \\cdot \\text{month}_m + \\beta_2 \\cdot \\text{tagged} + \\epsilon_{it} $$  where: - $Y_{it}$ is the observed service volume, - $\\text{date}$ captures time trends, - $\\text{month}_m$ controls for seasonality, - $\\text{tagged}$ is the disruption dummy (from the control chart analysis), - $\\epsilon_{it}$ is the error term.  The coefficient on `tagged` ($\\beta_2$) measures the relative change in service utilization during flagged disruptions. Separate regressions are run at the national, province and district levels to assess the impact across different geographic scales.  #### Country-wide Regression  The country-wide regression estimates how service utilization changes at the national level when a disruption occurs. Instead of analyzing individual provinces or districts separately, this model considers the entire country's data in a single regression. Errors are clustered at the lowest available geographic level (`lowest_geo_level`), typically districts.  **Model Specification:**  $$Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\sum_{m=1}^{12} \\gamma_m \\cdot \\text{month} + \\beta_2 \\cdot \\text{tagged} + \\epsilon_{it}$$  Where: - $Y_{it}$ = volume (e.g., number of deliveries) - $\\text{date}$ = time trend - $\\text{month}_m$ = controls for seasonality (factor variable) - $\\text{tagged}$ = dummy for disruption period - $\\epsilon_{it}$ = error term, clustered at the district level (`admin_area_3`)  #### Province-level Regression  The province-level disruption regression estimates how service utilization changes at the province level when a disruption occurs. Unlike the country-wide model, this approach runs separate regressions for each province to capture regional variations.  **Model specification:**  $$Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\sum_{m=1}^{12} \\gamma_m \\cdot \\text{month} + \\beta_2 \\cdot \\text{tagged} + \\alpha_{\\text{province}} + \\epsilon_{it}$$  Where: - $Y_{it}$ = volume (e.g., number of deliveries) - $\\text{date}$ = time trend - $\\text{month}_m$ = controls for seasonality (factor variable) - $\\text{tagged}$ = dummy for disruption period - $\\alpha_{\\text{province}}$ = province fixed effects - $\\epsilon_{it}$ = error term, clustered at the district level  #### District-level Regression  The district-level disruption regression estimates how service utilization changes at the district level when a disruption occurs. This approach runs separate regressions for each district to capture localized variations.  **Model specification:**  $$Y_{it} = \\beta_0 + \\beta_1 \\cdot \\text{date} + \\sum_{m=1}^{12} \\gamma_m \\cdot \\text{month} + \\beta_2 \\cdot \\text{tagged} + \\alpha_{\\text{district}} + \\epsilon_{it}$$  Where: - $Y_{it}$ = volume (e.g., number of deliveries) - $\\text{date}$ = time trend - $\\text{month}_m$ = controls for seasonality (factor variable) - $\\text{tagged}$ = dummy for disruption period - $\\alpha_{\\text{district}}$ = district fixed effects - $\\epsilon_{it}$ = error term  #### Regression Outputs  Each regression level produces the following outputs:  - **Expected values (`expect_admin_area_*`)**: Predicted service volume adjusted for seasonality and trends. - **Disruption effect (`b_admin_area_*`)**: Estimated relative change during disruptions: $$ b_{\\text{admin\\_area\\_*}} = -\\frac{\\text{diff mean}}{\\text{predict mean}} $$  - **Trend coefficient (`b_trend_admin_area_*`)**: Reflects long-term trend.   - Positive = increasing service use   - Negative = declining service use   - Near zero = stable trend - **P-value (`p_admin_area_*`)**: Measures statistical significance of the disruption effect.   - Lower values = stronger evidence of true disruption  ### Statistical Methods Used  **Robust Regression (`MASS::rlm`)**: - Uses iteratively reweighted least squares (IRLS) - Minimizes influence of outliers and extreme values - More resistant to model misspecification than ordinary least squares - Default: Huber weighting with maximum 100 iterations  **MAD (Median Absolute Deviation)**: - Robust measure of scale/variability - Formula: `MAD = median(|x - median(x)|)` - More resistant to outliers than standard deviation - Used to standardize residuals for anomaly detection  **Panel Regression (`fixest::feols`)**: - Fixed-effects estimation with clustered standard errors - Accounts for within-group correlation in errors - More efficient than traditional panel regression packages - Handles unbalanced panels gracefully  **Geographic Clustering**: - Regressions use clustered standard errors at the lowest available geographic level - This accounts for within-area correlation in service delivery patterns - Example: Country-wide model clusters by district, province model clusters by district - Prevents underestimation of standard errors and false positives   Detailed Analysis Steps  ### PART 1 - Control Chart Analysis  #### Step 1: Prepare the Data  - Load raw HMIS service utilization data. - Merge in outlier flags (`outlier_flag`) by facility \u00d7 indicator \u00d7 month. - Remove rows flagged as outliers (`outlier_flag == 1`). - Create a `date` variable from `period_id` and extract `year` and `month`. - Create a unique `panelvar` for each geographic area-indicator combination. - Aggregate data to the specified geographic level by summing `count_model` (based on `SELECTEDCOUNT`) by date. - Fill in missing months within each panel to ensure continuity. - Fill missing metadata using forward and backward fill.  #### Step 2: Filter Out Low-Volume Months  - Compute the global mean service volume for each `panelvar`. - If `count_original` is &lt;50% of the global mean, drop the value by setting it to `NA`.  #### Step 3: Apply Regression and Smoothing  Estimate expected service volume using robust regression, then smooth the predicted trend.  - Fit robust regression (`rlm`) for each panel using one of three model specifications based on data availability. - Apply rolling median smoothing to predictions using window size `SMOOTH_K`. - If smoothing is not possible (e.g., at series edges), fallback to model predictions. - Calculate residuals: actual - smoothed - Standardize residuals using MAD  This standardized control variable is used to detect anomalies in Step 4.  #### Step 4: Tag Disruptions  Apply rule-based tagging to identify potential disruptions. Each rule is governed by user-defined parameters that can be tuned for sensitivity:  - **Sharp Disruptions**: Tag if `|robust_control| \u2265 MADS_THRESHOLD` - **Sustained Drops**: Tag final month if 3 consecutive months have mild deviations and final month exceeds threshold - **Sustained Dips**: Tag entire sequence if `count_original &lt; DIP_THRESHOLD \u00d7 count_smooth` for 3+ months - **Sustained Rises**: Tag entire sequence if `count_original &gt; RISE_THRESHOLD \u00d7 count_smooth` for 3+ months - **Missing Data**: Tag if 2+ of past 3 months are missing or zero - **Recent Tail Override**: Automatically tag all months in last 6 months of data  A month is assigned `tagged = 1` if any of the above conditions are met. Tagged records are saved in `M3_chartout.csv` and passed to the disruption analysis.  ### PART 2 - Disruption Analysis  #### Step 1: Data preparation  - The `M3_chartout` dataset is merged with the main dataset to integrate the `tagged` variable, which identifies flagged disruptions. - The lowest available geographic level (`lowest_geo_level`) is identified for clustering, based on the highest-resolution `admin_area_*` column available.  #### Step 2: Country-wide regression  For each `indicator_common_id`, estimate the national-level model with errors clustered at district level.  - A panel regression model is applied at the country-wide level, estimating the expected service volume (`expect_admin_area_1`) for each indicator. - The model adjusts for historical trends and seasonal variations. - If a disruption (`tagged` = 1) is detected, the predicted service volume is adjusted by subtracting the estimated effect of the disruption to isolate its impact.  #### Step 3: Province-level regression  For each `indicator_common_id` \u00d7 `admin_area_2` combination, estimate province-specific models with errors clustered at district level.  - A fixed effects panel regression model is applied at the province level, estimating expected service volume (`expect_admin_area_2`) while controlling for province-specific factors. - The model adjusts for historical trends and seasonal variations. - If a disruption is detected, predicted volumes are adjusted to isolate the impact.  #### Step 4: District-level regression (if enabled)  For each `indicator_common_id` \u00d7 `admin_area_3` combination, estimate district-specific models with errors clustered at ward level.  - A fixed effects panel regression model is applied at the district level, estimating expected service volume (`expect_admin_area_3`). - The model adjusts for historical trends and seasonal variations. - If a disruption is detected, predicted volumes are adjusted to isolate the impact.  #### Step 5: Prepare Outputs for Visualization  Once expected values have been calculated for each level (country, province, district), the pipeline compares predicted and actual values to assess the magnitude of disruption.  For each month and indicator, the pipeline calculates:  - **Absolute and percentage difference** between predicted and actual values: $$ \\text{diff\\_percent} = 100 \\times \\frac{\\text{predicted} - \\text{actual}}{\\text{predicted}} $$  - A configurable threshold parameter `DIFFPERCENT` (default: `10`) is used to determine when a disruption is significant.    If the percentage difference exceeds \u00b110%, the expected (predicted) value is retained and used for plotting and summary statistics. Otherwise, the actual observed value is used.    This ensures that minor fluctuations do not lead to artificial disruptions in the visualization, while meaningful deviations are preserved.  - The final adjusted value for plotting is stored in a field such as `count_expected_if_above_diff_threshold`.    This value reflects either:   - The predicted count (if deviation &gt; threshold), or   - The actual count (if within acceptable range).  This logic is applied consistently across all admin levels. These adjusted values are then exported as part of the final output files for each level.   Code Examples  ### Basic Execution  <pre><code># Set working directory\nsetwd(\"/path/to/project\")\n\n# Ensure Module 1 and 2 outputs exist\nstopifnot(file.exists(\"M1_output_outliers.csv\"))\nstopifnot(file.exists(\"M2_adjusted_data.csv\"))\n\n# Run Module 3\nsource(\"03_module_service_utilization.R\")\n</code></pre>  ### Example Use Case 1: National COVID-19 Impact Assessment  **Objective**: Quantify disruptions in maternal health services during pandemic  **Configuration**: <pre><code>SELECTEDCOUNT &lt;- \"count_final_both\"\nMADS_THRESHOLD &lt;- 1.5  # Standard sensitivity\nDIP_THRESHOLD &lt;- 0.90  # Flag 10% drops\nRUN_DISTRICT_MODEL &lt;- TRUE  # District-level estimates\n</code></pre>  **Analysis Approach**: 1. Run control chart to identify disrupted months 2. Examine `M3_chartout.csv` for temporal patterns 3. Use `M3_all_indicators_shortfalls_admin_area_1.csv` for national summary 4. Use `M3_all_indicators_shortfalls_admin_area_3.csv` for district hotspots  **Expected Outputs**: - Sharp disruptions during lockdown months - Sustained dips in antenatal care, deliveries - Possible sustained rises in missed appointments  ### Example Use Case 2: Routine Monitoring Dashboard  **Objective**: Monthly monitoring of all health services for early warning  **Configuration**: <pre><code>SELECTEDCOUNT &lt;- \"count_final_completeness\"  # Adjust for reporting gaps\nMADS_THRESHOLD &lt;- 1.0  # High sensitivity\nDIP_THRESHOLD &lt;- 0.95  # Flag even small drops (5%)\nRUN_DISTRICT_MODEL &lt;- FALSE  # Faster runtime for frequent updates\n</code></pre>  **Analysis Approach**: 1. Run monthly after new data available 2. Filter `M3_chartout.csv` for `tagged == 1` in recent months 3. Investigate flagged indicator-geography combinations 4. Generate automated alerts for stakeholders  **Expected Outputs**: - Early detection of emerging disruptions - Timely identification of data quality issues - Geographic targeting for supportive supervision  ### Example Use Case 3: Post-Conflict Recovery Assessment  **Objective**: Assess recovery of health services after conflict ends  **Configuration**: <pre><code>SELECTEDCOUNT &lt;- \"count_final_both\"\nMADS_THRESHOLD &lt;- 2.0  # Conservative (major disruptions only)\nDIP_THRESHOLD &lt;- 0.80  # Flag 20% drops\nSMOOTH_K &lt;- 11  # Heavy smoothing to identify stable trends\nRUN_DISTRICT_MODEL &lt;- TRUE  # District-level for targeted support\n</code></pre>  **Analysis Approach**: 1. Focus on post-conflict period in outputs 2. Compare `count_sum` to `count_expect_sum` over time 3. Track `shortfall_percent` trends toward zero (recovery) 4. Identify districts with persistent disruptions  **Expected Outputs**: - Quantified service volume gaps during conflict - Recovery trajectories by service type - Geographic prioritization for rebuilding efforts   Troubleshooting  ### Common Issues and Solutions  **Issue**: Script crashes with \"out of memory\" error  **Solutions**: - Reduce batch sizes (e.g., `BATCH_SIZE_IND &lt;- 3`) - Set `RUN_DISTRICT_MODEL &lt;- FALSE` - Set `RUN_ADMIN_AREA_4_ANALYSIS &lt;- FALSE` - Close other applications - Run on machine with more RAM  ---  **Issue**: Warning \"model failed to converge\"  **Explanation**: Robust regression didn't fully converge within 100 iterations  **Impact**: Usually minimal - partial convergence often sufficient  **Solutions**: - Check data quality for that panel - Increase `maxit` parameter in `rlm()` call (line 229, 247) - Generally safe to ignore if only a few panels affected  ---  **Issue**: Many empty rows in output files  **Explanation**: Insufficient data for certain indicator-geography combinations  **Solutions**: - Expected behavior for sparse indicators - Filter outputs to non-missing values - Consider aggregating to higher geographic level  ---  **Issue**: All recent months flagged as disruptions  **Explanation**: Automatic flagging of last 6 months  **Purpose**: Ensures recent trends reviewed even without strong statistical evidence  **Solutions**: - Expected behavior, not a bug - Review recent months manually - Adjust `last_6_months` logic if needed (line 333)  ---  **Issue**: `tagged` variable dropped from regression  **Message**: Variable automatically set to 0  **Explanation**: No variation in `tagged` within that panel (all 0 or all 1)  **Solutions**: - Expected in panels with no disruptions or constant disruption - Not an error - disruption effect correctly set to 0  ---  **Issue**: Temporary files remain after run  **Cause**: Script crashed before cleanup  **Solutions**: - Delete manually: `M3_temp_*.csv` - Or re-run script (automatic cleanup at start)  ---  **Issue**: Very different results at different geographic levels  **Explanation**: Different geographic aggregation captures different patterns  **Example**: National trend may be stable while some districts have large disruptions  **Solutions**: - Expected behavior - not a bug - Use appropriate level for your research question - Cross-check patterns across levels for robustness   Usage Notes  ### Interpretation Guidelines  **Disruption Effects (b_admin_area_*)**: - Negative values indicate service volume shortfalls during disrupted periods - Positive values indicate service volume surpluses during disrupted periods - Values closer to zero indicate smaller disruption impacts  **P-values (p_admin_area_*)**: - Values &lt; 0.05 suggest statistically significant disruptions - Values &gt; 0.05 may indicate normal variation rather than true disruptions  **Trend Coefficients (b_trend_admin_area_*)**: - Positive values indicate increasing service utilization over time - Negative values indicate declining service utilization over time - Values near zero indicate stable utilization patterns  ### Performance Considerations  **Runtime Factors**: - **Number of indicators**: Linear scaling - **Number of geographic units**: Linear scaling within each level - **Time series length**: Minimal impact (efficient regression) - **Geographic detail**: Exponential scaling (many more units at finer levels)  **Estimated Runtimes** (example dataset: 50 indicators, 100 districts): - Country-wide + Province models: ~5-10 minutes - Add District models: ~30-60 minutes - Add Ward models: Several hours (depends on number of wards)  **Optimization Strategies**: - Set `RUN_DISTRICT_MODEL = FALSE` for faster execution (skips district level) - Set `RUN_ADMIN_AREA_4_ANALYSIS = FALSE` (default) to avoid ward-level analysis - Reduce `SMOOTH_K` for faster rolling median calculation - Use `SELECTEDCOUNT = \"count_final_none\"` to avoid completeness adjustments  ### Data Processing Details  **Memory Management**: - Uses `data.table` for efficient operations on large datasets - Batch processing: Results saved to disk periodically - Progressive cleanup: Objects deleted when no longer needed - Temporary files enable processing datasets larger than RAM  **Batch Sizes** (tunable for memory constraints): - Control chart: 100 panels per batch - Indicators: 5 indicators per batch - Provinces: 20 results per batch - Districts: 15 results per batch - Admin area 4: 10 results per batch  **Missing Data Handling**: 1. Missing months filled via `tidyr::complete()` 2. Forward/backward fill for metadata 3. Linear interpolation (`zoo::na.approx`) for count values 4. Maximum gap: Unlimited (rule = 2 extends endpoints)  ### Model Fallback Logic  The control chart analysis uses adaptive model selection based on data availability:  **Full Model** (requires \u226512 obs AND &gt;12 unique dates): <pre><code>count ~ month_factor + as.numeric(date)\n</code></pre> Accounts for both seasonality and linear trend  **Trend-Only Model** (requires \u226512 obs): <pre><code>count ~ as.numeric(date)\n</code></pre> Accounts for linear trend only (insufficient data for seasonality)  **Median Fallback** (&lt;12 observations): <pre><code>count_predict = median(count)\n</code></pre> Uses global median when insufficient data for regression  **Convergence Checks**: - Models checked for convergence status - Warnings issued for non-convergent models - Non-convergent models still used (partial convergence often sufficient)  ### Quality Assurance  **Data Cleaning**: - Outliers removed prior to control chart analysis (based on Module 1 flags) - Low-volume months (&lt;50% of mean) excluded to improve model stability - Predictions bounded at zero (counts cannot be negative)  **Automatic Flagging**: - Recent months (last 6 months) automatically flagged to ensure current disruptions captured - Prevents missing ongoing disruptions due to insufficient deviation from trend  **Robustness Checks**: - Model coefficients checked for `NA` values before use - If `tagged` variable dropped from model (no variation), disruption effect set to 0 - P-values calculated only when valid standard errors available  **Edge Case Handling**: - Single-cluster panels: No clustering applied (would fail) - Insufficient data: Skip analysis for that panel/level - Missing predictions: Filled with original values where possible  ### Workflow Integration  This module is **Module 3** in the FASTR analytical pipeline:  **Prerequisites**: 1. **Module 0**: Data preparation and harmonization 2. **Module 1**: Data Quality Assessment (generates `M1_output_outliers.csv`) 3. **Module 2**: Data Quality Adjustments (generates `M2_adjusted_data.csv`)  **Downstream Modules**: 4. **Module 4**: Coverage Estimates (may use disruption flags) 5. **Module 5**: Visualization and reporting (uses all M3 outputs)  ### Dependencies  **R Packages Required**: - `data.table`: Efficient data manipulation - `lubridate`: Date handling - `zoo`: Rolling statistics and interpolation - `MASS`: Robust regression (rlm) - `fixest`: Fixed-effects panel regression - `dplyr`: Data manipulation - `tidyr`: Data tidying   References and Further Reading  **Statistical Process Control**: - Montgomery, D. C. (2009). *Introduction to Statistical Quality Control*. Wiley. - Shewhart, W. A. (1931). *Economic Control of Quality of Manufactured Product*. Van Nostrand.  **Robust Regression**: - Huber, P. J. (1981). *Robust Statistics*. Wiley. - Maronna, R. A., Martin, R. D., &amp; Yohai, V. J. (2006). *Robust Statistics: Theory and Methods*. Wiley.  **Panel Data Methods**: - Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data*. MIT Press. - Cameron, A. C., &amp; Miller, D. L. (2015). \"A Practitioner's Guide to Cluster-Robust Inference.\" *Journal of Human Resources*, 50(2), 317-372.  **Health Service Disruptions**: - WHO (2020). *Pulse survey on continuity of essential health services during the COVID-19 pandemic*. - Arsenault, C., et al. (2022). \"COVID-19 and resilience of healthcare systems in ten countries.\" *Nature Medicine*, 28, 1314-1324.   <p>Last updated: 2025 January Module version: 3.0 Contact: FASTR Project Team</p>"},{"location":"04_module_coverage_estimates_documentation/","title":"Module 4: Coverage Estimates","text":""},{"location":"04_module_coverage_estimates_documentation/#1-overview-what-why","title":"1. Overview (What &amp; Why)","text":""},{"location":"04_module_coverage_estimates_documentation/#what-does-this-module-do","title":"What does this module do?","text":"<p>This module estimates health service coverage by integrating three key data sources: adjusted health service volumes from HMIS (Module 2), population projections from the United Nations, and household survey data from MICS/DHS. It answers the fundamental question: \"What percentage of the target population received this health service?\"</p> <p>The module operates in two distinct parts. Part 1 calculates target population sizes (denominators) using multiple methods and automatically selects the best option for each health indicator by comparing results against survey benchmarks. Part 2 allows users to refine these selections, choose specific denominators based on programmatic knowledge, and project survey estimates forward in time using administrative data trends to fill gaps where surveys are unavailable.</p> <p>Together, these parts transform raw service counts into meaningful coverage estimates that can be analyzed for trends, compared across regions, and used for policy decisions.</p>"},{"location":"04_module_coverage_estimates_documentation/#why-is-it-needed-in-the-fastr-pipeline","title":"Why is it needed in the FASTR pipeline?","text":"<p>Understanding coverage is essential for monitoring health system performance and equity. While Module 2 provides adjusted service volumes, these numbers alone don't tell us whether services are reaching their intended populations. Coverage estimates provide context by comparing service delivery to population need.</p> <p>This module addresses key challenges in coverage estimation: - Multiple data sources: Integrates HMIS data (continuous but may have quality issues) with survey data (high quality but infrequent) - Denominator uncertainty: Different methods for estimating target populations may yield different results; the module systematically evaluates options - Temporal gaps: Surveys occur every 3-5 years; the module projects estimates for intervening years using administrative trends - Subnational analysis: Enables coverage monitoring at national, provincial, and district levels</p>"},{"location":"04_module_coverage_estimates_documentation/#quick-summary","title":"Quick Summary","text":"Component Details Inputs M2_adjusted_data (national &amp; subnational) from Module 2Survey data (MICS/DHS) from GitHub repositoryPopulation data (UN WPP) from GitHub repository Outputs M4_denominators (national, admin2, admin3) - calculated target populationsM4_combined_results (national, admin2, admin3) - coverage estimates with all denominatorsM5_coverage_estimation (national, admin2, admin3) - final coverage with projections Purpose Estimate health service coverage by comparing service volumes to target populations, validated against survey benchmarks"},{"location":"04_module_coverage_estimates_documentation/#part-1-and-part-2-explained","title":"Part 1 and Part 2 Explained","text":"<p>Part 1: Denominator Calculation and Selection - Calculates target populations (denominators) using multiple approaches: HMIS-based (from ANC1, delivery, BCG, Penta1) and population-based (UN WPP) - Compares coverage estimates from each denominator against survey data - Automatically selects the \"best\" denominator for each indicator by minimizing error - Outputs: Denominator datasets and combined results showing all options</p> <p>Part 2: Denominator Selection and Survey Projection - Allows users to override automatic selections and choose specific denominators - Calculates year-over-year coverage trends from administrative data - Projects survey estimates forward using HMIS trends to fill temporal gaps - Outputs: Final coverage estimates combining HMIS, survey, and projected values</p>"},{"location":"04_module_coverage_estimates_documentation/#2-how-it-works","title":"2. How It Works","text":""},{"location":"04_module_coverage_estimates_documentation/#high-level-workflow","title":"High-Level Workflow","text":""},{"location":"04_module_coverage_estimates_documentation/#part-1-denominator-calculation-and-selection","title":"Part 1: Denominator Calculation and Selection","text":"<p>Step 1: Load and Prepare Data Sources The module begins by loading three data sources and ensuring they're compatible. HMIS data is aggregated from monthly to annual totals. Survey data is harmonized (DHS prioritized over MICS) and forward-filled to create continuous time series. Population data is filtered to the target country.</p> <p>Step 2: Calculate Multiple Denominator Options For each health indicator, the module calculates several possible target populations: - Service-based denominators: Using HMIS volumes divided by survey coverage (e.g., if 10,000 women received ANC1 and survey says coverage is 80%, estimated pregnancies = 10,000/0.80 = 12,500) - Population-based denominators: Using UN population projections and birth rates - Each denominator is adjusted for demographic factors (pregnancy loss, stillbirths, mortality rates) to match the indicator's target age group</p> <p>Step 3: Calculate Coverage for Each Denominator The module computes coverage by dividing the service volume by each denominator option. This produces multiple coverage estimates per indicator, each based on a different population assumption.</p> <p>Step 4: Compare to Survey Benchmarks Each coverage estimate is compared to survey data using squared error calculation. The survey serves as the \"truth\" benchmark since it's based on representative household sampling.</p> <p>Step 5: Select the Best Denominator The denominator producing the lowest error (closest match to survey) is automatically selected as \"best.\" The selection prioritizes HMIS-based denominators over population projections to ensure data is driven by observed service delivery.</p> <p>Step 6: Generate Outputs The module saves denominator datasets for transparency and combined results files showing coverage from all denominators plus the selected best option.</p> <p>Step 7: Repeat for Subnational Levels If subnational data is available, the process repeats for administrative level 2 (e.g., provinces) and level 3 (e.g., districts), with fallback mechanisms to handle missing local survey data.</p>"},{"location":"04_module_coverage_estimates_documentation/#part-2-denominator-selection-and-survey-projection","title":"Part 2: Denominator Selection and Survey Projection","text":"<p>Step 1: User Configuration Users review Part 1 results and configure denominator selections for each indicator. Options include using the automatic \"best\" selection or overriding with a specific denominator based on programmatic knowledge.</p> <p>Step 2: Filter to Selected Denominators The module filters Part 1's combined results to include only user-selected denominators, creating a focused dataset for analysis.</p> <p>Step 3: Calculate Coverage Trends Year-over-year changes (deltas) in HMIS-based coverage are calculated. This shows whether coverage is increasing, decreasing, or stable over time.</p> <p>Step 4: Identify Survey Baseline For each geographic area and indicator, the most recent survey observation is identified as the baseline anchor point for projections.</p> <p>Step 5: Project Survey Estimates Forward The module extends survey coverage estimates into years without surveys by applying HMIS trends. The projection uses: Last survey value + (Current year HMIS coverage - Survey year HMIS coverage). This preserves the survey calibration while incorporating observed trends.</p> <p>Step 6: Combine All Estimates The final output merges three types of estimates: - HMIS-based coverage: Direct calculation from service volumes and selected denominators - Original survey values: Actual household survey observations - Projected survey coverage: Survey estimates extended using HMIS trends</p> <p>Step 7: Save Final Outputs Results are saved with standardized column structures for each administrative level, ready for visualization and reporting.</p>"},{"location":"04_module_coverage_estimates_documentation/#visual-workflow-diagram","title":"Visual Workflow Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         PART 1: DENOMINATOR                      \u2502\n\u2502                    CALCULATION AND SELECTION                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 HMIS Data    \u2502     \u2502 Survey Data  \u2502     \u2502 Population   \u2502\n  \u2502 (Module 2)   \u2502     \u2502 (MICS/DHS)   \u2502     \u2502 Data (UN WPP)\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                    \u2502                    \u2502\n         \u25bc                    \u25bc                    \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502          Process and Harmonize Data                 \u2502\n  \u2502  \u2022 Aggregate monthly \u2192 annual                       \u2502\n  \u2502  \u2022 Prioritize DHS over MICS                         \u2502\n  \u2502  \u2022 Forward-fill survey gaps                         \u2502\n  \u2502  \u2022 Filter population to country                     \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502      Calculate Multiple Denominator Options          \u2502\n  \u2502                                                       \u2502\n  \u2502  HMIS-Based:              Population-Based:          \u2502\n  \u2502  \u2022 From ANC1              \u2022 From birth rate          \u2502\n  \u2502  \u2022 From Delivery          \u2022 From total population    \u2502\n  \u2502  \u2022 From SBA               \u2022 From under-1 population  \u2502\n  \u2502  \u2022 From BCG                                          \u2502\n  \u2502  \u2022 From Penta1                                       \u2502\n  \u2502                                                       \u2502\n  \u2502  [Apply demographic adjustments to each]             \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Calculate Coverage for Each Denominator            \u2502\n  \u2502   Coverage = (Service Volume / Denominator) \u00d7 100    \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502      Compare Each Coverage to Survey Benchmark       \u2502\n  \u2502      Error = (HMIS Coverage - Survey Coverage)\u00b2      \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502         Select Best Denominator                      \u2502\n  \u2502         (Minimum error, prefer HMIS-based)           \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502                Save Outputs                          \u2502\n  \u2502  \u2022 M4_denominators (all options)                     \u2502\n  \u2502  \u2022 M4_combined_results (coverage + best selection)   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       \u25bc              PART 2: DENOMINATOR         \u2502\n\u2502                                  SELECTION AND PROJECTION        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502          User Configures Denominator Selection       \u2502\n  \u2502          (Use \"best\" or specify for each indicator)  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502      Filter Part 1 Results to Selected Denominators  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502         Calculate Year-Over-Year Coverage Deltas     \u2502\n  \u2502         \u0394 Coverage = Coverage(t) - Coverage(t-1)     \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502    Identify Most Recent Survey as Baseline           \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502         Project Survey Estimates Forward             \u2502\n  \u2502  Projection = Last Survey + (HMIS_now - HMIS_then)  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502              Combine Final Results                   \u2502\n  \u2502  \u2022 HMIS-based coverage                               \u2502\n  \u2502  \u2022 Original survey values                            \u2502\n  \u2502  \u2022 Projected survey coverage                         \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502                Save Final Outputs                    \u2502\n  \u2502        M5_coverage_estimation (by admin level)       \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"04_module_coverage_estimates_documentation/#key-decision-points","title":"Key Decision Points","text":"<p>1. Which denominator to use? Part 1 automatically selects based on alignment with survey data, but users can override in Part 2. The choice affects whether coverage is anchored to service delivery patterns (HMIS-based) or demographic projections (population-based).</p> <p>2. How to handle survey gaps? Surveys occur infrequently (every 3-5 years). The module forward-fills survey values in Part 1 (assumes constant coverage until next survey) and uses projection in Part 2 (incorporates HMIS trends).</p> <p>3. Should subnational analysis use local or national survey data? When local survey data is unavailable, the module falls back to national values. This assumes national coverage rates apply locally, which may not hold in all contexts.</p> <p>4. How to adjust denominators for different target populations? Each health indicator targets a specific population (e.g., pregnant women for ANC, infants for vaccines). The module applies sequential demographic adjustments (pregnancy loss, stillbirths, mortality) to align denominators with target populations.</p>"},{"location":"04_module_coverage_estimates_documentation/#what-happens-to-the-data","title":"What Happens to the Data","text":"<p>The transformation flow:</p> <p>Input: Monthly service counts by facility \u2192 Aggregate \u2192 Annual service volumes by area</p> <p>Input: Survey observations (scattered years) \u2192 Harmonize &amp; forward-fill \u2192 Continuous coverage time series</p> <p>Input: Population projections \u2192 Filter &amp; extract \u2192 Target population estimates</p> <p>Combine: Service volumes \u00f7 Survey coverage \u2192 Calculate \u2192 HMIS-based denominators</p> <p>Combine: Service volumes \u00f7 Each denominator \u2192 Calculate \u2192 Multiple coverage estimates</p> <p>Compare: Coverage estimates vs. Survey benchmarks \u2192 Select \u2192 Best denominator per indicator</p> <p>Project: Last survey + HMIS trends \u2192 Extend \u2192 Coverage estimates for all years</p> <p>Output: Comprehensive dataset with HMIS coverage, survey values, projections, and metadata</p>"},{"location":"04_module_coverage_estimates_documentation/#3-detailed-reference","title":"3. Detailed Reference","text":""},{"location":"04_module_coverage_estimates_documentation/#part-1-denominator-calculation-technical-details","title":"Part 1: Denominator Calculation (Technical Details)","text":"Configuration Parameters  The module begins with several configurable parameters that control the analysis:  <pre><code>COUNTRY_ISO3 &lt;- \"AFG\"                          # ISO3 country code\nSELECTED_COUNT_VARIABLE &lt;- \"count_final_both\"  # Which adjusted count to use\nANALYSIS_LEVEL &lt;- \"NATIONAL_PLUS_AA2\"          # Geographic scope\n</code></pre>  **Analysis Level Options:** - `NATIONAL_ONLY`: National-level analysis only - `NATIONAL_PLUS_AA2`: National + administrative area 2 (e.g., provinces) - `NATIONAL_PLUS_AA2_AA3`: National + admin area 2 + admin area 3 (e.g., districts)  **Demographic Adjustment Rates:** <pre><code>PREGNANCY_LOSS_RATE &lt;- 0.03      # 3% pregnancy loss\nTWIN_RATE &lt;- 0.015               # 1.5% twin births\nSTILLBIRTH_RATE &lt;- 0.02          # 2% stillbirths\nP1_NMR &lt;- 0.039                  # Neonatal mortality rate\nP2_PNMR &lt;- 0.028                 # Post-neonatal mortality rate\nINFANT_MORTALITY_RATE &lt;- 0.063   # Infant mortality rate\nUNDER5_MORTALITY_RATE &lt;- 0.103   # Under-5 mortality rate\n</code></pre>  **Count Variable Options:** - `count_final_none`: No adjustments (raw reported data) - `count_final_outlier`: Outlier adjustment only - `count_final_completeness`: Completeness adjustment only - `count_final_both`: Both adjustments **(recommended)**   Input Data Sources  Part 1 integrates three primary data sources:  **1. HMIS Adjusted Data** (from Module 2) - National: `M2_adjusted_data_national.csv` - Subnational: `M2_adjusted_data_admin_area.csv` - Contains service volumes by indicator, area, and time period  **2. Survey Data** (DHS/MICS) - Source: GitHub repository (unified survey dataset) - Provides coverage benchmarks for comparison - DHS data prioritized over MICS when both available  **3. Population Data** (UN WPP) - Source: GitHub repository - Provides population-based denominators - Includes total population, births, under-1, and under-5 populations  **Additional Data Context:**  **Population Projections (UN WPP)** Sourced from the United Nations World Population Prospects, these estimates provide age-specific and total population figures used to calculate denominators for coverage estimates. These projections account for demographic trends, including fertility, mortality, and migration.  **Survey Data - MICS** The Multiple Indicator Cluster Surveys (MICS), conducted by UNICEF, provide household survey-based estimates for key health indicators, including coverage of maternal and child health services.  **Survey Data - DHS** The Demographic and Health Surveys (DHS), conducted by USAID, provide survey data on health service utilization, including immunization rates and maternal care coverage.   Core Functions Documentation  #### 1. `process_hmis_adjusted_volume()`  **Purpose**: Prepares HMIS data for denominator calculation  **Input**: - Adjusted volume data from Module 2 - Selected count variable (e.g., `count_final_both`)  **Processing**: - Aggregates monthly data to annual totals - Counts number of reporting months per year - Pivots data to wide format (one column per indicator)  **Output**: - `annual_hmis`: Annual service counts by area and year - `hmis_countries`: List of countries in dataset - `hmis_iso3`: ISO3 code(s) present  **Example Structure**: <pre><code>admin_area_1  admin_area_2  year  countanc1  countdelivery  ...  nummonth\nAfghanistan   Kabul         2020  12500      10200          ...  12\nAfghanistan   Kabul         2021  13000      10500          ...  11\n</code></pre>  #### 2. `process_survey_data()`  **Purpose**: Harmonizes and extends survey data for use as coverage benchmarks  **Input**: - Survey data (DHS/MICS) - HMIS country names and ISO3 codes - Optional national reference (for subnational fallback)  **Key Processing Steps**:  1. **Harmonization**    - Recodes indicator names (e.g., `polio1` \u2192 `opv1`, `vitamina` \u2192 `vitaminA`)    - Normalizes source labels (`dhs`, `mics`)    - Filters by country and date range  2. **Source Prioritization**    - When both DHS and MICS exist for same year/area/indicator    - DHS is selected preferentially    - Preserves source details for transparency  3. **Fallback Logic**    - If `sba` missing, uses `delivery` values    - If `pnc1_mother` missing, uses `pnc1` values    - Subnational areas use national values when local data unavailable (for BCG, Penta1, Penta3)  4. **Forward-Filling**    - Creates complete time series for each area    - Carries forward last observed value (`na.locf`)    - Creates \"carry\" columns (e.g., `anc1carry`, `bcgcarry`)  **Output**: - `carried`: Extended survey data with forward-filled values - `raw`: Raw survey observations (wide format) - `raw_long`: Raw survey observations (long format) with source details  #### 3. `process_national_population_data()`  **Purpose**: Prepares UN WPP population estimates for denominator calculation  **Input**: - Population estimates (UN WPP) - HMIS country identifiers  **Processing**: - Filters to national level and target country - Extracts key population indicators:   - `crudebr_unwpp`: Crude birth rate   - `poptot_unwpp`: Total population   - `totu1pop_unwpp`: Under-1 population  **Output**: - `wide`: Population indicators in wide format - `raw_long`: Population data in long format with source tracking  #### 4. `calculate_denominators()`  **Purpose**: Calculates all possible denominators from HMIS and population data  This is the core function that generates multiple denominator estimates.  **Input**: - `hmis_data`: Annual service counts - `survey_data`: Survey reference values (carried forward) - `population_data`: UN WPP estimates (national only)  **Denominator Types Calculated**:  **A. Service-Based Denominators** (using HMIS numerator \u00f7 survey coverage):  1. **From ANC1**:    - `danc1_pregnancy`: Estimated pregnancies    - `danc1_delivery`: Estimated deliveries    - `danc1_birth`: Estimated births (live + stillbirths)    - `danc1_livebirth`: Estimated live births    - `danc1_dpt`: Eligible for DPT (adjusted for neonatal mortality)    - `danc1_measles1`: Eligible for MCV1    - `danc1_measles2`: Eligible for MCV2  2. **From Delivery**:    - `ddelivery_livebirth`, `ddelivery_birth`, `ddelivery_pregnancy`    - `ddelivery_dpt`, `ddelivery_measles1`, `ddelivery_measles2`  3. **From SBA** (Skilled Birth Attendance):    - Same structure as delivery denominators    - `dsba_livebirth`, `dsba_birth`, `dsba_pregnancy`    - `dsba_dpt`, `dsba_measles1`, `dsba_measles2`  4. **From BCG** (national only):    - `dbcg_pregnancy`, `dbcg_livebirth`, `dbcg_dpt`  5. **From Penta1**:    - `dpenta1_dpt`, `dpenta1_measles1`, `dpenta1_measles2`  **B. Population-Based Denominators** (national only):  - `dwpp_pregnancy`: From crude birth rate \u00d7 total population \u00f7 (1 + twin rate) - `dwpp_livebirth`: From crude birth rate \u00d7 total population - `dwpp_dpt`: Under-1 population - `dwpp_measles1`: Under-1 population adjusted for neonatal mortality - `dwpp_measles2`: Further adjusted for post-neonatal mortality  **C. Vitamin A and Full Immunization**:  For each livebirth denominator, additional denominators are automatically created: - `d*_vitaminA`: Livebirth \u00d7 (1 - U5MR) \u00d7 4.5 (children 6-59 months) - `d*_fully_immunized`: Livebirth \u00d7 (1 - IMR)  **Adjustment for Incomplete Reporting**: When `nummonth &lt; 12`, population-based denominators are scaled: <pre><code>denominator_adjusted = denominator \u00d7 (nummonth / 12)\n</code></pre>  **Output**: Data frame with all calculated denominators plus original HMIS and survey data  #### 5. `classify_source_type()`  **Purpose**: Categorizes denominators to prevent circular references  **Logic**: - `reference_based`: Denominator calculated from same indicator (e.g., `danc1_pregnancy` for ANC1) - `unwpp_based`: Denominator from UN WPP population data - `independent`: Denominator from a different service indicator  **Importance**: This classification ensures that when selecting \"best\" denominators, we avoid using reference-based denominators (which would artificially show 100% coverage equal to the survey value).  #### 6. `compare_coverage_to_survey()`  **Purpose**: Selects the best-performing denominator for each indicator  **Input**: - Coverage estimates from all denominators - Survey reference values (forward-filled)  **Selection Algorithm**:  1. **Calculate Coverage**: For each denominator option    <pre><code>coverage = (service_volume / denominator) \u00d7 100\n</code></pre>  2. **Calculate Error**: Compare to survey benchmark    <pre><code>squared_error = (HMIS_coverage - survey_coverage)\u00b2\n</code></pre>  3. **Classify Source Type**: Label each denominator as independent, reference-based, or UNWPP  4. **Selection Hierarchy**:    <pre><code>Priority 1: Independent denominators (non-reference, non-UNWPP) \u2192 lowest error\nPriority 2: Reference-based denominators (only if no independent available)\nPriority 3: UNWPP denominators (last resort fallback)\n</code></pre>  5. **Geographic Consistency**: Best denominator selected per geographic area \u00d7 indicator (not per year)  **Output**: Coverage data filtered to only the best-performing denominator for each indicator, with ranking  **Key Design Decision**: - UNWPP denominators excluded from \"best\" selection by default - Prevents over-reliance on population projections - Ensures HMIS data drives coverage when available - UNWPP used only when no HMIS-based options exist  #### 7. `create_combined_results_table()`  **Purpose**: Merges coverage estimates and survey observations into unified output  **Input**: - Coverage comparison results (best denominator selected) - Raw survey observations - All coverage data (optional, includes all denominators)  **Output Structure**: <pre><code>admin_area_1  year  indicator_common_id  denominator_best_or_survey  value\nAfghanistan   2020  anc1                 best                        85.3\nAfghanistan   2020  anc1                 survey                      84.2\nAfghanistan   2020  anc1                 danc1_pregnancy             85.3\nAfghanistan   2020  anc1                 dwpp_pregnancy              82.1\n</code></pre>  **Denominator Categories**: - `best`: Selected optimal denominator - `survey`: Actual survey observation - `d*_*`: Individual denominator results (all options)   Statistical Methods &amp; Algorithms  #### Forward-Filling (Last Observation Carried Forward)  Survey data typically has gaps (e.g., DHS every 5 years). To create continuous denominators:  <pre><code>na.locf(survey_value, na.rm = FALSE)\n</code></pre>  **Example**: <pre><code>Year:   2015  2016  2017  2018  2019  2020\nRaw:    85.3  NA    NA    NA    87.2  NA\nFilled: 85.3  85.3  85.3  85.3  87.2  87.2\n</code></pre>  This assumes coverage remains constant until next observation.  #### Squared Error Minimization  To select the best denominator:  $$ \\text{Best denominator} = \\arg \\min_d \\sum_{t} (C_{d,t} - S_t)^2 $$  Where: - $C_{d,t}$ = Coverage using denominator $d$ in year $t$ - $S_t$ = Survey coverage in year $t$ - Summation is across all years with survey data  #### Adjustment Factor Cascading  Denominators are adjusted through sequential application of demographic factors:  **Example: ANC1 \u2192 DPT denominator** <pre><code>Base (pregnancies):     10,000\nAfter pregnancy loss:   10,000 \u00d7 (1 - 0.03) = 9,700\nAfter twin adjustment:  9,700 \u00d7 (1 - 0.015/2) = 9,627\nAfter stillbirths:      9,627 \u00d7 (1 - 0.02) = 9,435\nAfter neonatal deaths:  9,435 \u00d7 (1 - 0.039) = 9,067\n</code></pre>  Each adjustment reduces the denominator to reflect population losses at different life stages.  #### HMIS-based Denominator Calculations  **General Formula:** $$ \\text{Denominator} = \\frac{\\text{Service volume}}{\\text{Survey-based coverage} / 100} $$  **ANC1 Denominator** $$ d_{\\text{anc1, pregnancy}} = \\frac{\\text{count}_{\\text{anc1}}}{\\text{coverage}_{\\text{anc1}} / 100} $$ $$ d_{\\text{anc1, livebirth}} = d_{\\text{anc1, pregnancy}} \\times (1 - \\text{pregnancy loss rate}) \\times (1 - \\frac{\\text{twin rate}}{2}) \\times (1 - \\text{stillbirth rate}) $$ $$ d_{\\text{anc1, dpt}} = d_{\\text{anc1, pregnancy}} \\times (1 - \\text{pregnancy loss rate}) \\times (1 - \\frac{\\text{twin rate}}{2}) \\times (1 - \\text{stillbirth rate}) \\times (1 - \\text{neonatal mortality rate}) $$ $$ d_{\\text{anc1, mcv}} = d_{\\text{anc1, pregnancy}} \\times (1 - \\text{pregnancy loss rate}) \\times (1 - \\frac{\\text{twin rate}}{2}) \\times (1 - \\text{stillbirth rate}) \\times (1 - \\text{infant mortality rate}) $$  **Delivery Denominator (Live Births)** $$ d_{\\text{delivery, pregnancy}} = \\frac{\\text{count}_{\\text{delivery}}}{\\text{coverage}_{\\text{delivery}} / 100} \\times (1 - \\text{pregnancy loss rate}) $$ $$ d_{\\text{delivery, dpt}} = d_{\\text{delivery, pregnancy}} \\times (1 + \\text{twin rate}) \\times (1 - \\text{stillbirth rate}) \\times (1 - \\text{neonatal mortality rate}) $$ $$ d_{\\text{delivery, mcv}} = d_{\\text{delivery, pregnancy}} \\times (1 + \\text{twin rate}) \\times (1 - \\text{stillbirth rate}) \\times (1 - \\text{infant mortality rate}) $$  **BCG Denominator** $$ d_{\\text{bcg, pregnancy}} = \\frac{\\text{count}_{\\text{bcg}}}{\\text{coverage}_{\\text{bcg}} / 100} \\times (1 - \\text{pregnancy loss rate}) \\times (1 + \\text{twin rate}) \\times (1 - \\text{stillbirth rate}) $$ $$ d_{\\text{bcg, dpt}} = d_{\\text{bcg, pregnancy}} \\times (1 - \\text{neonatal mortality rate}) $$ $$ d_{\\text{bcg, mcv}} = d_{\\text{bcg, pregnancy}} \\times (1 - \\text{neonatal mortality rate}) \\times (1 - \\text{post-neonatal mortality rate}) $$  **Penta1 Denominator** $$ d_{\\text{penta1, pregnancy}} = \\frac{\\text{count}_{\\text{penta1}}}{\\text{coverage}_{\\text{penta1}} / 100} \\times (1 - \\text{pregnancy loss rate}) \\times (1 + \\text{twin rate}) \\times (1 - \\text{stillbirth rate}) \\times (1 - \\text{neonatal mortality rate}) $$ $$ d_{\\text{penta1, livebirth}} = d_{\\text{penta1, pregnancy}} \\times (1 - \\text{stillbirth rate}) \\times (1 - \\text{neonatal mortality rate}) $$ $$ d_{\\text{penta1, mcv}} = d_{\\text{penta1, pregnancy}} \\times (1 - \\text{post-neonatal mortality rate}) $$  #### UNWPP-based Denominator Calculations  Some denominators can also be derived from World Population Prospects (WPP) projections instead of service volumes.  **Estimated Pregnancies Based on Crude Birth Rate (CBR) and Total Population** $$ d_{\\text{wpp, pregnancy}} = \\left( \\frac{\\text{CBR}}{1000} \\right) \\times \\text{Total population} \\times \\frac{12}{\\text{months reported}} $$  **Estimated Live Births** $$ d_{\\text{wpp, livebirth}} = \\text{Total live births from WPP} \\times \\frac{12}{\\text{months reported}} $$  **Estimated Population Eligible for DPT1 (Under 1 Year)** $$ d_{\\text{wpp, dpt}} = \\text{Total under-1 population from WPP} \\times \\frac{12}{\\text{months reported}} $$  **Estimated Population Eligible for MCV (Under 5 Years)** $$ d_{\\text{wpp, mcv}} = \\text{Total under-5 population from WPP} \\times \\frac{12}{\\text{months reported}} $$  HMIS data can contain incomplete or partial reporting, where service volumes are only available for a subset of months in a given year. If fewer than 12 months of data are reported, directly using the raw count would underestimate the total number of pregnancies. To adjust for this, we scale the reported number up to a full year equivalent by applying the factor $\\frac{12}{\\text{months reported}}$.   Output Files Specification  Part 1 generates six CSV files:  #### Denominator Files  **1. M4_denominators_national.csv** **2. M4_denominators_admin2.csv** **3. M4_denominators_admin3.csv**  **Structure**: <pre><code>admin_area_1, [admin_area_2/3], year, denominator, source_indicator, target_population, value\n</code></pre>  **Fields**: - `denominator`: Full denominator name (e.g., `danc1_livebirth`) - `source_indicator`: Service used (e.g., `source_anc1`, `source_wpp`) - `target_population`: Target group (e.g., `target_livebirth`, `target_dpt`) - `value`: Calculated denominator size  #### Combined Results Files  **4. M4_combined_results_national.csv** **5. M4_combined_results_admin2.csv** **6. M4_combined_results_admin3.csv**  **Structure**: <pre><code>admin_area_1, [admin_area_2/3], year, indicator_common_id, denominator_best_or_survey, value\n</code></pre>  **Fields**: - `indicator_common_id`: Health indicator (e.g., `anc1`, `penta3`) - `denominator_best_or_survey`: Either `best`, `survey`, or specific denominator name - `value`: Coverage percentage (0-100+)  **Special \"best\" Entry**: Duplicates the selected optimal denominator for easy filtering   Data Safeguards and Validation  Part 1 includes multiple validation checks:  1. **ISO3 Validation**: Ensures survey and population data match HMIS country 2. **Geographic Matching**: Validates admin area names between HMIS and survey    - Reports match rate (e.g., \"15/20 regions match\")    - Falls back to higher geographic level if mismatch detected 3. **Fallback Mechanisms**:    - Subnational \u2192 National if no local survey data    - SBA \u2192 Delivery if SBA missing    - PNC1_mother \u2192 PNC1 if missing 4. **Edge Case Handling**: Detects when admin_area_3 should be used as admin_area_2 (e.g., Afghanistan districts) 5. **Empty Data Handling**: Creates empty CSVs with correct structure when data unavailable 6. **Error Handling**: Wraps survey processing in `tryCatch` to handle mismatches gracefully   Indicators Supported  Part 1 processes the following health indicators:  **Maternal Health**: - `anc1`: Antenatal care 1st visit - `anc4`: Antenatal care 4+ visits - `delivery`: Institutional delivery - `sba`: Skilled birth attendance - `pnc1`: Postnatal care (child) - `pnc1_mother`: Postnatal care (mother)  **Immunization**: - `bcg`: BCG vaccine - `penta1`, `penta2`, `penta3`: Pentavalent vaccine - `measles1`, `measles2`: Measles-containing vaccine - `rota1`, `rota2`: Rotavirus vaccine - `opv1`, `opv2`, `opv3`: Oral polio vaccine - `fully_immunized`: Full immunization status  **Child Health**: - `nmr`: Neonatal mortality rate (survey only) - `imr`: Infant mortality rate (survey only) - `vitaminA`: Vitamin A supplementation   Usage Notes and Best Practices  #### When to Use Which Count Variable  - `count_final_none`: No adjustments (raw reported data) - `count_final_outlier`: Outlier adjustment only - `count_final_completeness`: Completeness adjustment only - `count_final_both`: Both adjustments **(recommended)**  #### Interpreting \"best\" Denominators  The \"best\" denominator may vary by indicator and area based on: - Data availability (some services not universally reported) - Reporting completeness (affects HMIS-based denominators) - Population projection quality (affects WPP denominators) - Survey coverage levels (extreme values reduce denominator options)  #### Why Multiple Denominators?  Different denominators serve different purposes: - **Independent denominators**: Provide cross-validation between services - **Reference denominators**: Show internal HMIS consistency (but excluded from \"best\" by default) - **WPP denominators**: Offer population-based benchmarks - Comparing multiple options reveals data quality issues   Troubleshooting Common Issues  **Issue**: No matching admin areas between HMIS and survey - **Solution**: Check ISO3 code is correct; verify admin area naming conventions; module will fall back to national analysis  **Issue**: All denominators show &gt;100% coverage - **Solution**: May indicate under-reporting in survey or over-reporting in HMIS; check data quality from Module 2  **Issue**: UNWPP selected as \"best\" for most indicators - **Solution**: May indicate poor HMIS data quality or completeness; review Module 2 adjustments"},{"location":"04_module_coverage_estimates_documentation/#part-2-denominator-selection-projection-technical-details","title":"Part 2: Denominator Selection &amp; Projection (Technical Details)","text":"Purpose and Objectives  Part 2 serves three key purposes:  1. **User-Driven Denominator Selection**: While Part 1 automatically selects the \"best\" denominator by minimizing error against survey data, Part 2 allows users to override this selection and choose specific denominators based on programmatic knowledge or policy priorities 2. **Temporal Trend Analysis**: Computes year-over-year changes (deltas) in coverage to understand service delivery trends over time 3. **Survey Projection**: Projects survey-based coverage estimates forward in time using trends observed in administrative (HMIS) data, filling gaps where survey data is unavailable   Input Data Requirements  Part 2 requires the following inputs from Part 1:  | Input File | Description | Key Columns | |-----------|-------------|-------------| | `M4_combined_results_national.csv` | Combined coverage estimates for all denominators at national level | admin_area_1, year, indicator_common_id, denominator_best_or_survey, value | | `M4_combined_results_admin2.csv` | Combined coverage estimates for admin level 2 | admin_area_1, admin_area_2, year, indicator_common_id, denominator_best_or_survey, value | | `M4_combined_results_admin3.csv` | Combined coverage estimates for admin level 3 | admin_area_1, admin_area_3, year, indicator_common_id, denominator_best_or_survey, value |  These files contain: - Coverage estimates calculated using different denominators - Survey values for comparison - A \"best\" denominator selection from Part 1 - Source and source detail information   User Configuration Parameters  Users configure Part 2 through two key parameter sets:  #### 1. Denominator Selection Configuration  At the top of the script, users specify which denominator to use for each indicator:  <pre><code>DENOMINATOR_SELECTION &lt;- list(\n  # PREGNANCY-RELATED INDICATORS\n  anc1 = \"best\",                    # Options: \"best\", \"danc1_pregnancy\", \"ddelivery_pregnancy\", \"dbcg_pregnancy\", \"dlivebirths_pregnancy\", \"dwpp_pregnancy\"\n  anc4 = \"best\",\n\n  # LIVE BIRTH-RELATED INDICATORS\n  delivery = \"best\",                # Options: \"best\", \"danc1_livebirth\", \"ddelivery_livebirth\", \"dbcg_livebirth\", \"dlivebirths_livebirth\", \"dwpp_livebirth\"\n  bcg = \"best\",\n  sba = \"best\",\n  pnc1_mother = \"best\",\n  pnc1 = \"best\",\n\n  # DPT-ELIGIBLE AGE GROUP INDICATORS\n  penta1 = \"best\",                  # Options: \"best\", \"danc1_dpt\", \"ddelivery_dpt\", \"dpenta1_dpt\", \"dbcg_dpt\", \"dlivebirths_dpt\", \"dwpp_dpt\"\n  penta2 = \"best\",\n  penta3 = \"best\",\n  opv1 = \"best\",\n  opv2 = \"best\",\n  opv3 = \"best\",\n\n  # MEASLES-ELIGIBLE AGE GROUP INDICATORS\n  measles1 = \"best\",                # Options: \"best\", \"danc1_measles1\", \"ddelivery_measles1\", \"dpenta1_measles1\", \"dbcg_measles1\", \"dlivebirths_measles1\", \"dwpp_measles1\"\n  measles2 = \"best\",\n\n  # ADDITIONAL INDICATORS\n  vitaminA = \"best\",                # Options: \"best\", \"danc1_vitaminA\", \"dbcg_vitaminA\", \"ddelivery_vitaminA\", \"dwpp_vitaminA\"\n  fully_immunized = \"best\"          # Options: \"best\", \"danc1_fully_immunized\", \"dbcg_fully_immunized\", \"ddelivery_fully_immunized\", \"dwpp_fully_immunized\"\n)\n</code></pre>  **Denominator Options by Indicator Type:**  The available denominators vary by indicator type based on the appropriate target population:  - **Pregnancy-based indicators** (ANC1, ANC4): Use pregnancy-adjusted denominators - **Live birth-based indicators** (Delivery, BCG, SBA, PNC): Use live birth-adjusted denominators - **DPT-eligible age group** (Penta1-3, OPV1-3): Use DPT-adjusted denominators (children eligible for DPT) - **Measles-eligible age group** (Measles1, Measles2): Use measles-adjusted denominators (children eligible for measles vaccine)  Each denominator option combines a source (ANC1, Delivery, BCG, Penta1, or WPP) with an age-adjustment factor.  #### 2. Administrative Level Configuration  <pre><code>RUN_NATIONAL &lt;- TRUE  # Always TRUE - national analysis is mandatory\nRUN_ADMIN2 &lt;- TRUE    # Enable/disable admin level 2 analysis\nRUN_ADMIN3 &lt;- TRUE    # Enable/disable admin level 3 analysis\n</code></pre>  The script automatically checks data availability and disables admin levels with no data.   Core Functions and Methods  #### Function 1: `coverage_deltas()`  **Purpose**: Calculates year-over-year changes in coverage for each indicator-denominator-geography combination.  **Algorithm**: <pre><code>coverage_deltas &lt;- function(coverage_df, lag_n = 1, complete_years = TRUE)\n</code></pre>  **Process**: 1. Groups data by geography (admin areas), indicator, and denominator 2. Optionally fills in missing years to create a complete time series 3. Sorts data chronologically within each group 4. Calculates delta as: $\\Delta\\text{coverage}_t = \\text{coverage}_t - \\text{coverage}_{t-1}$  **Mathematical Formulation**: $$ \\Delta C_{i,d,g,t} = C_{i,d,g,t} - C_{i,d,g,t-1} $$  where: - $C$ = coverage estimate - $i$ = indicator - $d$ = denominator - $g$ = geographic area - $t$ = time (year)  **Input**: - `coverage_df`: Data frame with coverage estimates - `lag_n`: Number of years to lag (default = 1 for year-over-year) - `complete_years`: Whether to fill missing years (default = TRUE)  **Output**: Data frame with original coverage values plus a `delta` column showing year-over-year change.  **Example Output**:  | admin_area_1 | indicator_common_id | denominator | year | coverage | delta | |--------------|-------------------|-------------|------|----------|-------| | Country A | penta3 | dpenta1_dpt | 2018 | 75.2 | NA | | Country A | penta3 | dpenta1_dpt | 2019 | 78.5 | 3.3 | | Country A | penta3 | dpenta1_dpt | 2020 | 80.1 | 1.6 |  #### Function 2: `project_survey_from_deltas()`  **Purpose**: Projects survey-based coverage estimates forward using administrative data trends.  **Algorithm**: <pre><code>project_survey_from_deltas &lt;- function(deltas_df, survey_raw_long)\n</code></pre>  **Process**:  1. **Identify Baseline**: For each geography-indicator combination, find the most recent survey observation    - Extract the last observed survey year    - Record the baseline coverage value at that year  2. **Attach Baseline to Each Denominator Path**: Since Part 2 operates on specific denominator selections, attach the baseline to each denominator series  3. **Compute Cumulative Deltas**: For years after the baseline year, calculate cumulative sum of deltas:    $$\\text{cumulative delta}_t = \\sum_{\\tau = \\text{baseline year} + 1}^{t} \\Delta C_\\tau$$  4. **Calculate Projection**: Add cumulative delta to baseline value:    $$\\text{Projected coverage}_t = \\text{Baseline coverage} + \\text{cumulative delta}_t$$  **Mathematical Formulation**:  For each indicator $i$, denominator $d$, and geography $g$:  1. Find baseline: $$ y_{\\text{baseline}} = \\max\\{t : S_{i,g,t} \\text{ exists}\\} $$ $$ S_{\\text{baseline}} = S_{i,g,y_{\\text{baseline}}} $$  2. For $t &gt; y_{\\text{baseline}}$: $$ \\hat{S}_{i,d,g,t} = S_{\\text{baseline}} + \\sum_{\\tau = y_{\\text{baseline}} + 1}^{t} \\Delta C_{i,d,g,\\tau} $$  where: - $S$ = survey-based coverage estimate - $\\hat{S}$ = projected survey coverage - $\\Delta C$ = year-over-year change in administrative coverage  **Assumptions**: - Trends observed in administrative data reflect true changes in service coverage - The baseline survey provides an accurate reference point - Administrative data trends can be applied to survey estimates  **Input**: - `deltas_df`: Output from `coverage_deltas()` containing coverage changes - `survey_raw_long`: Raw survey data with years and values  **Output**: Data frame with projected coverage for each year, indicator, denominator, and geography combination.  **Example Output**:  | admin_area_1 | indicator_common_id | denominator | year | baseline_year | projected | |--------------|-------------------|-------------|------|---------------|-----------| | Country A | penta3 | dpenta1_dpt | 2018 | 2018 | 75.0 | | Country A | penta3 | dpenta1_dpt | 2019 | 2018 | 78.3 | | Country A | penta3 | dpenta1_dpt | 2020 | 2018 | 79.9 |  #### Function 3: `build_final_results()`  **Purpose**: Combines HMIS coverage, projected survey estimates, and original survey values into a unified output dataset.  **Algorithm**: <pre><code>build_final_results &lt;- function(coverage_df, proj_df, survey_raw_df = NULL)\n</code></pre>  **Process**:  1. **Prepare HMIS Coverage**: Extract coverage estimates from administrative data    - Rename coverage column to `coverage_cov` for clarity  2. **Merge Projections**: Join projected survey estimates    - Match by geography, year, indicator, and denominator    - Create `coverage_avgsurveyprojection` column  3. **Process Original Survey Data** (if available):    - Collapse multiple survey sources by taking mean value    - Preserve source metadata (source, source_detail)    - Expand survey values across all denominators for that indicator  4. **Calculate Final Projections**: Use an improved projection formula that anchors to the last survey value:     For years after the last survey year:    $$    \\text{Projected coverage}_t = \\text{Last survey value} + (C_{\\text{HMIS},t} - C_{\\text{HMIS, last survey year}})    $$     This additive approach:    - Preserves the calibration to survey data    - Applies the HMIS trend (delta) to extend the estimate forward    - Avoids compounding errors from year-to-year deltas  5. **Combine Results**: Merge all components using full outer join to preserve:    - Years with only HMIS data    - Years with only survey data    - Years with both data sources  **Mathematical Formulation**:  Let: - $t_s$ = year of last survey - $S_{t_s}$ = survey coverage at year $t_s$ - $C_{\\text{HMIS},t}$ = HMIS-based coverage at year $t$  For $t &gt; t_s$: $$ \\hat{C}_t = S_{t_s} + (C_{\\text{HMIS},t} - C_{\\text{HMIS},t_s}) $$  **Input**: - `coverage_df`: HMIS-based coverage estimates from selected denominators - `proj_df`: Projected survey estimates from `project_survey_from_deltas()` - `survey_raw_df`: Original survey data (optional)  **Output**: Comprehensive data frame with columns: - Geographic identifiers (admin_area_1, admin_area_2, admin_area_3) - year, indicator_common_id, denominator - `coverage_cov`: HMIS-based coverage - `coverage_original_estimate`: Original survey values - `coverage_avgsurveyprojection`: Projected survey coverage - `survey_raw_source`: Survey data source (e.g., \"DHS\", \"MICS\") - `survey_raw_source_detail`: Detailed source information   Helper Functions  #### `filter_by_denominator_selection()`  **Purpose**: Filters the combined results from Part 1 based on user's denominator selection.  **Algorithm**: 1. Iterate through each indicator in `DENOMINATOR_SELECTION` 2. For each indicator:    - If selection is \"best\": Keep rows where `denominator_best_or_survey == \"best\"`    - If selection is a specific denominator: Keep rows where `denominator_best_or_survey == selected_denominator` 3. Convert selected rows to coverage format (rename columns, filter out survey entries) 4. Combine results across all indicators  **Input**: - `combined_results_df`: Output from Part 1 with all denominator options - `selection_list`: The DENOMINATOR_SELECTION configuration list  **Output**: Filtered data frame containing only the user-selected denominators.  #### `extract_survey_from_combined()`  **Purpose**: Extracts raw survey values from Part 1 combined results.  **Algorithm**: 1. Filter for rows where `denominator_best_or_survey == \"survey\"` 2. Rename `value` column to `survey_value` 3. Select relevant columns dynamically based on admin levels present  **Input**: Combined results data frame from Part 1  **Output**: Survey data frame with columns: admin areas, year, indicator_common_id, survey_value   Workflow Execution Steps  Part 2 executes the following workflow for each administrative level (national, admin2, admin3):  **Step 1: Load Data** - Load combined results from Part 1 for all admin levels - Check which admin levels have data - Extract survey data for use as projection baseline - Display messages about data availability  **Step 2: For Each Admin Level**     **Sub-step 1: Filter by Denominator Selection**    - Apply user's denominator choices using `filter_by_denominator_selection()`    - Message: Number of records selected     **Sub-step 2: Compute Deltas**    - Calculate year-over-year coverage changes using `coverage_deltas()`    - Creates complete time series with gaps filled     **Sub-step 3: Project Survey Values**    - Use `project_survey_from_deltas()` to extend survey estimates    - Baseline is anchored to most recent survey    - Projections use cumulative deltas from HMIS trends     **Sub-step 4: Build Final Results**    - Combine HMIS coverage, projections, and original surveys    - Calculate final projected estimates using additive formula    - Preserve all metadata  **Step 3: Standardize and Save Outputs** - Define required columns for each admin level - Ensure all required columns exist (add as NA if missing) - Order columns correctly - Remove inappropriate admin level columns - Save as CSV with UTF-8 encoding - Create empty files for admin levels with no data   Output Specifications  Part 2 produces three output files:  #### 1. National Output: `M5_coverage_estimation_national.csv`  **Columns**: - `admin_area_1`: Country name - `year`: Year of estimate - `indicator_common_id`: Standardized indicator code - `denominator`: Selected denominator source - `coverage_original_estimate`: Original survey-based coverage (NA for years without surveys) - `coverage_avgsurveyprojection`: Projected survey coverage using HMIS trends - `coverage_cov`: HMIS-based coverage estimate - `survey_raw_source`: Survey source (e.g., \"DHS 2018\") - `survey_raw_source_detail`: Additional source details  **Note**: Does NOT include admin_area_2 or admin_area_3 columns.  #### 2. Admin Level 2 Output: `M5_coverage_estimation_admin2.csv`  **Columns**: Same as national, plus: - `admin_area_2`: Second-level administrative division name (e.g., province, region)  **Note**: Does NOT include admin_area_3 column.  #### 3. Admin Level 3 Output: `M5_coverage_estimation_admin3.csv`  **Columns**: - `admin_area_1`: Country name - `admin_area_3`: Third-level administrative division name (e.g., district) - `year`: Year of estimate - `indicator_common_id`: Standardized indicator code - `denominator`: Selected denominator source - `coverage_original_estimate`: Original survey coverage - `coverage_avgsurveyprojection`: Projected survey coverage - `coverage_cov`: HMIS-based coverage - `survey_raw_source`: Survey source - `survey_raw_source_detail`: Source details  **Note**: Does NOT include admin_area_2 column (skips straight from admin_area_1 to admin_area_3).   Methodological Considerations  #### 1. Denominator Selection Strategy  **When to use \"best\"**: - Uncertain about which denominator is most appropriate - Want to rely on data-driven selection from Part 1 - Starting point for analysis  **When to specify a denominator**: - Programmatic knowledge suggests a specific denominator is most accurate - Policy requirements dictate use of specific population estimates - Conducting sensitivity analyses - Known issues with certain data sources  #### 2. Projection Methodology  The projection approach in Part 2 uses an **additive delta method** rather than multiplicative or direct replacement:  **Advantages**: - Preserves the level calibration from survey data - Smoothly extends survey estimates using administrative trends - Avoids compounding errors from year-to-year changes - Maintains consistency when HMIS coverage is stable  **Limitations**: - Assumes HMIS trends reflect true coverage changes - May diverge from reality if administrative data quality declines - Projections become less reliable further from baseline survey - Does not account for systematic biases in HMIS data  **Best Practice**: Projections should be validated against new survey data when available, and the baseline should be updated with the most recent survey.  #### 3. Handling Missing Data  Part 2 implements several strategies for missing data:  - **Complete time series**: The `coverage_deltas()` function can fill missing years, creating a continuous series - **Survey gaps**: Projections extend estimates forward, but years before the first survey remain NA - **Admin level gaps**: Script automatically detects and skips admin levels with no data - **Missing denominators**: If a selected denominator doesn't exist for an indicator, that indicator-denominator combination is omitted  #### 4. Multi-Level Analysis Consistency  Part 2 processes each administrative level independently:  - **National**: Aggregated country-level estimates - **Admin 2**: Provincial/regional estimates (may not sum to national due to different denominators) - **Admin 3**: District-level estimates  **Important**: Estimates across levels may not be directly comparable if different denominators are selected or if data quality varies by level.   Example Use Case  **Scenario**: Analyst wants to estimate Penta3 coverage for 2015-2024 using Penta1-derived denominators.  **Configuration**: <pre><code>DENOMINATOR_SELECTION &lt;- list(\n  penta3 = \"dpenta1_dpt\"  # Override \"best\" to use Penta1-based denominator\n)\n</code></pre>  **Input Data**: - Part 1 combined results with multiple denominators calculated - Survey data available for 2018 (75.0% coverage) - HMIS data available for all years 2015-2024  **Process**:  1. **Filter**: Extract only Penta3 estimates using dpenta1_dpt denominator 2. **Compute Deltas**: Calculate year-over-year changes in HMIS-based coverage 3. **Project**:    - Baseline year: 2018 (last survey)    - Baseline value: 75.0%    - For 2019: 75.0% + (HMIS 2019 - HMIS 2018)    - For 2020: 75.0% + (HMIS 2020 - HMIS 2018)    - Continue through 2024  **Output**: - Years 2015-2017: Only HMIS coverage available - Year 2018: Survey value (75.0%) and HMIS coverage - Years 2019-2024: Projected coverage based on HMIS trends   Validation and Quality Checks  Users should validate Part 2 outputs by:  1. **Checking projection reasonableness**:    - Are projected values within plausible ranges (0-100%)?    - Do trends make programmatic sense?  2. **Comparing denominators**:    - Run Part 2 with different denominator selections    - Assess sensitivity of results to denominator choice  3. **Validating against new surveys**:    - When new survey data becomes available, compare projections to actual values    - Update baseline and re-run if necessary  4. **Reviewing HMIS trends**:    - Large deltas may indicate data quality issues    - Sudden changes should be investigated  5. **Admin level consistency**:    - Check if subnational trends align with national patterns    - Investigate large discrepancies   Integration with Part 1  Part 2 builds directly on Part 1 outputs:  | Part 1 Output | Part 2 Use | |---------------|------------| | Combined results with all denominators | Input for denominator filtering | | \"Best\" denominator selections | Default option if user doesn't specify | | Coverage estimates | Basis for delta calculations | | Survey values | Baseline for projections | | Source metadata | Preserved in final output |  **Workflow Connection**: <pre><code>Part 1: Calculate denominators \u2192 Select \"best\" \u2192 Output combined results\n                                                          \u2193\nPart 2: User selects denominators \u2192 Calculate trends \u2192 Project surveys \u2192 Final estimates\n</code></pre> Troubleshooting Common Issues  **Issue**: \"No data in admin2 combined results\" - **Cause**: Part 1 didn't process admin level 2, or no subnational data exists - **Solution**: Set `RUN_ADMIN2 &lt;- FALSE` or check Part 1 inputs  **Issue**: Projections show implausible values (&gt;100% or &lt;0%) - **Cause**: Large errors in HMIS data or inappropriate denominator - **Solution**: Review denominator selection, check HMIS data quality, consider different denominator  **Issue**: Missing denominators in output - **Cause**: Selected denominator not calculated in Part 1 for that indicator - **Solution**: Check Part 1 denominator options, verify indicator-denominator compatibility  **Issue**: Gaps in projected coverage - **Cause**: Missing HMIS data for some years - **Solution**: Review Module 2 outputs, check data completeness adjustments   <p>Last edit: 2025 November 8</p>"}]}